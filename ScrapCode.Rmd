---
title: "ScrapCode"
author: "Jordan Von Eggers"
date: "2023-05-30"
output: html_document
---



## 10000. OLD CODE

## 2. Stats for manuscript

Updated Sept 2, 2022

```{r}
# what is the total number of cores and lakes?
length(unique(metadata$lake_id)) # 36 lakes
length(unique(metadata$lake_drive)) # 48 cores 

#average and range of sediment depth for cores? 
lake_drives<-unique(metadata$lake_drive)
maxd<-NULL
for(i in 1:length(lake_drives)){
        tmp<-metadata[metadata$lake_drive==lake_drives[i],]
       maxd <-c(maxd,max(tmp$depth))
}
mean(maxd)
#21.83333
max(maxd)
#88
min(maxd)   
#2

min(metadata$max_lake_depth)
max(metadata$max_lake_depth)
#0.5-19m

#how many lakes greater than or equal to 26 cm deep? 
table(maxd>=26)
22/(26+22)
#0.4583333 of the lakes were at or greater than 26 cm 

# how many samples are less than or equal to 26 cm
nrow(metadata[metadata$depth<=26,])
# 429 samples

# how many samples greater than 26 cm
478-429
#49 - 6 sept 2022

meta_sed_26<-metadata[metadata$depth<=26,]

# JVE notes - I need to say how many samples are measured in ALL samples, not just 26 cm 

# #how many samples <=26 have elemental measurements?
# sub<-meta_sed_26[is.na(meta_sed_26$d_13_c)==FALSE,]
# nrow(sub)
# #121 samples
# 
# #how many cores and samples have C:N data
# length(unique(ps@sam_data[is.na(ps@sam_data$cn)==FALSE,]$lake_id))
# # 28
# #samples less than 26 cm that have CN data
# nrow(meta_sed_26[is.na(meta_sed_26$cn)==FALSE,])
# # 262 samples have C:N, C%, N%
# 
# #how many cores have d13C data
# length(unique(ps@sam_data[is.na(ps@sam_data$d_13_c)==FALSE,]$lake_id))
# # 13
# #how many samples <=26 have elemental measurements?
# nrow(meta_sed_26[is.na(meta_sed_26$d_13_c)==FALSE,])
# #121 samples

# so INSTEAD - did this:
# all the true
table(is.na(metadata$pH)==F)
#278

table(is.na(metadata$cn)==F)
#282

table(is.na(metadata$d_13_c)==F)
#130



#where are those from?
unique(ps@sam_data[is.na(ps@sam_data$d_13_c)==FALSE,]$mountain_range)
#just the Snowies

#Values for elemental measurements for results section



print("surface")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$n_perc, na.rm=T),digits=2)

print("replacement")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$n_perc, na.rm=T),digits=2)

print("depauperate")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$n_perc, na.rm=T),digits=2)

print("replacement and depauperate")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$c_perc, na.rm=T),digits=3)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$n_perc, na.rm=T),digits=2)

# Sept 6
# [1] "surface"
# [1] 13.09
# [1] 1.34
# [1] "replacement"
# [1] 11.46
# [1] 1.02
# [1] "depauperate"
# [1] 11.34
# [1] 0.95
# [1] "replacement and depauperate"
# [1] 11.40
# [1] 0.98


# a positive shift in carbon isotopic composition (Î´13C) of occurred from depths 0-4 cm
sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(0),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -30.0454 - 6Sept
sd(sub$d_13_c)
#[1] 2.124595 - 6 sept


sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(4),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
sd(sub$d_13_c)
#[1] 4.420052 - 6 sept
(-30.0454) - (-25.2764)
#-4.769 (round to 4.77%) - 6 sept

sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -24.65961 - 6 sept
sd(sub$d_13_c)
#[1] 4.301652  - 6 sept

sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(24,26),]
nrow(sub)
#10 samples at depths 24 and 26 with isotopic data
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -26.081  - 6 sept
sd(sub$d_13_c)
#[1] 2.139629  - 6 sept


# range of elevations
range(metadata$elevation_meters,na.rm=T) #NAs are the blanks that don't have elevations
#2032 3350
#subset for 26 cm cutoff elevation
range(meta_sed_26$elevation_meters,na.rm=T) 
#2032 3350 (same)

#range of temperatures

#these are the same for the 26 cm and less samples and the entire dataset
range(metadata$water_sample_t_bot, na.rm=T)
#3.8 20.1
#these are the same for the 26 cm and less samples and the entire dataset
range(metadata$water_sample_t_surf, na.rm=T)
#11.3 20.7
range(metadata$water_sample_ph_bot, na.rm=T)
#5.39 9.68
range(metadata$water_sample_ph_surf, na.rm=T)
#6.83 9.63
range(metadata$water_sample_do_bot, na.rm=T)
#-0.06 12.14
range(metadata$water_sample_do_surf, na.rm=T)
#3.14 8.23

# Started with 555 samples
# ended with 478 samples

# How many lakes cored in 2018 and 2017
sub2017<-metadata[metadata$year_sample=="2017",]
length(unique(sub2017$lake_name))
sub2018<-metadata[metadata$year_sample=="2018",]
length(unique(sub2018$lake_name))

#how many cores from the Snowies
sub<-metadata[metadata$mountain_range=="Snowy",]
length(unique(sub$lake_name))

#looking for how many lakes were cored more than once (eight)
sort(unique(sub$lake_drive))

#bighorn
sub<-metadata[metadata$mountain_range=="Bighorn",]
length(unique(sub$lake_name))
sub<-metadata[metadata$mountain_range=="Beartooth",]
length(unique(sub$lake_name))
sub<-metadata[metadata$mountain_range=="Wind River",]
length(unique(sub$lake_name))


#create supplementary table 1

ST1<-metadata[,names(metadata)%in%c("lake_id"                
, "drive"                  
,"lake_name"              
,"lake_number"            
, "year_sample"            
, "mountain_range"         
, "latitude"               
, "longitude"              
, "elevation_meters"       
, "max_lake_depth"
,"water_sample_depth_surf"
, "water_sample_ph_surf"   
,"water_sample_do_surf"   
, "water_sample_t_surf"    
, "water_sample_depth_bot" 
,"water_sample_ph_bot"    
, "water_sample_do_bot"    
,"water_sample_t_bot")]

ST1<-unique(ST1)

write.csv(ST1,"SupplementaryTable1.csv")


#what are the top 3 ESVs?

#what percent of reads are archaea?
arch<-subset_taxa(ps, Kingdom=="Archaea")
sum(rowSums(arch@otu_table))/sum(rowSums(ps@otu_table))
#[1] 0.172937

# additional generalizations between shallow, warm and cool, deep lakes

#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata

require(GGally)

metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

cor(metadata_sub)

which(is.na(metadata_sub[3]==T))
metadata_sub[which(is.na(metadata_sub[3]==T)),]

ggpairs(metadata_sub)
```

## 3. Null models (BNTI and RCBray)

### a. Phylogenetic tree

create Phylogenetic tree on Teton

```{r}
library(phylotools)
library(readr)

uniqueOTUs_split<- phylotools::read.fasta("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/original files off Teton/uniqueOTUs_16S.fa")
uniqueOTUs_split$OTU <- paste(rep(">centroid=",nrow(uniqueOTUs_split)),str_split(uniqueOTUs_split$seq.name, pattern="_",2,simplify=T)[,2],sep="")
uniqueOTUs_split$OTU <- str_split(uniqueOTUs_split$OTU, pattern=";",3,simplify=T)[,1]

centroids <- rownames(as.data.frame(ps@otu_table@.Data))
centroids <- paste(rep(">",length(centroids)),centroids,sep="")
sed_norm_subset<-uniqueOTUs_split[uniqueOTUs_split$OTU%in%centroids,]
sed_norm_subset$seq.name<-NULL
names(sed_norm_subset)<-c("seq","OTU")
sed_norm_subset$even <- seq(2,by=2, len=nrow(sed_norm_subset))
sed_norm_subset$odd <- seq(1,by=2, len=nrow(sed_norm_subset))

table(sort(centroids,decreasing=T)==sort(sed_norm_subset$OTU,decreasing=T))

odd<-sed_norm_subset[,c(2,4)]
names(odd)<-c("item","number")
even<-sed_norm_subset[,c(1,3)]
names(even)<-c("item","number")
all<-rbind(odd,even)
all<-all[order(all$number),]
all$number<-NULL
write_lines(all$item,file = "sed_norm_fasta_29Aug2022.txt")
```

on teton copy the file over

```{bash}
rsync /Users/jordanscheibe/Desktop/sed_norm_fasta_29Aug2022.txt jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree

#rename as a fasta file
mv sed_norm_fasta_29Aug2022.txt sed_norm_fasta_29Aug2022.fa
```

create bash file for muscle and fastTree clustalo_fasttree.sh

```{bash}
#!/bin/bash
#SBATCH --job-name clustalo_fasttree
#SBATCH --mem=512GB
#SBATCH --time=20:00:00
#SBATCH --account=microbiome
#SBATCH --output=clustalo_fasttree_%A.out
hostname; date
module load miniconda3/4.3.30 gcc/7.3.0
source activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree

echo "clustalo -i sed_norm_fasta_29Aug2022.fa -o sed_norm_fasta_29Aug2022_muscled.fasta -v"
clustalo -i sed_norm_fasta_29Aug2022.fa -o sed_norm_fasta_29Aug2022_muscled.fasta -v
echo "finish muscle"

echo "FastTree -nt sed_norm_fasta_29Aug2022_muscled.fasta > sed_norm_fasta_29Aug2022_muscled.nwk"
FastTree -nt sed_norm_fasta_29Aug2022_muscled.fasta > sed_norm_fasta_29Aug2022_muscled.nwk
echo "finished FastTree"
source deactivate shotgun_env
date
```

copy to computer

```{bash}
rsync jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/sed_norm_fasta_29Aug2022_muscled.nwk /Users/jordanscheibe/Desktop/
```

### b. BNTI on Teton

### i. subset by the top 10K ESVs

ps and ps_tr (transformed) are very similar, so I use the ESVs from ps_tr since thats likely more accurate, and then subset from the ps (untransformed table)

```{r}
require(ape)
#how many samples greater than 26 cm
nrow(ps_tr@sam_data[ps_tr@sam_data$bin_depth==100,])
# 49 samples - 30 aug

all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)
#3500 = 10888 OTUs - 30Aug

#compare with the top 5,000 and 10,000 most abundant centroids across all samples
means <- as.data.frame(rowMeans(ps_tr@otu_table))
names(means)<-"val"
means$otu<-rownames(means)
avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:5000] 
table(avgabundtax %in% unique_alldepths)
# TRUE 
# 5000 
table(unique_alldepths %in% avgabundtax)
# FALSE  TRUE 
#  5888  5000 - 30Aug

avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:10000]
table(avgabundtax %in% unique_alldepths)
# FALSE  TRUE 
#  1065  8935 - 30Aug
table(unique_alldepths %in% avgabundtax)
# FALSE  TRUE 
#  1953  8935 - 30Aug

#what percent of the entire dataset are these 10,888 ESVs? 
tmp<-as.data.frame(otu_table(ps_tr))
tmp2<-tmp[rownames(tmp)%in%unique_alldepths,]
sum(tmp2)/sum(tmp)
#  0.8720786 - 30Aug2022 10888 ESVs included here make up 87.21% of the 91957 total ESVs
rm(tmp2)
rm(tmp)



my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
tree<-read.tree("sed_norm_fasta_29Aug2022_muscled.nwk")
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), tree)
table(sort(physeq@phy_tree[["tip.label"]])==sort(unique_alldepths)) #all true
tree_dist <- cophenetic(physeq@phy_tree)

rm(my_subset)
rm(means)
rm(ps)
rm(ps_tr)
rm(tree)
rm(unique_alldepths)
rm(ps_tmp)
rm(cm)
rm(i)
rm(all_depths)
rm(avgabundtax)
rm(metadata)
rm(otu_tab_10knorm)
rm(tax_tab)

#save.image("WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData")
```

copy to Teton server

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI


BNTI_32cores_abundwt_10kESVs.R
require(phyloseq)
require(MicEco)
load("WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData")
BNTI<-ses.comdistnt2(t(physeq@otu_table),tree_dist, cores=32, abundance.weighted = TRUE)
save.image("WyLakeMicrobes_env_30Aug2022_finalBNTI_abundwt_10888ESVs.RData")


runR_32_abundwt_10kESVs.sh
#!/bin/bash
#SBATCH --job-name BNTI_BDC_abundwt_10k
#SBATCH --mem=120GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=32
#SBATCH --account=microbiome
#SBATCH --output=BNTI_BDC_abundwt_10kESVs%A.out
hostname; date
module load miniconda3/4.3.30 gcc/7.3.0 r/4.0.5-py27
source activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/BNTI
module load r/4.0.5-py27
srun Rscript BNTI_32cores_abundwt_10kESVs.R
echo "srun Rscript BNTI_32cores_abundwt_10kESVs.R"
source deactivate shotgun_env
echo "finished BNTI"
date
```

## c. RCBray on Teton

```{r}
RC_Bray_Wisnoski.R
load("WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData")
#load required packages
require(phyloseq)
require(vegan)

#pull out OTU table
OTUs<-t(as.data.frame(physeq@otu_table))
# Calculate abundance-weighted Raup-Crick dissimilarities
regional.abunds <- t(as.matrix(colSums(OTUs)))
regional.relabunds <- decostand(regional.abunds, method = "total")
occupancy.probs <- t(as.matrix(colSums(decostand(OTUs, method = "pa")) / nrow(OTUs)))
site.abunds <- rowSums(OTUs)
site.rich <- specnumber(OTUs)
a <- regional.relabunds * occupancy.probs
# Create a null community based on Stegen et al. 2015
set.seed(47405)

# stochastic community assembly nulls


nullcom.rcabund <- function(OTUs, stand = "total", distance = "bray", nsims = 999){
  
  # create output object
  r <- nrow(OTUs)
  c <- ncol(OTUs)
  spec.vec <- 1:ncol(OTUs)
  RCbray.nulls <- array(NA, c(r, r, nsims))
  
  
  for(i in 1:nsims){
    # if(i == 1) pb <- progress_bar$new(total = nsims, force = T)
    # pb$update(ratio = i/nsims)
    
    null.comm <- OTUs * 0
    # for first simulation:
    for(row.i in 1:nrow(null.comm)){
      #print(paste("run :", i, " -> ", row.i, " : ", site.abunds[row.i], " inds"))
      
      while(rowSums(null.comm)[row.i] < site.abunds[row.i]){
        
        
        # choose a species based on its occupancy
        local.specs <- sample(x = spec.vec, size = site.rich[row.i],
                              prob = as.vector(occupancy.probs), replace = FALSE)
        
        local.probs <- decostand(t(as.matrix(regional.abunds[,local.specs])), method = "total")
        
        local.inds <- sample(x = local.specs, size = site.abunds[row.i],
                             prob = as.vector(local.probs), replace = TRUE)
        
        local.abunds <- rle(sort(local.inds))
        
        # add an individual to the local community
        null.comm[row.i, local.abunds$values] <- local.abunds$lengths
      }
    }
    null.bray <- as.matrix(vegdist(decostand(null.comm, method = stand), method = distance))
    RCbray.nulls[,,i] <- null.bray
  }
  return(RCbray.nulls)
}

RCbray_output<-nullcom.rcabund(OTUs=OTUs, stand = "total", distance = "bray", nsims = 999)
save.image("WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData")


run_RC_Bray_Wisnoski.sh
#!/bin/bash
#SBATCH --job-name RC_Bray_W_10K
#SBATCH --mem=110GB
#SBATCH --time=2-00:00:00
#SBATCH --account=microbiome
#SBATCH --output=RC_Bray_W_10K_%A.out
hostname; date
module load miniconda3/4.3.30 gcc/7.3.0 r/4.0.5-py27
source activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/BNTI
module load r/4.0.5-py27
srun Rscript RC_Bray_Wisnoski.R
echo "srun Rscript RC_Bray_Wisnoski.R"
source deactivate shotgun_env
echo "finished BNTI"
date

```

## 4. Hill's number

### a. ran on rarefied data

```{r}
require(iNEXT)
bio1 <- as.matrix(ps@otu_table@.Data)
bio2 <- split(bio1, as.numeric(rep(1:ncol(bio1), each = nrow(bio1))))
bio3 <- iNEXT(bio2, q=0, datatype="abundance") 

# select asymptotic estimates 
bio4 <- bio3$AsyEst
bio5 <- subset(bio4, bio4$Diversity =="Species richness")  
bio6 <- subset(bio4, bio4$Diversity =="Shannon diversity")  
bio7 <- subset(bio4, bio4$Diversity =="Simpson diversity")  

#create a table with diversity estimates
bio8 <- data.frame(cbind(bio5$Observed, bio6$Observed, bio7$Observed))
rownames(bio8) <- colnames(bio1)
colnames(bio8) <- c("Species_Richness", "Shannon_Diversity","Simpson_Dominance")
head(bio8)
write.table(bio8,file="hillnumbers_norm_endpoint_6Sept2022.txt",sep="\t",row.names=T, col.names=T)
hill_norm<-bio8

rm(bio1)
rm(bio2)
rm(bio3) 
rm(bio4)
rm(bio5)
rm(bio6)
rm(bio7)
rm(bio8)
```

### b. regression of richness and depth

```{r}
require(mgcv)
#load environment and Hill's number file. 
meta_sed_26<-metadata[metadata$depth<=26,]
hills<-read.delim("hillnumbers_norm_endpoint_6Sept2022.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")

lm<-lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth)
plot(meta_sed_26_hills$depth, meta_sed_26_hills$Species_Richness)
abline(lm)

summary(lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Shannon_Diversity~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Simpson_Dominance~meta_sed_26_hills$depth))

#tried this with a GAM too, but not much different 
Data<-data.frame(x=meta_sed_26_hills$depth,y=meta_sed_26_hills$Species_Richness)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=3), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)

```

Call: lm(formula = meta_sed_26_hills$Species_Richness ~ meta_sed_26_hills$depth)

Residuals: Min 1Q Median 3Q Max -1829.58 -394.87 -16.98 432.42 1375.02

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 2608.585 47.284 55.168 \<2e-16 ***meta_sed_26_hills\$depth -30.408 3.482 -8.732 \<2e-16***

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 554.2 on 427 degrees of freedom Multiple R-squared: 0.1515, Adjusted R-squared: 0.1495 F-statistic: 76.25 on 1 and 427 DF, p-value: \< 2.2e-16

Call: lm(formula = meta_sed_26_hills$Shannon_Diversity ~ meta_sed_26_hills$depth)

Residuals: Min 1Q Median 3Q Max -877.93 -233.20 -27.67 216.85 1036.11

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 892.845 27.068 32.985 \< 2e-16 ***meta_sed_26_hills\$depth -14.226 1.993 -7.137 4.13e-12***

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 317.2 on 427 degrees of freedom Multiple R-squared: 0.1066, Adjusted R-squared: 0.1045 F-statistic: 50.93 on 1 and 427 DF, p-value: 4.134e-12

Call: lm(formula = meta_sed_26_hills$Simpson_Dominance ~ meta_sed_26_hills$depth)

Residuals: Min 1Q Median 3Q Max -267.24 -86.48 -22.33 68.79 448.47

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 269.7218 10.4390 25.838 \< 2e-16 ***meta_sed_26_hills\$depth -4.0058 0.7688 -5.211 2.94e-07***

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 122.3 on 427 degrees of freedom Multiple R-squared: 0.05978, Adjusted R-squared: 0.05758 F-statistic: 27.15 on 1 and 427 DF, p-value: 2.936e-07

GAM RESULTS Family: gaussian Link function: identity

Formula: y \~ s(x, k = 3)

Parametric coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 2268.14 26.76 84.77 \<2e-16 \*\*\*

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms: edf Ref.df F p-value\
s(x) 1 1 76.25 \<2e-16 \*\*\*

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) = 0.15 Deviance explained = 15.2% GCV = 3.0854e+05 Scale est. = 3.071e+05 n = 429

## 5. Top phyla and families

### a. Caluclate top phyla

went back and changed to Phylum without the kingdom and phylum pasted together, since tax_glom knows how to keep those separate

```{r}
require(stringr)
require(pivottabler)
require(reshape)

#subset samples less than or equal to 26
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)

#Change the Phylum column to include kingdom as well
#tax_tmp<-as.data.frame(ps_tr_26@tax_table@.Data)
#tax_tmp$Phylum_only<-tax_tmp$Phylum
#tax_tmp$Phylum<-paste(tax_tmp$Kingdom,tax_tmp$Phylum,sep="@")
#tax_tmp[tax_tmp$Phylum=="Archaea@Unknown_Order",]$Phylum<-"Archaea@Unassigned"
#ps_tr_26@tax_table@.Data<-as.matrix(tax_tmp)
ps_tr_26_phy<-tax_glom(ps_tr_26, taxrank = "Phylum",NArm = FALSE)
ps_melt<-psmelt(ps_tr_26_phy)
#write.csv(ps_melt, "allphyla_29Sept2022.csv")
ps_melt<-read.csv("allphyla_29Sept2022.csv", header=T, row.names=1)

#which phyla go over a threshold on AVERAGE for each bin depth
    pt <- PivotTable$new()
    pt$addData(ps_melt)
    pt$addRowDataGroups("bin_depth") 
    pt$addColumnDataGroups("Phylum") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    
    
    phy_avg_abund <- pt$asDataFrame()
    phy_avg_abund$bin_depth<-rownames(phy_avg_abund)
    phy_avg_abund<-phy_avg_abund[-which(phy_avg_abund$bin_depth=="Total"),]
    phy_avg_abund$Total<-NULL
    phy_avg_abund$bin_depth <- as.numeric(phy_avg_abund$bin_depth)
    phy_avg_abund <- melt(phy_avg_abund, id.vars = "bin_depth")
    
    rm(pt)
    
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.05),]$variable)))
#9
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.1),]$variable)))
#4
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.08),]$variable)))
#4
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.075),]$variable)))
#6
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.07),]$variable)))
#8


subphy<-unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.07),]$variable))
### this is the one we used! 
# [1] "Bacteroidetes"                      "Chloroflexi"                       
# [3] "Cyanobacteria"                      "Euryarchaeota"                     
# [5] "Miscellaneous_Crenarchaeotic_Group" "Planctomycetes"                    
# [7] "Proteobacteria"                     "Verrucomicrobia" 
#same if we added the domain in the string
# [1] "Archaea@Euryarchaeota"                     
# [2] "Archaea@Miscellaneous_Crenarchaeotic_Group"
# [3] "Bacteria@Bacteroidetes"                    
# [4] "Bacteria@Chloroflexi"                      
# [5] "Bacteria@Cyanobacteria"                    
# [6] "Bacteria@Planctomycetes"                   
# [7] "Bacteria@Proteobacteria"                   
# [8] "Bacteria@Verrucomicrobia"  
#same as before, updated 6 Sept 2022, checked again sept 27

# what percent are these 8 phyla in the entire dataset?

length(unique(ps_melt$samp_names))
#429 samples
sum(ps_melt[ps_melt$Phylum%in%subphy,]$Abundance)/length(unique(ps_melt$samp_names))
#[1] 0.6392306 - 6 sept 2022 and again 29 sept 2022

```

### b. Family abundances

redoing not on Teton

```{r}
require(stringr)
require(pivottabler)
require(reshape)
require(phyloseq)
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

#only taxa <= 26 cm
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)

#subset by the top 8 phyla
subphy<-c("Euryarchaeota" , "Miscellaneous_Crenarchaeotic_Group"
,"Bacteroidetes" ,"Chloroflexi"                      
, "Cyanobacteria", "Planctomycetes"                   
,"Proteobacteria" ,"Verrucomicrobia")
ps_tr_26_subphy<-subset_taxa(ps_tr_26, Phylum%in%subphy)


#change taxonomy that is unknown
tax_tmp<-as.data.frame(ps_tr_26_subphy@tax_table@.Data)
row_names<-rownames(tax_tmp)

tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Order", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Family", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Phylum", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Class", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("unidentified", "Unassigned", x)}))
#make a string with all of the assignments up to Family
rownames(tax_tmp)<-row_names
ps_tr_26_subphy@tax_table@.Data<-as.matrix(tax_tmp)
length(unique(ps_tr_26_subphy@tax_table[,5]))
#[1] 235
temp2<-as.data.frame(ps_tr_26_subphy@tax_table@.Data)

temp2$alltax_tofam<-paste(temp2$Kingdom,temp2$Phylum,temp2$Class,temp2$Order,temp2$Family,sep="@")
length(unique(temp2$alltax_tofam))
#391
#melt<-psmelt(ps_tr_26_subphy)

ps_tr_26_fam<-tax_glom(ps_tr_26_subphy, taxrank = "Family",NArm = FALSE)
length(unique(rownames(ps_tr_26_fam@tax_table)))
#391
length(unique(ps_tr_26_fam@tax_table[,5]))
#235 - so it correctly conglomerated by family with unique phyla-order

melt<-psmelt(ps_tr_26_fam)
#write.csv(melt, "family_abundances_27sept2022.csv")

```

### i. calculate top 10 families in each bin depth

```{r}
require(pivottabler)
require(reshape)
fam<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/family_abundances_27sept2022.csv",header=T, row.names=1)

fam$alltax_tofam<-paste(fam$Kingdom,fam$Phylum,fam$Class,fam$Order,fam$Family,sep="@")

#which family has a relative abundance greater than 2% on average in any in depth less than or equal to 26 cm
    pt <- PivotTable$new()
    pt$addData(fam)
    pt$addColumnDataGroups("alltax_tofam")
    pt$addRowDataGroups("bin_depth")
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance, na.rm=T)")
    pt$renderPivot()
    pt_df <- pt$asDataFrame()
    pt_df$Total<-NULL
    pt_df<-pt_df[-which(rownames(pt_df)=="Total"),] 
    
    data_long <- as.data.frame(pt_df)    # Reshape data from wide to long
    data_long$bin_depth <- as.numeric(rownames(data_long))
    data_long <- melt(data_long, id.vars = "bin_depth")
    data_long <-data_long[order(data_long$value, decreasing=T),]
        
    fam_GTX<-data_long[which(data_long$value>0.02),]
    length(unique(as.character(fam_GTX$variable))) #16
   families<-sort(unique(as.character(fam_GTX$variable)))
#  [1] "Archaea@Euryarchaeota@Thermoplasmata@Thermoplasmatales@Marine_Benthic_Group_D_and_DHVEG-1"
#  [2] "Archaea@Miscellaneous_Crenarchaeotic_Group@Unassigned@Unassigned@Unassigned"              
#  [3] "Bacteria@Bacteroidetes@Bacteroidetes_vadinHA17@Unassigned@Unassigned"                     
#  [4] "Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"                         
#  [5] "Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned"                                    
#  [6] "Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned"                                        
#  [7] "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned"                                    
#  [8] "Bacteria@Cyanobacteria@Cyanobacteria@SubsectionI@FamilyI"                                 
#  [9] "Bacteria@Cyanobacteria@Cyanobacteria@SubsectionIII@FamilyI"                               
# [10] "Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned"                                   
# [11] "Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned"                           
# [12] "Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned"                         
# [13] "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae"              
# [14] "Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned"                           
# [15] "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae"            
# [16] "Bacteria@Verrucomicrobia@OPB35_soil_group@Unassigned@Unassigned"                          
    
#what percentage are these 16 families in the top 8 phyla
sum(fam[fam$alltax_tofam%in%families,]$Abundance, na.rm=T)/sum(fam$Abundance, na.rm=T) 
 #0.620541 -27 sept 2022

#what percent are these 16 families in the entire dataset
sum(fam[fam$alltax_tofam%in%families,]$Abundance, na.rm=T)/429 
#0.3966688 -27 sept 2022
```

### ii. SFig6 plot the top 17 families

this is where I looked to see which phyla should be split into families. All 17 of these families have greater than 2% relative abundance across the dataset.

```{r}
require(stringr)
require(RColorBrewer)
require(pivottabler)
fam<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/family_abundances_27sept2022.csv",header=T, row.names=1)

fam$Phylum <- gsub("Miscellaneous_Crenarchaeotic_Group", "Bathyarchaeota",  fam$Phylum)
fam$Phylum <- gsub("Chloroflexi", "Chloroflexota", fam$Phylum)
fam$Phylum <- gsub("Bacteroidetes", "Bacteroidota", fam$Phylum)
fam$Class <- gsub("Bacteroidetes", "Bacteroidota", fam$Class)
fam$Family <- gsub("Marine_Benthic_Group_D_and_DHVEG-1", "MBG-D_and_DHVEG-1", fam$Family)

fam$alltax_tofam<-paste(fam$Kingdom,fam$Phylum,fam$Class,fam$Order,fam$Family,sep="@")

 families<-c("Archaea@Euryarchaeota@Thermoplasmata@Thermoplasmatales@Marine_Benthic_Group_D_and_DHVEG-1"
,"Archaea@Miscellaneous_Crenarchaeotic_Group@Unassigned@Unassigned@Unassigned"              
,"Bacteria@Bacteroidetes@Bacteroidetes_vadinHA17@Unassigned@Unassigned"                     
,"Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"                         
,"Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned"                                    
,"Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned"                                        
, "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned"                                    
,"Bacteria@Cyanobacteria@Cyanobacteria@SubsectionI@FamilyI"                                 
, "Bacteria@Cyanobacteria@Cyanobacteria@SubsectionIII@FamilyI"                               
, "Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned"                                   
, "Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned"                           
, "Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned"                         
, "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae"              
,"Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned"                           
, "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae"            
,"Bacteria@Verrucomicrobia@OPB35_soil_group@Unassigned@Unassigned")   


 
families<-as.data.frame(families)
names(families)<-"alltax_tofam"
families$Kingdom<-str_split_fixed(families$alltax_tofam,"@",5)[,1]
families$Phylum<-str_split_fixed(families$alltax_tofam,"@",5)[,2]
families$Class<-str_split_fixed(families$alltax_tofam,"@",5)[,3]
families$Order<-str_split_fixed(families$alltax_tofam,"@",5)[,4]
families$Family<-str_split_fixed(families$alltax_tofam,"@",5)[,5]

families$Phylum <- gsub("Miscellaneous_Crenarchaeotic_Group", "Bathyarchaeota",  families$Phylum)
families$Phylum <- gsub("Chloroflexi", "Chloroflexota", families$Phylum)
families$Phylum <- gsub("Bacteroidetes", "Bacteroidota", families$Phylum)
families$Class <- gsub("Bacteroidetes", "Bacteroidota", families$Class)
families$Family <- gsub("Marine_Benthic_Group_D_and_DHVEG-1", "MBG-D_and_DHVEG-1", families$Family)
families$alltax_tofam<-paste(families$Kingdom,families$Phylum,families$Class,families$Order,families$Family,sep="@")


unique(families$Phylum)

fam$kingphy<-paste(fam$Kingdom,fam$Phylum, sep=": ")
fam$kingphy<-as.factor(fam$kingphy)

fam$kinphycol<-PNWColors::pnw_palette("Bay",8)[fam$kingphy]


temp<-fam
temp<-temp[temp$alltax_tofam%in%families$alltax_tofam,]


w=1
l=1
pdf("Figures/SF6_FamilyRA_top16_diffscale_27Sept2022.pdf",height=15,width=25)
par(mfrow=c(4,4), mar=c(3,3,5,3))
for (w in 1:length(families$alltax_tofam)){
  temp2<-temp[temp$alltax_tofam==families$alltax_tofam[w],]
  lake_drives<-unique(temp2$lake_drive)
  plot(temp2$bin_depth,temp2$Abundance, xlim=c(0,26), main=paste(temp2$Kingdom[1],temp2$Phylum[1],temp2$Class[1],paste("\n",temp2$Order[1],sep=""),temp2$Family[1], sep=", "), cex.axis=1.5, cex.main=2 ,bty="n",col="white", type="l", ylab="Relative abundance (%)", xlab="Depth (cm)")
for(l in 1:length(lake_drives)){
  temp3<-temp2[temp2$lake_drive==lake_drives[l],]
  temp3<-temp3[order(temp3$bin_depth,decreasing = F),]
  temp3<-temp3[,names(temp3)%in%c("bin_depth","Abundance"),]
  temp3<-na.omit(temp3) # removing all NAs and connecting the lines 
  lines(temp3$bin_depth,temp3$Abundance, col="gray")
}
    pt <- PivotTable$new()
    pt$addData(temp2)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = F),]
    lines(meanabund$bin_depth,meanabund$Abundance,col=temp2$kinphycol[1], lwd=7)
}
dev.off()


```

### iii. combine phylum and family

```{r}
#only taxa <= 26 cm
allphy<-read.csv("allphyla_29Sept2022.csv", header=T, row.names=1)
allfam<-read.csv("family_abundances_27sept2022.csv",header=T, row.names=1)

#subset by the top 8 phyla
top8phy<-allphy[allphy$Phylum%in%c("Euryarchaeota" , "Miscellaneous_Crenarchaeotic_Group","Bacteroidetes" ,"Chloroflexi", "Cyanobacteria", "Planctomycetes","Proteobacteria" ,"Verrucomicrobia"),]

table(names(allfam)[1:56]==names(allphy)[1:56])# all of this is true 

#which phyla are NOT broken up
phyla_notbrokenup<-c("Euryarchaeota" ,"Miscellaneous_Crenarchaeotic_Group", "Bacteroidetes", "Cyanobacteria" ,"Verrucomicrobia")

set1<-top8phy[top8phy$Phylum%in%phyla_notbrokenup,]

#phyla broken up
allfam$tax<-paste(allfam$Kingdom,allfam$Phylum,allfam$Class, allfam$Order, allfam$Family, sep="@")

split_into_fam_nosum<-sort(c("Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned", "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae",
 "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae",
                             "Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae",
                             "Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned",
                             "Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned",
                             "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned"))
set2<-allfam[allfam$tax%in%split_into_fam_nosum,]
table(sort(unique(set2$tax))==sort(split_into_fam_nosum))

# #Ones to sum#
# # 1. Proteobacteria
# # 2. Chloroflexi
# # 3. Planctomycetes other 
# # 4. Planctomycetes - Phycisphaerae (class) 
#                              "Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned"
#                              "Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned"
#                              "Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned"

# 1. Proteobacteria - other
sub<-allfam[allfam$Phylum=="Proteobacteria",]
sub2<-sub[sub$tax!="Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned" & sub$tax!="Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae",]

#this is summing all the rest of the Proteobacteria families for each depth and lake drive
df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Proteobacteria@Undifferentiated_Proteobacteria")
    df<-rbind(df,tempdf)
}

set3<-df


# 2. Chloroflexi - other
sub<-allfam[allfam$Phylum=="Chloroflexi",]
sub2<-sub[sub$tax!="Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae" & sub$tax!="Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned" & sub$tax!="Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned"  & sub$tax!="Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned",]

df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Undifferentiated_Chloroflexi")
    df<-rbind(df,tempdf)
}

set3<-rbind(set3,df)


# 3. Planctomycetes - class Phycisphaerae
sub<-allfam[allfam$Phylum=="Planctomycetes",]
sub2<-sub[sub$tax=="Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned" | sub$tax=="Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned" | sub$tax=="Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned",]

df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Planctomycetes@Phycisphaerae")
    df<-rbind(df,tempdf)
}

set3<-rbind(set3,df)

# 4. Planctomycetes - other
sub<-allfam[allfam$Phylum=="Planctomycetes",]
sub2<-sub[sub$tax!="Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned" & sub$tax!="Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned" & sub$tax!="Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned" & sub$tax!="Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae",]

df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Undifferentiated_Planctomycetes")
    df<-rbind(df,tempdf)
}

set3<-rbind(set3,df)

#make set3 look like the rest of the allphy and allfam

forset3<-allphy
forset3$Abundance<-NULL
forset3$Phylum<-NULL
forset3$Kingdom<-NULL
forset3$OTU<-NULL
forset3<-unique(forset3)
table(sort(unique(set3$Sample))==sort(forset3$Sample)) #all true

set4<-merge(set3,forset3,by="Sample")
table(set4$Sample%in%set3$Sample)
table(set4$Abundance%in%set3$Abundance)
table(set4$tax%in%set3$tax)

names(set4)
names(set1)

set4$tax2<-set4$tax
set4$tax<-NULL
names(set4)[54]<-"tax"

set1$OTU<-NULL
set1$tax<-paste(set1$Kingdom,set1$Phylum, sep="@")
set1$Kingdom<-NULL
set1$Phylum<-NULL

set2$OTU<-NULL
set2$Kingdom<-NULL
set2$Phylum<-NULL
set2$Class<-NULL
set2$Order<-NULL
set2$Family<-NULL

table(names(set1)==names(set4)) #all true
table(names(set1)==names(set2)) #all true

allset<-rbind(set1,set2,set4) #combine set 1, 2, and 4 (not 3)

write.csv(allset, "Top_phyla_and_families_14Oct2022.csv")
```

### iv. setup for fig. 3

```{r}
require(vegan)
require(rioja)

load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
#sed char summary
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
OTU<-as.data.frame(ps_tr_26@otu_table@.Data)
meta_sed_26<-metadata[metadata$samp_names%in%names(OTU),]

hills<-read.delim("hillnumbers_norm_endpoint_6Sept2022.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")
meta_sed_26_hills$log_cn<-log(meta_sed_26_hills$cn)

depthbins<-sort(unique(meta_sed_26_hills$bin_depth),decreasing=F)

summary<-data.frame(bin_depth=NULL,
                    rich_mean=NULL,rich_sd=NULL,
                    shan_mean=NULL,shan_sd=NULL,
                    simp_mean=NULL,simp_sd=NULL,
                    c13_mean=NULL,c13_sd=NULL,
                    c_perc_mean=NULL,c_perc_sd=NULL,
                    n_mean=NULL,n_sd=NULL,
                    cn_mean=NULL,cn_sd=NULL)
i=1
for(i in 1:length(depthbins)){
  tmp<-meta_sed_26_hills[meta_sed_26_hills$bin_depth==depthbins[i],]
  tmp2<-data.frame(bin_depth=depthbins[i],
                   rich_mean=mean(tmp$Species_Richness,na.rm=T),rich_sd=sd(tmp$Species_Richness,na.rm=T),
                   shan_mean=mean(tmp$Shannon_Diversity,na.rm=T),shan_sd=sd(tmp$Shannon_Diversity,na.rm=T),
                   simp_mean=mean(tmp$Simpson_Dominance,na.rm=T),simp_sd=sd(tmp$Simpson_Dominance,na.rm=T),
                    c13_mean=mean(tmp$d_13_c,na.rm=T),c13_sd=sd(tmp$d_13_c,na.rm=T),
                    c_perc_mean=mean(tmp$c_perc,na.rm=T),c_perc_sd=sd(tmp$c_perc,na.rm=T),
                    n_mean=mean(tmp$n_perc,na.rm=T),n_sd=sd(tmp$n_perc,na.rm=T),
                   cn_mean=mean(tmp$cn,na.rm=T),cn_sd=sd(tmp$cn,na.rm=T))
  summary<-rbind(summary,tmp2)
}


# average CONISS across all samples
OTU<-as.data.frame(ps_tr@otu_table@.Data)
OTU<-t(OTU)

OTU_meta<-as.matrix(ps_tr@sam_data)
OTU_meta<-as.data.frame(OTU_meta)
OTU_meta$bin_depth<-as.numeric(OTU_meta$bin_depth)

#create a mean relative abundance of each ESV for each depth
mean_abundances<-data.frame()
i=1
centimeters<-c(0,2,4,6,8,10,12,14,16,18,20,22,24,26)
  sub<-OTU[rownames(OTU)%in%(OTU_meta[OTU_meta$bin_depth==centimeters[1] & is.na(OTU_meta$bin_depth)==FALSE,]$samp_names),]
  mean_abundances<-as.data.frame(colMeans(sub))
  colnames( mean_abundances)[1]<-centimeters[i]
for(i in 2:length(centimeters)){
  sub<-OTU[rownames(OTU)%in%(OTU_meta[OTU_meta$bin_depth==centimeters[i] & is.na(OTU_meta$bin_depth)==FALSE,]$samp_names),]
  means<-as.data.frame(colMeans(sub))
  colnames(means)[1]<-centimeters[i]
  mean_abundances<-cbind(mean_abundances,means)
}

tma<-as.matrix(t(mean_abundances))
dist<-vegdist(tma)
  clust<-chclust(dist)

domtax<-read.csv("Top_phyla_and_families_14Oct2022.csv",header=T,row.names=1)
lake_drives<-unique(domtax$lake_drive)
```

### v. Fig 3. Dominant Groups

top of plot

```{r}
require(pivottabler)
unique_tax<-c("Bacteria@Proteobacteria@Undifferentiated_Proteobacteria",
          "Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"   ,
                            "Archaea@Euryarchaeota", 
               "Archaea@Miscellaneous_Crenarchaeotic_Group",  
                        "Bacteria@Planctomycetes@Phycisphaerae" )

zone_colors<- c(rep('#018571',1),rep('#C4AD79',2),rep('#a6611a',2))
i=1
l=1
pdf("Figures/Phyla_family_lines_26cm_topcolumn_14Oct2022.pdf", width=13, height=4)
par(mfrow=c(1,13), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)
for(i in 1:(length(unique_tax))){
  temp<-domtax[domtax$tax==unique_tax[i],]
  temp<-temp[order(temp$bin_depth,decreasing=T),]
  pt <- PivotTable$new()
    pt$addData(temp)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = T),]
    plot(temp$Abundance,temp$bin_depth,col="white",ylim=c(26,0),xlim=c(0,max(meanabund$Abundance)*1.8),bty="n",ylab="",xlab="",yaxt="n",cex.axis=1.6)

    for(l in 1:length(lake_drives)){
  temp2<-temp[temp$lake_drive==lake_drives[l],]  
  temp2<-temp2[order(temp2$bin_depth,decreasing = F),]
  lines(temp2$Abundance,temp2$bin_depth, col="gray")}
    lines(meanabund$Abundance,meanabund$bin_depth, col=zone_colors[i],lwd=4)
}


seq<-seq(2,14,2)
lab<-c("Species\nrichness","Shannon\ndiversity","Simpson\ndiversity","d13C", "%C","%N","C:N")
lines<-c(rep(4.5,3), rep(3.5,4))
sed_col<-c(rep("#607B8B",3), rep("#616161",4))
i=1

par(mar=c(6,0.5,1,1), xpd=TRUE)

  sum_temp<-summary[,c(1,seq[i],seq[i]+1)]
  sum_temp$sdp<-sum_temp[,2]+sum_temp[,3]
  sum_temp$sdm<-sum_temp[,2]-sum_temp[,3]
  plot(sum_temp[,2],sum_temp$bin_depth,ylim=c(26,0),xlim=c(min(sum_temp$sdm),max(sum_temp$sdp)),type="l",bty="n", yaxt="n", ylab="",col="white",lwd=2, xlab="", cex.axis=1.6)
    mtext(lab[i],side=1,line=lines[i], cex=1.1)
polygon(x = c(sum_temp$sdp,rev(sum_temp$sdm)),  # X-Coordinates of polygon 
        y = c(sum_temp$bin_depth, rev(sum_temp$bin_depth)),    # Y-Coordinates of polygon
        col = paste(sed_col[i],"50",sep=""), 
        border=paste(sed_col[i],"50",sep="")) 
lines(sum_temp[,2],sum_temp$bin_depth, col=sed_col[i],lwd=3)
  dev.off()
  
pdf("Figures/Phyla_family_lines_26cm_topcolumn_2nd_14Oct2022.pdf", width=13, height=4)  
seq<-seq(2,14,2)
lab<-c("Species\nrichness","Shannon\ndiversity","Simpson\ndiversity","d13C", "%C","%N","C:N")
lines<-c(rep(4.5,3), rep(3.5,4))
sed_col<-c(rep("#607B8B",3), rep("#616161",4))
i=2


par(mfrow=c(1,13), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)

for(i in 2:length(seq)){
  sum_temp<-summary[,c(1,seq[i],seq[i]+1)]
  sum_temp$sdp<-sum_temp[,2]+sum_temp[,3]
  sum_temp$sdm<-sum_temp[,2]-sum_temp[,3]
  plot(sum_temp[,2],sum_temp$bin_depth,ylim=c(26,0),xlim=c(min(sum_temp$sdm),max(sum_temp$sdp)),type="l",bty="n", yaxt="n", ylab="",col="white",lwd=2, xlab="", cex.axis=1.6)
    mtext(lab[i],side=1,line=lines[i], cex=1.1)
polygon(x = c(sum_temp$sdp,rev(sum_temp$sdm)),  # X-Coordinates of polygon 
        y = c(sum_temp$bin_depth, rev(sum_temp$bin_depth)),    # Y-Coordinates of polygon
        col = paste(sed_col[i],"50",sep=""), 
        border=paste(sed_col[i],"50",sep="")) 
lines(sum_temp[,2],sum_temp$bin_depth, col=sed_col[i],lwd=3)
}
 dev.off()
   
```

bottom of plot

```{r}
zone_colors<- c(rep('#018571',5),rep('#C4AD79',3),rep('#a6611a',3))
unique_tax <-c(     "Bacteria@Cyanobacteria", 
                    "Bacteria@Bacteroidetes", 
               "Bacteria@Verrucomicrobia" ,
               "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae",
               "Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned", 
               "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae" ,
              "Bacteria@Undifferentiated_Planctomycetes",
               "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned",
                "Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned",
               "Bacteria@Undifferentiated_Chloroflexi"     ,
               "Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned")

table(unique_tax%in%domtax$tax)

pdf("Figures/Phyla_family_lines_26cm_bottomcolumn_14Oct2022.pdf", width=13, height=4)
par(mfrow=c(1,13), mar=c(5.5,6,1.5,0))
plot( 0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(5.5,0.5,1.5,1), xpd=TRUE)
for(i in 1:(length(unique_tax))){
  temp<-domtax[domtax$tax==unique_tax[i],]
  temp<-temp[order(temp$bin_depth,decreasing=T),]
  pt <- PivotTable$new()
    pt$addData(temp)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = T),]
    plot(temp$Abundance,temp$bin_depth,col="white",ylim=c(26,0),xlim=c(0,max(meanabund$Abundance)*1.8),bty="n",ylab="",xlab="",yaxt="n",cex.axis=1.6)

    for(l in 1:length(lake_drives)){
  temp2<-temp[temp$lake_drive==lake_drives[l],]  
  temp2<-temp2[order(temp2$bin_depth,decreasing = F),]
  lines(temp2$Abundance,temp2$bin_depth, col="gray")}
  if(meanabund$Abundance>0.05){points(((max(meanabund$Abundance)*1.8)/2),-1, pch=8, lwd=1.5,cex=2)}
    lines(meanabund$Abundance,meanabund$bin_depth, col=zone_colors[i],lwd=4)
}

par(mar=c(5.5,2,1.5,1))
 plot(clust, xvar=as.numeric(clust[["labels"]]),ylim=c(0,3.2), xaxt="n",hang=-1, cex=1.6, cex.lab=1.6, cex.main=1.8, horiz=TRUE, x.rev=TRUE) 
  mtext("Distance",side=1,line=3, cex=1.1)
  mtext("CONISS", side=3, line=0, cex=1.1)
  axis(side=1,at=c(0,1,2,3),labels=c(0,1,2,3), cex.axis=1.6)
dev.off()

```

## 6. NMDS

```{r}
require(colorspace)
require(vegan)
ps_table <- data.frame(otu_table(ps_tr))
ps_table <-t(ps_table)
sd_sed <- data.frame(sample_data(ps_tr))

ord_baseR<-metaMDS(ps_table, distance = "bray")
# Ran 6 Sept 2022
# Run 0 stress 0.1711495 
# Run 1 stress 0.195905 
# Run 2 stress 0.2022131 
# Run 3 stress 0.192464 
# Run 4 stress 0.1985516 
# Run 5 stress 0.1963562 
# Run 6 stress 0.202012 
# Run 7 stress 0.2018219 
# Run 8 stress 0.189256 
# Run 9 stress 0.1951161 
# Run 10 stress 0.187484 
# Run 11 stress 0.1948257 
# Run 12 stress 0.1979875 
# Run 13 stress 0.1976616 
# Run 14 stress 0.4193573 
# Run 15 stress 0.1938638 
# Run 16 stress 0.1873383 
# Run 17 stress 0.1927334 
# Run 18 stress 0.200088 
# Run 19 stress 0.1954923 
# Run 20 stress 0.1938701 
# *** No convergence -- monoMDS stopping criteria:
#      2: no. of iterations >= maxit
#     14: stress ratio > sratmax
#      4: scale factor of the gradient < sfgrmin

fig<-ordiplot(ord_baseR, type="points")

NMDS1<-fig$sites[,1]
NMDS2<-fig$sites[,2]

fig_sites<-as.data.frame(fig[["sites"]])
table(fig_sites$NMDS2==NMDS2) #same for both NMDS1 and NMDS2


fig_sites$samp_names<-rownames(fig_sites)
fig_sites$samp_names<-gsub("X","",as.character(fig_sites$samp_names))
table(names(as.data.frame(ps_tr@otu_table))==fig_sites$samp_names) # all true 

#add in metadata to the dataframe with the NMDS
ord_df<-merge(fig_sites,sd_sed, by="samp_names")

#8c510a darker brown
#a6611a # dark brown
#dfc27d
#80cdc1
#018571 # dark teal

darkertan<-darken("#dfc27d", 0.1)
#C4AD79

ord_df$zone_col<-rep('#01857180',nrow(ord_df))
ord_df[ord_df$bin_depth%in%c(6,8,10,12),]$zone_col<-"#C4AD7980"
ord_df[ord_df$bin_depth%in%c(14,16,18,20,22,24,26),]$zone_col<-'#a6611a80'
ord_df[ord_df$depth>26,]$zone_col<-'#00000080' #black

zone_colors<- c('#018571','#dfc27d','#a6611a','#000000')

sd_sed$bottom_water_temp<-sd_sed$water_sample_t_bot
sd_sed$lake_depth<-sd_sed$max_lake_depth
sd_sed$sediment_depth<-sd_sed$depth

#if you want first axis reversed!
ord_df$NMDS1<-(ord_df$NMDS1*(-1))

ord.fit <- envfit(ord_baseR ~  sediment_depth + lake_depth + bottom_water_temp,  data = sd_sed,  perm = 1000, na.rm = TRUE)
ord.fit[["vectors"]][["arrows"]][1,1]<-(ord.fit[["vectors"]][["arrows"]][1,1]*(-1))

#save.image("NMDS_6Sept2022.Rdata")
```

### ii. plot NMDS

```{r}
load("NMDS_6Sept2022.Rdata")

pdf("NMDS_6Sept2022_legend.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
```

### iii. plot with additional variables (just to check)

```{r}
ord.fit <- envfit(ord_baseR ~  sediment_depth + lake_depth + bottom_water_temp +water_sample_ph_surf +water_sample_ph_bot+water_sample_do_bot + water_sample_t_surf + water_sample_do_surf + elevation_meters,  data = sd_sed,  perm = 1000, na.rm = TRUE)
ord.fit[["vectors"]][["arrows"]][1,1]<-(ord.fit[["vectors"]][["arrows"]][1,1]*(-1))

pdf("NMDS_allvar_13Sept2022.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit,p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()


# which variables are correlated?
meta_sub<-metadata[,names(metadata)%in%c("elevation_meters","water_sample_ph_surf", "water_sample_do_surf", "water_sample_t_surf", "water_sample_ph_bot", "water_sample_do_bot", "water_sample_t_bot", "max_lake_depth" , "latitude"),]
meta_sub<-unique(meta_sub)

cor_results <- cor(meta_sub[1:length(names(meta_sub))], method = "spearman")
x <- which(cor_results > 0.6, arr.ind=TRUE)
x[] <- colnames(cor_results)[x]
rownames(x) <- NULL
x
#only surface water temperature and latitude 

```

### iv. Adonis testing - done 6 Sept 2022

```{r}
require(vegan)
require(parallelDist)
load("NMDS_6Sept2022.Rdata")

ps_table <- data.frame(otu_table(ps_tr)) 
names(ps_table)<-colnames(ps_tr@otu_table)
ps_table<-as.data.frame(t(ps_table))

sd <- data.frame(sample_data(ps_tr)) 
#JVE - first adding surface water temperature for round lake bottom water temperature since the lake it 1.5 m deep
sd[sd$lake_id==40,]$water_sample_t_bot<-20.1


#subset the sd to just the variables we are looking at
sd <- sd[names(sd)%in%c("water_sample_t_bot","max_lake_depth","depth")]
table(is.na(sd)) #make sure there are no NAs

# you must remove any samples with NAs prior to running this analysis 
table(rownames(ps_table)%in%rownames(sd))
adonis_table<-data.matrix(ps_table) 
adonis_dist<-parDist(adonis_table, method = "bray") 
adonis(adonis_dist ~ depth+  water_sample_t_bot + max_lake_depth, data = sd)  
```

Call: adonis(formula = adonis_dist \~ depth + water_sample_t\_bot + max_lake_depth, data = sd)

Permutation: free Number of permutations: 999

Terms added sequentially (first to last)

                    Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)    

depth 1 9.707 9.7074 27.915 0.05236 0.001 ***water_sample_t\_bot 1 7.267 7.2669 20.897 0.03920 0.001*** max_lake_depth 1 3.583 3.5826 10.302 0.01932 0.001 \*\*\* Residuals 474 164.835 0.3478 0.88912\
Total 477 185.392 1.00000

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### iv. Adonis testing - done 28 Sept 2022

```{r}
require(vegan)
require(parallelDist)
load("NMDS_6Sept2022.Rdata")

ps_table <- data.frame(otu_table(ps_tr)) 
names(ps_table)<-colnames(ps_tr@otu_table)
ps_table<-as.data.frame(t(ps_table))

sd <- data.frame(sample_data(ps_tr)) 
#JVE - first adding surface water temperature for round lake bottom water temperature since the lake it 1.5 m deep
sd[sd$lake_id==40,]$water_sample_t_bot<-20.1


sd$zone<-rep("A",nrow(sd))
sd[sd$bin_depth%in%c(6,8,10,12),]$zone<-"B"
sd[sd$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
sd[sd$bin_depth==100,]$zone<-"D"
sd$zone<-as.factor(sd$zone)

#subset the sd to just the variables we are looking at
sd <- sd[names(sd)%in%c("water_sample_t_bot","max_lake_depth","zone")]
table(is.na(sd)) #make sure there are no NAs

# you must remove any samples with NAs prior to running this analysis 
table(rownames(ps_table)%in%rownames(sd))
adonis_table<-data.matrix(ps_table) 
adonis_dist<-parDist(adonis_table, method = "bray") 
adonis(adonis_dist ~ zone+  water_sample_t_bot + max_lake_depth, data = sd)  
```

Call: adonis(formula = adonis_dist \~ zone + water_sample_t\_bot + max_lake_depth, data = sd)

Permutation: free Number of permutations: 999

Terms added sequentially (first to last)

                    Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)    

zone 3 13.452 4.4840 13.159 0.07256 0.001 ***water_sample_t\_bot 1 7.384 7.3843 21.670 0.03983 0.001*** max_lake_depth 1 3.719 3.7186 10.913 0.02006 0.001 \*\*\* Residuals 472 160.837 0.3408 0.86755\
Total 477 185.392 1.00000

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### iiv. plot each zone separately

```{r}
#ord_df_backup<-ord_df
#temp<-ord_df #reversed the whole dataset to temp to subset from
zone<-c("replacement","surface","depauperate","deep_depauperate")
zone_colors<-unique(temp$zone_col)
i=1
for(i in 1:4){
pdf(paste("Figures/NMDS_",zone[i],"_May2022.pdf", sep=""), width=5, height=4.5)
ord_df<-temp[temp$zone_col==zone_colors[i],]
    par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
#plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
}
```

### ix. points by mountain range

```{r}
ord_df<-ord_df_backup
ord_df$mtn_pch<-rep(NA,nrow(ord_df))
ord_df[ord_df$mountain_range=="Snowy",]$mtn_pch<-21 #circle
ord_df[ord_df$mountain_range=="Beartooth",]$mtn_pch<-22 #square
ord_df[ord_df$mountain_range=="Wind River",]$mtn_pch<-23 #diamond
ord_df[ord_df$mountain_range=="Bighorn",]$mtn_pch<-24 #triangle
ord_df$mtn_pch<-as.numeric(ord_df$mtn_pch)

pdf("Figures/NMDS_points_29Apr2022.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col="black",bg=ord_df$zone_col, pch=ord_df$mtn_pch, xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
#legend("top",inset=c(0.3,-0.13),xpd=TRUE,bty ="n",legend = c("                    ","                       ","                       ","                       "), col=zone_colors, pch=16, pt.cex = 3,cex=1,ncol=4)
#plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()

```

## 7. Pairwise comparisons

```{r}
require(vegan)
require(geosphere)
require(simba)

#used transformed data
OTU<-as.data.frame(ps_tr@otu_table@.Data)
OTU<-t(OTU)

#calculate Bray-Curtis similarity between all samples
comm.dist <- 1 - vegdist(OTU)

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),rownames(OTU))),]
table(rownames(OTU)==rownames(metadata)) #all true

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
dist_m_output<-as.dist(dist_m_output)


#transform all distance matrices into db format using liste function in simba
coord.dist.ls<-liste(dist_m_output, entry="geo_dist")
comm.dist.ls<-liste(comm.dist, entry="comm")
#check the names of these match

table(coord.dist.ls[,1]==comm.dist.ls[,1]) #both true
table(coord.dist.ls[,2]==comm.dist.ls[,2])

#create df with similarity of community and distance
comps<-data.frame(comm.dist.ls)
comps$geo_dist<-coord.dist.ls$geo_dist
names(comps)<-c("s1_samp_names","s2_samp_names","comm","geo_dist")
comps$s1_samp_names<-as.character(comps$s1_samp_names)
comps$s2_samp_names<-as.character(comps$s2_samp_names)

#add zone into metadata
metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

#merge in metadata to table
metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

comps<-merge(comps, metadata_s1, by="s1_samp_names")
comps<-merge(comps, metadata_s2, by="s2_samp_names")

#remove metadata notes
comps$s1_notes<-NULL
comps$s2_notes<-NULL
comps$s1_Notes_sed_water_wt<-NULL
comps$s2_Notes_sed_water_wt<-NULL
comps$s1_Notes_carbon_nitrogen<-NULL
comps$s2_Notes_carbon_nitrogen<-NULL
comps$s1_Notes._lake_sed_pH<-NULL
comps$s2_Notes._lake_sed_pH<-NULL

# add in centimeters into the distance
comps$abs_cm<-rep(NA, nrow(comps))
comps$s1_depth<-as.numeric(comps$s1_depth)
comps$s2_depth<-as.numeric(comps$s2_depth)
i=1
for(i in 1:nrow(comps)){
  ifelse(comps$s1_lake_drive[i]==comps$s2_lake_drive[i],comps$abs_cm[i]<-abs(comps$s2_depth[i]-comps$s1_depth[i]),NA)
}

#this one takes forever and adds 1 meter to cores within the same lake
comps$dist_1m_forsamelake<-rep(NA,nrow(comps))
i=1
for(i in 1:nrow(comps)){
 if(comps$s1_lake_name[i]==comps$s2_lake_name[i] & comps$s1_lake_drive[i] != comps$s2_lake_drive[i]){comps$dist_1m_forsamelake[i]<-comps$geo_dist[i]+1}else{comps$dist_1m_forsamelake[i]<-comps$geo_dist[i]}
}


#one sample with 2 replicates, so put in 0.001 for the dist_cm_core column
comps<-comps[order(comps$abs_cm,decreasing=F),]
which(names(comps)=="abs_cm")
comps[1,which(names(comps)=="abs_cm")]<-0.001
#check
comps[1,]

rm(comm.dist.ls)
rm(coord.dist.ls)
rm(metadata_s1)
rm(metadata_s2)
rm(OTU)
rm(xy)
rm(comm.dist)
rm(dist_m_output)
rm(i)
rm(tps_norm)
#write.csv(comps, "pairwise_comparisons_13Sept2022.csv")
```

### i. add env dist

```{r}
require(cluster)
#load data and comparison file
comps<-read.csv("pairwise_comparisons_13Sept2022.csv")

#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

daisy.mat <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))
env_distance <- simba::liste(daisy.mat, entry = "env_dist")
names(env_distance)<-c("s1_samp_names","s2_samp_names","env_dist")

comps$s1_samp_names<-as.character(comps$s1_samp_names)
comps$s2_samp_names<-as.character(comps$s2_samp_names)

env_distance$s1_samp_names<-as.character(env_distance$s1_samp_names)
env_distance$s2_samp_names<-as.character(env_distance$s2_samp_names)

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
env_distance<-env_distance[order(env_distance$s1_samp_names,env_distance$s2_samp_names),]

table(comps$s1_samp_names==env_distance$s1_samp_names)
table(comps$s2_samp_names==env_distance$s2_samp_names)

comps$env_dist<-env_distance$env_dist
rm(env_distance)

#overwrite file
#write.csv(comps, "pairwise_comparisons_13Sept2022.csv")
```

### iii. Fig. 5 env and geodist decay

Absolute sediment distance, only looking at \<26 samples

```{r}
require(mgcv)
comps<-read.csv("pairwise_comparisons_13Sept2022.csv",row.names=1, header=T)
comps$geo_dist_km<-comps$geo_dist/1000 # add a column for geographic distance in km and re-write csv
#write.csv(comps, "pairwise_comparisons_13Sept2022.csv")
```

Start here

```{r}
require(mgcv)
require(patchwork)
require(ggplot2)
comps<-read.csv("pairwise_comparisons_14Sept2022.csv",row.names=1, header=T)
```

summary figure (horizons not broken out)

```{r}
model_result<-list()
theme_comm<-theme_classic()+
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
              axis.text=element_text(size=13), #change font size of axis text
              axis.title=element_text(size=15), #change font size of axis titles  
              axis.title.x = element_text(vjust=-0.3),
              axis.title.y = element_text(margin = margin(r = 10)))
ggplt<-list()
#sediment distance from an individual core
sub<-comps[is.na(comps$abs_cm)==FALSE,]
sub<-sub[sub$s1_zone%in%c("A","B","C") & sub$s2_zone%in%c("A","B","C"),]
table(sub$s1_lake_drive==sub$s2_lake_drive)
#2081 comparisons (16 Feb 2023)


Data<-data.frame(x=sub$abs_cm,y=sub$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
model_result[[1]]<-summary(dat_gam)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

## plot
ggplt[[1]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                           col="black", alpha = 0.7) +
        geom_line(linewidth=1.5) + labs(x = "Sediment distance (cm)", y = "Community similarity (1-Bray)")+
        scale_x_continuous(breaks=seq(0,26,4))+
        ylim(0,0.8)+
        annotate("text", x=(26*0.95), y=0.8, label= "A", size=7)+
        annotate("text", x=(26*0.84), y=0.7,  label="paste(italic(R)^2,\"= 0.32\")", parse=T, size=5) +
        theme_comm


"paste(italic(R)^2,\"=0.11\")"
sub_bind<-NULL
#Geographic distance#
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        sub_bind<-rbind(sub_bind,sub)
}
        Data<-data.frame(x=sub_bind$geo_dist_km,y=sub_bind$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        model_result[[2]]<-summary(dat_gam)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        

ggplt[[2]]<-ggplot(data =Data , aes(x = x, y = fit)) +
        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                    col="black", alpha = 0.7) +
        geom_line(linewidth=1.5) + labs(x = "Distance (km)", y = "")+
        scale_x_continuous(breaks=seq(0,500,100))+
        annotate("text", x=(500*0.95), y=0.8, label= "B", size=7)+
        annotate("text", x=(500*0.82), y=0.7, label="paste(italic(R)^2,\"= 0.087\")", parse=T,  size=5) +
        ylim(0,0.8)+
        theme_comm  


sub_bind<-NULL
#Geographic distance#
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        sub_bind<-rbind(sub_bind,sub)
}
Data<-data.frame(x=sub_bind$env_dist,y=sub_bind$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
model_result[[3]]<-summary(dat_gam)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

   ggplt[[3]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       col="black", alpha = 0.7) +
   geom_line(linewidth=1.5) + labs(x = "Environmental dissimilarity", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           annotate("text", x=(0.73*0.95), y=0.8, label= "C", size=7)+
        annotate("text", x=(0.73*0.82), y=0.7, label="paste(italic(R)^2,\"= 0.175\")", parse=T, size=5) +

           ylim(0,0.8)+
           theme_comm


pdf("Figures/Fig5_simplifed_seddist_envdist_geodist_20Feb2023.pdf", height = 3.5, width= 11)
ggplt[[1]]+ ggplt[[2]] + ggplt[[3]] + plot_layout(ncol = 3)
dev.off()

```

```{r}
print("sediment distance, pval, rsq")
model_result[[1]]$s.pv
model_result[[1]]$r.sq

print("geographic distance, pval, rsq")
model_result[[2]]$s.pv
model_result[[2]]$r.sq

print("environmental distance, pval, rsq")
model_result[[3]]$s.pv
model_result[[3]]$r.sq



```

supplementary figure - GAMS with different horizons

```{r}
theme_comm<-theme_classic()+
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
              axis.text=element_text(size=13), #change font size of axis text
              axis.title=element_text(size=15), #change font size of axis titles  
              axis.title.x = element_text(vjust=-0.3),
              axis.title.y = element_text(margin = margin(r = 10)))
ggplt<-list()
#sediment distance from an individual core
sub<-comps[is.na(comps$abs_cm)==FALSE,]
sub<-sub[sub$s1_zone%in%c("A","B","C") & sub$s2_zone%in%c("A","B","C"),]
table(sub$s1_lake_drive==sub$s2_lake_drive)
#2081 comparisons (16 Feb 2023)


Data<-data.frame(x=sub$abs_cm,y=sub$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

## plot
ggplt[[1]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, alpha=0.3) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                            alpha = 0.3) +
        geom_line(linewidth=1.5) + labs(x = "Sediment distance (cm)", y = "Community similarity (1-Bray)")+
        scale_x_continuous(breaks=seq(0,26,4))+
        ylim(0,0.8)+
        theme_comm





#geographic distance
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        Data<-data.frame(x=sub$geo_dist_km,y=sub$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        
        if(i==1){     
                ggplt[[2]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "", y = "")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm
        }
        if(i==2){
                ggplt[[3]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "", y = "Community similarity (1-Bray)")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm
        }
        if(i==3){
                ggplt[[4]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "Distance (km)", y = "")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm  
        }}

#environmental dissimilarity
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        Data<-data.frame(x=sub$env_dist,y=sub$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        
if(i==1){     
        ggplt[[5]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                            alpha = 0.3) +
                geom_line(linewidth=1.5) + labs(x = "", y = "")+
               scale_x_continuous(breaks=seq(0,0.73,0.1))+
                ylim(0,0.8)+
                theme_comm
}
if(i==2){
   ggplt[[6]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       alpha = 0.3) +
   geom_line(linewidth=1.5) + labs(x = "", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           ylim(0,0.8)+
           theme_comm
}
if(i==3){
   ggplt[[7]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       alpha = 0.3) +
   geom_line(linewidth=1.5) + labs(x = "Environmental dissimilarity", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           ylim(0,0.8)+
           theme_comm
}}

pdf("Figures/Fig5_CommunityAssembly_BC_16Feb2023.pdf",height=7,width=7)
ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ ggplt[[7]]+ 
        plot_layout(ncol = 2, byrow = F)
dev.off()

pdf("Figures/Fig5_CommunityAssembly_A_16Feb2023.pdf",height=2.4,width=3.3)
ggplt[[1]]
dev.off()


```

### iv. add in BNTI and RCBRAY

```{bash}
rsync jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI/WyLakeMicrobes_env_30Aug2022_finalBNTI_abundwt_10888ESVs.RData /Users/jordanscheibe/Desktop

rsync jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI/WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData /Users/jordanscheibe/Desktop
 
```

```{r}
require(simba)
load("WyLakeMicrobes_env_30Aug2022_finalBNTI_abundwt_10888ESVs.RData")
comps<-read.csv("pairwise_comparisons_13Sept2022.csv",row.names=1, header=T)

dist_bnti<-as.dist(BNTI[["comdistnt.obs.z"]])
BNTI_dist<-liste(dist_bnti, entry="BNTI")
names(BNTI_dist)<-c("s1_samp_names","s2_samp_names","BNTI")
BNTI_dist$s1_samp_names<-as.character(BNTI_dist$s1_samp_names)
BNTI_dist$s2_samp_names<-as.character(BNTI_dist$s2_samp_names)

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
BNTI_dist<-BNTI_dist[order(BNTI_dist$s1_samp_names,BNTI_dist$s2_samp_names),]

table(comps$s1_samp_names==BNTI_dist$s1_samp_names)
table(comps$s2_samp_names==BNTI_dist$s2_samp_names)

comps$BNTI<-BNTI_dist$BNTI

hist(comps$BNTI, xlab="BNTI", main="Histogram of BNTI")
write.csv(comps, "pairwise_comparisons_bnti_backup_14Sept2022.csv")
rm(physeq)
rm(tree_dist)
rm(dist_bnti)
rm(BNTI)
rm(BNTI_dist)
```

```{r}
#add in RCBRAY to comps, from above
load("WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData")

rc.nulls<-RCbray_output
physeq<-phyloseq(otu_table(physeq),tax_table(physeq),sample_data(physeq))
physeq_t <- transform_sample_counts(physeq=physeq, function(x) x / sum(x))
OTUsREL<-t(as.data.frame(physeq@otu_table))
design<-physeq_t@sam_data

obs.bray <- as.matrix(vegdist(OTUsREL, method = "bray"))
site.compares <- expand.grid(site1 = 1:nrow(obs.bray), site2 = 1:nrow(obs.bray))
site.compares <- site.compares[-which(site.compares[,1] == site.compares[,2]),]
RC.bray <- matrix(NA, nrow = nrow(obs.bray), ncol = nrow(obs.bray))
for(row.i in 1:nrow(site.compares)){
  site1 <- site.compares[row.i,1]
  site2 <- site.compares[row.i,2]
  pairwise.null <- rc.nulls[site1,site2,]
  pairwise.bray <- obs.bray[site1,site2]
  num.greater <- sum(pairwise.null > pairwise.bray)
  num.ties <- sum(pairwise.null == pairwise.bray)
  val <- -1*((((1 * num.greater) + (0.5 * num.ties))/999 - 0.5) * 2)
  RC.bray[site1, site2] <- val
}
rownames(RC.bray) <- rownames(design)
colnames(RC.bray) <- rownames(design)
RC.bray.dist <- as.dist(RC.bray)
range(RC.bray.dist)
RC.bray_dist_df <- simba::liste(RC.bray.dist, entry = "RC_bray")
names(RC.bray_dist_df)<-c("s1_samp_names","s2_samp_names","RCbray")

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
RC.bray_dist_df<-RC.bray_dist_df[order(RC.bray_dist_df$s1_samp_names,RC.bray_dist_df$s2_samp_names),]

table(comps$s1_samp_names==RC.bray_dist_df$s1_samp_names)
table(comps$s2_samp_names==RC.bray_dist_df$s2_samp_names)

comps$RCbray<-RC.bray_dist_df$RCbray

```

### v. add in selection processes

```{r}
comps$process<-rep(NA, nrow(comps))
comps[comps$BNTI< (-2),]$process<-"Homogeneous selection"
comps[comps$BNTI> (2),]$process<-"Variable selection"
comps[comps$BNTI< (2) & comps$BNTI> (-2) & comps$RCbray > (0.95),]$process<-"Dispersal limitation"
comps[comps$BNTI< (2) & comps$BNTI> (-2) & comps$RCbray < (-0.95),]$process<-"Homogenizing dispersal"
comps[comps$BNTI< (2) & comps$BNTI> (-2) & comps$RCbray > (-0.95) & comps$RCbray < 0.95,]$process<-"Drift"
#write.csv(comps, "pairwise_comparisons_14Sept2022.csv")

```

### vi. stats community assembly processes - OLD

```{r}
table(comps$process)/nrow(comps)

#deterministic
  0.6871222687 + 0.2045297054
  
  Stochastic 
  1-(  0.6871222687 + 0.2045297054)
  0.0056665175 + 0.0002543793
  
```

Dispersal limitation Drift Homogeneous selection Homogenizing dispersal 0.1024271291 0.0056665175 0.2045297054 0.0002543793 Variable selection 0.6871222687 \### Fig 6 - Continuous Assembly Processes just like the decay relationships, I'm going to only look 26 cm then I'll also plot the 26+ ones as a supplementary figure

#### 0) RUN THIS FIRST

```{r}
comps_backup<-read.csv("pairwise_comparisons_14Sept2022.csv", header=T, row.names=1) # load this as a backup file and pull from there

comps_backup[comps_backup$process=="Homogeneous selection",]$process<-"B_Homogeneous selection"
comps_backup[comps_backup$process=="Variable selection",]$process<-"A_Variable selection"
comps_backup[comps_backup$process=="Dispersal limitation",]$process<-"E_Dispersal limitation"
comps_backup[comps_backup$process=="Homogenizing dispersal",]$process<-"D_Homogenizing dispersal"
comps_backup[comps_backup$process=="Drift",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

comps<-comps_backup
comps<-comps[comps$s1_zone%in%c("A","B","C") & comps$s2_zone%in%c("A","B","C"),]
```

#### 1) sediment depth

both figures (across sed depth all processes and the GAMs)

```{r}
require(Hmisc)


comps$d_val<-abs(comps$abs_cm)
comps<-comps[is.na(comps$d_val)==F,]
comps$d_val_group_breaks<-cut2(comps$d_val, g=10)
comps$d_val_group<-as.numeric(cut2(comps$d_val, g=10))
levels(comps$d_val_group_breaks)
break_labels<- c("[0, 2.5)" ,"[2.5, 5)" ,"[5, 7)", "[7, 9)" ,"[9, 11)" ,"[11, 13)"
,"[13, 15)" ,"[15, 20)" ,"[20, 26]")

tmp<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL)
proc<-unique(comps$process)
groups<-sort(unique(comps$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:5){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i]))
  }}


ggplt <- ggplot(tally,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_line() + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
        labels=break_labels) + xlab(label = "Sediment distance (cm)")+ ylab(label = "Proportion") + geom_line(size=2) +   labs(color="Assembly process")    +theme(axis.ticks.x = element_blank()) + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) + theme(axis.text.x = element_text(angle = 45,hjust=1))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=14), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) #change font size of legend title 
pdf("Figures/Fig6A_Continuous_EAP_sediment_distance_16Sept2022.pdf", height=4, width=8)
ggplt
dev.off()

unique(tally$n)
#341 311 274 241 208 180 149 213 164
```

```{r}

require(mgcv)
#Plot the sediment depth GAM for each process
pdf("Figures/Fig7_seddepth_GAM_16Sept2022.pdf", height=3, width=10)
par(mfrow=c(1,5),mar=c(6,4,2,1))
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="black", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels)),1), las=2, labels = break_labels)
title(xlab=c("","","Sediment distance (cm)","","")[i], cex.lab=2, line=4)
title(ylab=c("Proportion","","","","")[i], cex.lab=2)

Data<-data.frame(x=sub$d_val_group, y=sub$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=new_colors[i], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(new_colors[i], alpha.f = 0.40), border = NA)
        gam_res<-summary(Gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
}
dev.off()

```

#### 1.5) SFig 7 sediment distance with deep samples only

```{r}
comps<-comps_backup
comps$d_val<-comps$abs_cm
comps<-comps[is.na(comps$d_val)==F,]
comps<-comps[comps$d_val>26,] # 119 comparisons, so no many compared to the other ones

tmp2<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL)
proc<-unique(comps_backup$process)
for(p in 1:5){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2)))
  }
tally$group<-rep("deep", nrow(tally))

EAP_plot<-ggplot(tally, aes(x = group, y = percent , fill = process)) +
    geom_bar(stat="identity") + scale_fill_manual(values=new_colors,labels= c("Variable selection", "Homogeneous selection", "Drift", "Homogenizing dispersal", "Dispersal limitation" )) 
pdf("Figures/SFig7_deep_community_assembly.pdf", height=4, width=4)
EAP_plot
dev.off()
```

#### 2) environmental distance within each zone

old breaks "[0.0162,0.117)" "[0.1167,0.163)" "[0.1632,0.208)" "[0.2080,0.240)" "[0.2399,0.276)" "[0.2759,0.323)" "[0.3234,0.384)" [8] "[0.3835,0.454)" "[0.4543,0.540)" "[0.5399,0.722]"

determine breaks here

```{r}
require(stringr)
require(Hmisc)
comps$d_val<-comps$env_dist

#make breaks for zone A
comps_tmp<-comps[comps$s1_zone=="A" & comps$s2_zone=="A",]

comps_tmp<-comps_tmp[comps_tmp$d_val!=0,]
comps_tmp$d_val_group_breaks<-cut2(comps_tmp$d_val, g=10)
levels(comps_tmp$d_val_group_breaks)
breaks<-c(as.numeric(gsub("\\]","" ,gsub("\\[","",unlist(str_split(as.character(levels(comps_tmp$d_val_group_breaks)), pattern=",")))[c(seq(1, 20, 2),20)])))

#make sure first value is below the minimum env_distance measure that isn't 0
min(comps_backup[comps_backup$env_dist!=0,]$env_dist)
#[1] 0.01621537

# make a bottom break (0.016 - just below the lowest env_dist) that is above zero but below the first break
breaks<-c(0.016,breaks[2:11]) 

full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)

zone<-c("A","B","C")
j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

#breaks_labels<- levels(comps$d_val_group_breaks)
breaks_labels<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="env"))
  }
  }

full_output<-rbind(full_output,tally)
}
#check to make sure they add to one
range(full_output$n)
#[1]  171 2020 #11 jan 2023
mean(full_output$n)
#947.6364 #11 jan 2023


#write.csv(full_output,"Community_assembly_env_dist_summary_11Jan2023.csv")
```

redox (number of comparisons) 171 0.016 655 0.1167 616 0.1632 602 0.208 646 0.2399 628 0.2759 617 0.3234 634 0.3835 621 0.4543 626 0.5399 625 0.722 transition 315 0.016 1045 0.1167 1092 0.1632 909 0.208 929 0.2399 928 0.2759 1052 0.3234 996 0.3835 1028 0.4543 1030 0.5399 972 0.722 depauperate 821 0.016 1646 0.1167 2020 0.1632 1223 0.208 1511 0.2399 1414 0.2759 1579 0.3234 1122 0.3835 1238 0.4543 1133 0.5399 828 0.722

#### 3) geographic distance

11 Jan 2023, update by separate out cores in same lake reload 0) from above

```{r}
comps[comps$s1_lake_id==comps$s2_lake_id & comps$s1_lake_drive!=comps$s2_lake_drive,]$geo_dist_km<-0.001 #add one meter for cores in the same lake
comps$d_val<-comps$geo_dist_km
breaks<-c(seq(0,15,5), seq(100, 300, 200), seq(400,500,100))  ##nothing from  43.30254  174.6661
breaks<-c(0,0.1, breaks[2:8])
full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)


zone<-c("A","B","C")
j=1
for( j in 1:3) {
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

breaks_labels<- c("[0,0]",levels(cut(comps$d_val, breaks=breaks)))

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="geo"))
  }
  }
full_output<-rbind(full_output,tally)
#count the number of comparisons in each zone
}

range(full_output$n)
#[1]  75 2604 # 11 Jan 2023
mean(full_output$n)
#1158.222
print(breaks_labels)

#write.csv(full_output,"Community_assembly_geo_dist_summary_11Jan2023.csv")
```

"[0,0]" "(0,0.1]" "(0.1,5]" "(5,10]" "(10,15]" "(15,100]" "(100,300]" "(300,400]" "(400,500]"

Redox (number of comparisons in each bin) 96 0 75 0.1 705 5 558 10 583 15 757 100 1537 300 1136 400 994 500

Transition 191 0 124 0.1 944 5 810 10 844 15 1486 100 2604 300 1869 400 1424 500 Depauperate 390 0 431 0.1 1002 5 1168 10 1448 15 2461 100 2437 300 2599 400 2599 500

combine the two dataframes

```{r}
geo_dist_tally<-read.csv("Community_assembly_geo_dist_summary_11Jan2023.csv",header=T, row.names=1) #removed from folder after combined dataframe made
env_dist_tally<-read.csv("Community_assembly_env_dist_summary_11Jan2023.csv", header=T, row.names = 1) #removed from folder after combined dataframe made
full_output<-rbind(geo_dist_tally,env_dist_tally)

#add in color
full_output$zone_col<-rep('#018571',nrow(full_output))
full_output[full_output$zone=="B",]$zone_col<-"#C4AD79"
full_output[full_output$zone=="C",]$zone_col<-'#a6611a'

#add in xlab as dist column
full_output[full_output$dist=="env",]$dist<-'Environmental dissimilarity'
full_output[full_output$dist=="geo",]$dist<-'Distance (km)'
#write.csv(full_output,"Community_assembly_env_geo_dist_summary_11Jan2023.csv")
```

####4) GAMs with 0s for each process this most recent version breaks out the intra-lake comparisons (so different cores within the same lake (just for the geographic distnace))

```{r}
require(mgcv)
full_output<-read.csv("Community_assembly_env_geo_dist_summary_11Jan2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( "[0,0]","(0,0.1]" ,  "(0.1,5]"  , "(5,10]" , "(10,15]"  ,"(15,100]" , "(100,300]", "(300,400]" ,"(400,500]")
break_labels[[2]]<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")
```

GAM figures

```{r}
model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_",unique(full_output$dist)[d],"_GAM_11Jan2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(0,(length(break_labels[[d]])-1),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=gam_res$r.sq, p=gam_res$s.pv))
        
}}
dev.off()
}

#write.csv(model_results, "CommunityAssembly_GAMS_with0_11Jan2023.csv")
```

#### 5) Redoing the GAMS without 0

```{r}
require(mgcv)
full_output<-read.csv("Community_assembly_env_geo_dist_summary_11Jan2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( "[0,0]","(0,0.1]" ,  "(0.1,5]"  , "(5,10]" , "(10,15]"  ,"(15,100]" , "(100,300]", "(300,400]" ,"(400,500]")
break_labels[[2]]<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

full_output<-full_output[full_output$d_val_group!=0,]
```

```{r}
model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_",unique(full_output$dist)[d],"_GAM_11Jan2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels[[d]])),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
        
}}
dev.off()
}
#write.csv(model_results, "CommunityAssembly_GAMS_without0_11Jan2023.csv")

```

#### 6) redoing the Fig6 with patchwork

```{r}
require(patchwork)

full_output<-read.csv("Community_assembly_env_geo_dist_summary_11Jan2023.csv", header=T, row.names = 1)

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[1]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")


ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
for(i in 1:2){
        sub2<-sub[sub$dist==unique(sub$dist)[i],]       
ifelse(j==3, 
       
       
       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_point(size=3)+ geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(size=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.9) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) +
                theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       
       , 
       
       
              ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) + geom_point(size=3)+  geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(size=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.9) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15))+ #change font size of legend title
        theme(axis.title.x=element_blank(),
        axis.text.x=element_blank()) +
        theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       )
        }}


pdf("Figures/Fig6_community_assembly_16Jan2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
  plot_layout(ncol = 2)
dev.off()


```

## 8. SFig 2 - top 3 ESVs

```{r}
require(mgcv)
#determine top 3 ESVs
means <- as.data.frame(rowMeans(ps_tr@otu_table))
names(means)<-"val"
means$otu<-rownames(means)
avgabundtax<-means[order(means$val,decreasing=T),][1:10,]
top10esvs<-merge(avgabundtax,tax_tab,by="row.names")
top10esvs<-top10esvs[order(top10esvs$val,decreasing=T),]
Eury_esvs<-top10esvs$Row.names[1:3]

# subset ps_tr by top 3 esvs
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
tax_tmp<-as.data.frame(ps_tr_26@tax_table@.Data)
row_names<-rownames(tax_tmp)

#make a string with all of the assignments up to Family
tax_tmp$centroid <-rownames(tax_tmp)
ps_tr_26@tax_table@.Data<-as.matrix(tax_tmp)

sub<-subset_taxa(ps_tr_26, centroid%in%Eury_esvs)
melt<-psmelt(sub)

pdf("Figures/SFig2_top3_ESVs_9Jan2024.pdf", width=10, height =4)
par(mfrow=c(1,3))
titles<-c("Methanoregula","MBG-D","Methanosaeta")
i=1
gam_res<-list()
for(i in 1:3){
        tmp<-melt[melt$centroid==Eury_esvs[i],]
        Data<-data.frame(x=tmp$bin_depth,y=tmp$Abundance)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050", ylab = "Relative abundance", xlab="Sediment depth (cm)", main=c("Methanoregula","MBG-D","Methanosaeta")[i])
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res[[i]]<-summary(dat_gam)
        text(x=23, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res[[i]]$r.sq, digits=3), sep=""))
        text(x=22, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res[[i]]$s.pv, digits=3), sep=""))
         text(x=1, y=max(Data$y)*0.95, c("a", "b", "c")[i], cex=2)
        
}
dev.off()


        
```

## 9. SFig 3 - Bathy and Dehalo

both of these families are most abundant in the depauperate zone, but seem to only be at high abundances when the other is at low abundances

```{r}

fam<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/family_abundances_6sept2022.csv",header=T, row.names=1)

bath<-fam[fam$alltax_tofam=="Archaea@Miscellaneous_Crenarchaeotic_Group@Unassigned@Unassigned@Unassigned",]
names(bath)[3]<-"bathabund"
dehalo<-fam[fam$alltax_tofam=="Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned",]
names(dehalo)[3]<-"dehaloabund"

bath_dehalo<-merge(bath, dehalo, by="Sample")

pdf("Figures/SFig3_dehalo_bathy_13Sept2022.pdf", height=5, width=6)
par(mar=c(4,5,4,4))
plot(bath_dehalo$dehaloabund, bath_dehalo$bathabund, pch=16, col="#00000050", xlab="Dehalococcoidia (family MSBL5)\nrelative abundance", ylab="Bathyarchaeota (family unassigned)\nrelative abundance", main="Dehalococcoidia MSBL5 v. Bathyarchaeota")
dev.off()

```

## 10. SFig 1 - individual CONISS

```{r}
require(rioja)
require(vegan)
# use transformed data
OTU<-as.data.frame(ps_tr@otu_table)
OTU<-t(OTU)

lake_drives<-unique(metadata$lake_drive)
rn<-rownames(OTU)
meta_sed<-metadata
meta_sed<-meta_sed[order(match(rownames(meta_sed),rownames(OTU))),]
table(meta_sed$samp_names==rownames(OTU)) #all true
which(table(metadata$lake_drive)<2)

#remove lakes with one sample
lake_drives<-lake_drives[lake_drives!="BN_1"]
lake_drives<-lake_drives[lake_drives!="ML_1"]

#lakes with two samples, can't be used
lake_drives<-lake_drives[lake_drives!="LB_1"]
lake_drives<-lake_drives[lake_drives!="NB_1"]
lake_drives<-lake_drives[lake_drives!="SG_1"]


#individual lake CONISS clustering
pdf("Figures/SFig1_CONISS_individ_12Sept2022.pdf",height=12,width=17)
i=1
par(mfrow=c(5,9),mar=c(2,4,2,0))
for(i in 1:length(lake_drives)){
  subset<-meta_sed[meta_sed$lake_drive==lake_drives[i],]
  subset<-subset[order(subset$depth),]
  names<-subset$samp_names
  rows<-which(rn%in%names)
  sub_otu<-as.data.frame(OTU[rows,])
  sub_otu$rownames<-rownames(sub_otu)
  new_dataset <- sub_otu[match(names, sub_otu$rownames), ]       
  rownames(new_dataset)==new_dataset$rownames
  rownames(new_dataset)==rownames(subset)
  new_dataset$rownames<-NULL
  if(length(unique(subset$depth)) < length(subset$depth)){
      dup<-subset$depth[which(duplicated(subset$depth))]
      pos<-which(subset$depth==dup)
        subset$depth[pos[2]]<-subset$depth[pos[2]]+0.5
  }
  rownames(new_dataset)<-subset$depth
  new_dataset<-as.matrix(new_dataset)
  dist<-vegdist(new_dataset)
  clust<-chclust(dist)
  plot(clust, xvar=as.numeric(clust[["labels"]]), main=paste(subset$lake_name[1],subset$drive[1], sep=" "), ylim= c(0,5), hang=-1, horiz=TRUE, x.rev=TRUE)
}
dev.off()

```

## 11. Anero and methanos

```{r}
require(mgcv)
#check what genuses are in the Methanosaetaceae
sub<-subset_taxa(ps_tr, Family=="Methanosaetaceae")
plot_bar(sub, fill="Genus")
#methanosaeta is the only genus in the family methanosaetaceae 
sub<-tax_glom(sub,taxrank = "Family")

methanosaeta<-psmelt(sub)
names(methanosaeta)[3]<-"abund_methanosaeta"

sub<-subset_taxa(ps_tr, Family=="Anaerolineaceae")
sub<-tax_glom(sub,taxrank = "Family")

anaero<-psmelt(sub)
names(anaero)[3]<-"abund_anaero"

#lm
methano_anaero<-merge(methanosaeta,anaero, by="Sample")
plot(methano_anaero$abund_methanosaeta, methano_anaero$abund_anaero)
summary(lm(methano_anaero$abund_methanosaeta~methano_anaero$abund_anaero))


#gam
Data<-data.frame(x=methano_anaero$abund_methanosaeta,y=methano_anaero$abund_anaero)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050",ylab="Anaerolineaceae abundance", xlab="Methanosaeta abundance", main="Anaerolineaceae v. Methanosaeta")
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res<-summary(dat_gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
         
gam_res
        
```

## 12. Methanoreg and Syntro

```{r}
#look to see what genuses are in Methanoregulaceae
sub<-subset_taxa(ps_tr, Family=="Methanoregulaceae")
sub<-tax_glom(sub,taxrank = "Genus")
plot_bar(sub, x="bin_depth", fill="Genus") #mainly methanoregula

#subset and glom by the family
sub<-subset_taxa(ps_tr, Family=="Methanoregulaceae")
sub<-tax_glom(sub,taxrank = "Family")
meth<-psmelt(sub)
names(meth)[3]<-"abund_meth"

sub<-subset_taxa(ps_tr, Family=="Syntrophaceae")
sub<-tax_glom(sub,taxrank = "Family")

syn<-psmelt(sub)
names(syn)[3]<-"abund_syn"

methano_syn<-merge(meth,syn, by="Sample")
plot(methano_syn$abund_meth, methano_syn$abund_syn)
#lm
summary(lm(methano_syn$abund_meth~methano_syn$abund_syn))

#gam
Data<-data.frame(x=methano_syn$abund_meth,y=methano_syn$abund_syn)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050",ylab="Syntrophaceae abundance", xlab="Methanoregulaceae abundance", main="Syntrophaceae v. Methanoregulaceae")
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res<-summary(dat_gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
         
gam_res
```

## 13. Bathy and Thaum

```{r}
sub<-subset_taxa(ps_tr, Phylum=="Thaumarchaeota")
sub<-tax_glom(sub,taxrank = "Phylum")

sub1<-psmelt(sub)
names(sub1)[3]<-"abund_sub1"

sub<-subset_taxa(ps_tr, Phylum=="Miscellaneous_Crenarchaeotic_Group")
sub<-tax_glom(sub,taxrank = "Phylum")

sub2<-psmelt(sub)
names(sub2)[3]<-"abund_sub2"

sub3<-merge(sub1,sub2, by="Sample")
plot(sub3$abund_sub1, sub3$abund_sub2)
summary(lm(sub3$abund_sub1~sub3$abund_sub2))


Data<-data.frame(x=sub3$abund_sub1,y=sub3$abund_sub2)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050",ylab="Bathyarchaeota abundance", xlab="Thaumarchaeota abundance", main="Thaumarchaeota v. Bathyarchaeota")
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res<-summary(dat_gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
         
gam_res
```

## 14. SFig. 4 EnvDist vs GeoDist

```{r}
comps<-read.csv("pairwise_comparisons_14Sept2022.csv",row.names=1, header=T)

sub<-comps[,names(comps)%in%c("env_dist", "geo_dist_km")]
sub<-unique(sub)
sub<-sub[order(sub$geo_dist_km,sub$env_dist ),]
sub<-sub[2:nrow(sub),]# get rid of row with only zero for both values
pdf("Figures/SFig4_env_geo_dist_14Sept2022.pdf", height=3, width =4)
par(mar=c(4,4,2,2))
plot(sub$geo_dist_km, sub$env_dist,  pch=16, col="#00000050", ylab="Environmental dissimilarity", xlab="Geographic distance (km)")
abline(lm(sub$env_dist~sub$geo_dist_km), lwd=4, col="plum3")
dev.off()
summary(lm(sub$env_dist~sub$geo_dist_km))
```

Call: lm(formula = sub$env_dist ~ sub$geo_dist_km)

Residuals: Min 1Q Median 3Q Max -0.29498 -0.11251 -0.01654 0.10730 0.39676

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 3.255e-01 9.463e-03 34.40 \<2e-16 \*\*\* sub\$geo_dist_km -5.871e-05 3.394e-05 -1.73 0.0842 .

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1522 on 662 degrees of freedom Multiple R-squared: 0.004498, Adjusted R-squared: 0.002994 F-statistic: 2.991 on 1 and 662 DF, p-value: 0.08418

## 15. mantel tests of RCBray values

These all need to be dist objects

```{r}
#this is copied from above for the mantel tests
require(cluster)
require(geosphere)
#load data

#Environmental distance
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

env_dist <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))

#RCbray distance matrix
load("WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData")

rc.nulls<-RCbray_output
physeq<-phyloseq(otu_table(physeq),tax_table(physeq),sample_data(physeq))
physeq_t <- transform_sample_counts(physeq=physeq, function(x) x / sum(x))
OTUsREL<-t(as.data.frame(physeq@otu_table))
design<-physeq_t@sam_data

obs.bray <- as.matrix(vegdist(OTUsREL, method = "bray"))
site.compares <- expand.grid(site1 = 1:nrow(obs.bray), site2 = 1:nrow(obs.bray))
site.compares <- site.compares[-which(site.compares[,1] == site.compares[,2]),]
RC.bray <- matrix(NA, nrow = nrow(obs.bray), ncol = nrow(obs.bray))
for(row.i in 1:nrow(site.compares)){
  site1 <- site.compares[row.i,1]
  site2 <- site.compares[row.i,2]
  pairwise.null <- rc.nulls[site1,site2,]
  pairwise.bray <- obs.bray[site1,site2]
  num.greater <- sum(pairwise.null > pairwise.bray)
  num.ties <- sum(pairwise.null == pairwise.bray)
  val <- -1*((((1 * num.greater) + (0.5 * num.ties))/999 - 0.5) * 2)
  RC.bray[site1, site2] <- val
}
rownames(RC.bray) <- rownames(design)
colnames(RC.bray) <- rownames(design)
RCbray<- as.dist(RC.bray)


# geographic distance
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
geo_dist<-as.dist(dist_m_output)

#check to make sure all labels match!
table(labels(RCbray)==labels(env_dist)) #all true
table(labels(RCbray)==labels(geo_dist)) #all true

mantel(RCbray, env_dist)
mantel(RCbray, geo_dist)

#add partial mantel test, spatially structured environmental variables
mantel.partial(RCbray, env_dist, geo_dist, method = "pearson", permutations = 999)
mantel.partial(RCbray, geo_dist, env_dist, method = "pearson", permutations = 999)
```

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = env_dist)

Mantel statistic r: 0.2314 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.00874 0.01089 0.01331 0.01641 Permutation: free Number of permutations: 999

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = geo_dist)

Mantel statistic r: 0.1938 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0091 0.0119 0.0146 0.0191 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = env_dist, zdis = geo_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.2283 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0088 0.0108 0.0130 0.0161 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = geo_dist, zdis = env_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.19 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0109 0.0147 0.0176 0.0209 Permutation: free Number of permutations: 999

## 16. Do archeal ESVs increase with depth? - I need to go back through this and make sure its clean

ran this with rarefied data, not transformed

```{r}
require(iNEXT)
arch<-subset_taxa(ps, Kingdom=="Archaea")

bio1 <- as.matrix(arch@otu_table@.Data)
bio2 <- split(bio1, as.numeric(rep(1:ncol(bio1), each = nrow(bio1))))
bio3 <- iNEXT(bio2, q=0, datatype="abundance") 

# select asymptotic estimates 
bio4 <- bio3$AsyEst
bio5 <- subset(bio4, bio4$Diversity =="Species richness")  
bio6 <- subset(bio4, bio4$Diversity =="Shannon diversity")  
bio7 <- subset(bio4, bio4$Diversity =="Simpson diversity")  

#create a table with diversity estimates
bio8 <- data.frame(cbind(bio5$Observed, bio6$Observed, bio7$Observed))
rownames(bio8) <- colnames(bio1)
colnames(bio8) <- c("Species_Richness", "Shannon_Diversity","Simpson_Dominance")
head(bio8)
#write.table(bio8,file="hillnumbers_norm_endpoint_archaea_30Sept2022.txt",sep="\t",row.names=T, col.names=T)
hill_norm<-bio8

rm(bio1)
rm(bio2)
rm(bio3) 
rm(bio4)
rm(bio5)
rm(bio6)
rm(bio7)
rm(bio8)
```

```{r}
require(mgcv)
#load environment and Hill's number file. 
meta_sed_26<-metadata[metadata$depth<=26,]
hills<-read.delim("hillnumbers_norm_endpoint_archaea_30Sept2022.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")

lm<-lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth)
plot(meta_sed_26_hills$depth, meta_sed_26_hills$Species_Richness)
abline(lm)

summary(lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Shannon_Diversity~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Simpson_Dominance~meta_sed_26_hills$depth))

#tried this with a GAM too, but not much different 
Data<-data.frame(x=meta_sed_26_hills$depth,y=meta_sed_26_hills$Species_Richness)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=3), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)
```

all three LMs are not significant in archeal ASV richness with depth

looking at archeal esvs by zone

```{r}

ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
arch<-subset_taxa(ps_tr_26, Kingdom=="Archaea")
arch_esv<-as.data.frame(otu_table(arch))

#subset metadata less than 27 cm 
meta_sub<-metadata[metadata$samp_names%in%names(arch_esv),]
meta_sub$zone<-rep("A",nrow(meta_sub))
meta_sub[meta_sub$bin_depth%in%c(6,8,10,12),]$zone<-"B"
meta_sub[meta_sub$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
meta_sub$zone<-as.factor(meta_sub$zone)

#subset, check, and merge
meta_sub<-meta_sub[,names(meta_sub)%in%c("samp_names","bin_depth","zone","lake_drive")]
arch_esv<-as.data.frame(t(as.matrix(arch_esv)))
table(rownames(meta_sub)%in%rownames(arch_esv))

combo<-merge(arch_esv, meta_sub, by="row.names")


#turn to presence absence
for(i in 1:nrow(combo)){
        for(j in 2:10421){
                if(combo[i,j]>0){
                combo[i,j]<-1
                }}}

presence<-as.data.frame(rowSums(combo[,2:10421]))
names(presence)<-"num_ESVs"

presence$depth_bin<-combo$bin_depth
presence$lake_drive<-combo$lake_drive
presence$zone<-as.factor(combo$zone)
presence$horizon<-rep(NA,nrow(presence))
presence[presence$zone=="A",]$horizon<-"Redox"
presence[presence$zone=="B",]$horizon<-"Transition"
presence[presence$zone=="C",]$horizon<-"Depauperate"

library(dplyr)
group_by(presence,zone) %>%
  summarise(
    count = n(),
    mean = mean(num_ESVs, na.rm = TRUE),
    sd = sd(num_ESVs, na.rm = TRUE)
  )


library("ggpubr")
ggboxplot(presence, x ="horizon", y = "num_ESVs", color = c('#018571','#dfc27d','#a6611a'), order=c("Redox","Transition","Depauperate"), ylab = "Number of archeal ESVs", xlab = "Horizon")

summary(aov(num_ESVs~ horizon, data = presence))

model<-(aov(num_ESVs~ horizon, data = presence))
TukeyHSD(model)


#              Df  Sum Sq Mean Sq F value  Pr(>F)   
# horizon       2  117721   58861   5.988 0.00272 **
# Residuals   426 4187360    9829                   
# ---
# Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1


# 
#   Tukey multiple comparisons of means
#     95% family-wise confidence level
# 
# Fit: aov(formula = num_ESVs ~ horizon, data = presence)
# 
# $horizon
#                             diff       lwr       upr
# Redox-Depauperate      -26.48538 -54.67975  1.708988
# Transition-Depauperate  16.40607  -9.96735 42.779484
# Transition-Redox        42.89145  13.65890 72.123991
#                            p adj
# Redox-Depauperate      0.0707331
# Transition-Depauperate 0.3098302
# Transition-Redox       0.0017758



i=1
track<-NULL
for(i in 1:length(unique(presence$depth_bin))){
        sub<-presence[presence$depth_bin==unique(presence$depth_bin)[i],]
        track<-c(track,mean(sub$num_ESVs))
}
track<-data.frame(bin_depth=unique(presence$depth_bin), meanESV=track)
track<-track[order(track$bin_depth),]
presence<-presence[order(presence$depth_bin),]

i=1
        plot(presence$depth_bin,presence$num_ESVs, type="l", col="white", xlab="Sediment depth (cm)", ylab="Number of archeal ESVs")
for(i in 1:length(unique(presence$lake_drive))){

        sub3<-presence[presence$lake_drive==unique(presence$lake_drive)[i],]
        lines(sub3$depth_bin, sub3$num_ESVs, type="l", col="gray")
}

lines(track$bin_depth,track$meanESV, type="b", pch=16, col="darkgreen")
legend("topright", legend=c("Individual core", "Average across cores"),lty=1, lwd=2,col = c("gray","darkgreen") )



```

## 17. correlation of env dist measures

```{r}

# I'm duplicating here, but just looking
plot(metadata$water_sample_ph_bot, metadata$max_lake_depth)
summary(lm(metadata$water_sample_ph_bot~metadata$max_lake_depth))
plot(metadata$water_sample_do_bot, metadata$max_lake_depth)
summary(lm(metadata$water_sample_do_bot~metadata$max_lake_depth))
plot(metadata$water_sample_t_bot, metadata$max_lake_depth)
summary(lm(metadata$water_sample_t_bot~metadata$max_lake_depth))


cor()


```

## 18. SedaDNA sites

```{r}
library(ggplot2)
library(ggmap)
library(MetBrewer)

df<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/Collaborations/sedaDNA cyberinfrastructure workshop/V1_20220718_sedimentary_aeDNA_sites.csv", header=T)

mb<-df[df$MolecularMethod=="MB",]
mb<-mb[mb$TargetGroup=="Microorganisms",]
unique(mb$TargetTaxa)
mb<-mb[mb$TargetTaxa%in%c("Archaea","Prokaryotes","Bacteria"),]
mapview::mapview(mb$Latitude..DD.,mb$Longitude..DD.)

write.csv(mb,"Num_sedaDNA_sites.csv")

df<-mb

mapWorld <- borders("world", colour="gray50", fill="gray50") 

colnames(df)[4]<-"lat"
colnames(df)[5]<-"lon"
df$lat<-as.numeric(df$lat)
df$lon<-as.numeric(df$lon)

cols<-met.brewer(name="Egypt", n=4, type="discrete")
cols<-c("#000000",cols[1],cols[4],cols[2],cols[3])
cols<-paste(cols,"95", sep="")
point_types<-c(21,22,24,23,28)

mp <- NULL
mp <- ggplot() +   mapWorld
mp <- mp + geom_point(aes(x=df$lon, y=df$lat, fill=df$TargetGroup, shape=df$SampleType_factor), size=3) +   labs(x = "Latitude", y = "Longitude", fill= "Target group", shape = "Sample type")  + scale_shape_manual(values=point_types) +scale_fill_manual(values=cols)
mp <- mp + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line=element_blank(),axis.text.x=element_blank(),panel.border=element_blank(), axis.text.y=element_blank(),axis.ticks=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank())  + theme(legend.position="none")+ annotate("text", y=95, x=-145, label= "Microbe metabarcoding", size=6) 
mp 
```

## 19. BNTI vs env_dist

```{r}
require(mgcv)
comps<-read.csv("pairwise_comparisons_14Sept2022.csv", header=T, row.names=1) # load this as a backup file and pull from there

plot(comps$env_dist ,comps$BNTI)

Data<-data.frame(x=comps$env_dist,y=comps$BNTI)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=5), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y,)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)

table(bnti)

```

## 20. phylogenetic signal

```{r}
#load data
require(ape)
require(vegan)
require(picante)
```

Subset for the most abundant ESVs (10,888 ESVs) for computational tractability

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)

my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
```

issues is that i'm doing a weighted mean for some samples where the OTU isn't present...

from Vass's email: Find optimal niche for conductivity with abundance-weighted-mean conductivity value for each OTU Make data table of conductivity in first column, and OTU relative abundance in rest of columns Read in environmental data and separate average conductivity column for each site Read in envi data

From Tripathi et al. 2018 "To calculate the abundance-weighted mean for a given OTU, we first found all samples in which that OTU was present. We then found the abundance-weighted mean pH of all those samples. To do so, in the calculation of mean pH we weighted each pH value by the abundance of the OTU in the associated sample. This procedure was repeated for each OTU, and the resulting value was used as a rough estimate of that OTU's pH optimum. Then, between-OTU differences in pH optima were calculated as Euclidean distances. Finally, we used Mantel correlograms to measure the correlation coefficients between differences in pH optima and phylogenetic distances [3, 9], and significance of these correlations was assessed using 999 permutations with Bonferroni correction."

```{r}
#using just the 10888 most abundant ESVs
#match up with metadata
OTU<-as.data.frame(otu_table(my_subset))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),] 
#change OTUs that are 0 and turn them into NAs
OTU[OTU == 0] <- NA

metadata<-metadata[order(rownames(metadata)),] 
table(rownames(OTU)==rownames(metadata)) #all true

#bottom water temp (missing one value)
#metadata[metadata$lake_id==40,]$water_sample_pH_bot<-20.1

#Select variable
env.var <- metadata["water_sample_ph_bot"]
names(env.var)<-"environmental_variable"
table(is.na(env.var)) #all false   

#Combine env.var and OTU table
env.varOTU <- merge(env.var,OTU,by="row.names")
rownames(env.varOTU)<-env.varOTU$Row.names
env.varOTU$Row.names<-NULL

#make sure the env.var is column 1 and OTU is column 2 on..
head(names(env.varOTU))

#Find optimal temperature by applying weighted mean function to each OTU column
#subset by only the samples that HAD that otu present, and calculated the abundance weighted mean env.var for that specific esv
NiOp<-data.frame(OTUID=NULL,env.var.wt.mn=NULL)
i=2
for(i in 2:ncol(env.varOTU)){
       sub<-env.varOTU[,c(1,i)] 
       sub<-sub[complete.cases(sub),]
       NiOp<-rbind(NiOp,data.frame(OTUID=names(sub)[2],env.var.wt.mn=weighted.mean(x=sub[,1],w=sub[,2])))
}
rownames(NiOp)<-NiOp$OTUID
NiOp$OTUID<-NULL

#did not do yet
#dist_NiOp<-dist(abundwtmn)

########################################################################
#Make phylogenetic distance matrix
#Load phylogenetic tree
phylo <- read.tree("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/sed_norm_fasta_29Aug2022_muscled.nwk")
#subset by the abundant 10888 ESVs
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), phylo)
phylo <- physeq@phy_tree
rm(physeq)

# make sure these match
table(phylo$tip.label%in%unique_alldepths) #all TRUE
table(unique_alldepths%in%phylo$tip.label) #all TRUE

# making new matrix that will have standardized species scores
#JVE - did this for an OTU table with zeros instead of NAs since it wouldn't scale right below with NAs
OTU_0<-as.data.frame(otu_table(my_subset))
OTU_0<-t(OTU_0)
OTU_0<-OTU_0[order(rownames(OTU_0)),] 
otu.scores.std=(OTU_0)
otu.scores.std = decostand(otu.scores.std, "standardize", MARGIN = 2) #standardize by columns (OTUs) where  scale x to zero mean and unit variance

match.phylo.otu = match.phylo.comm(phylo, otu.scores.std) ; #or .data if transposed

#this merges the abundance weighted mean value (NiOp) back in with the OTU table and then scales the whole thing
dim(NiOp)
# JVE Jan 4 - I'm not exactly sure what the comm values are in this object
commm=t(match.phylo.otu$comm)
dim(commm)
table(is.na(NiOp$env.var.wt.mn)) #all false
NiOp=merge(NiOp, commm, by="row.names")
rownames(NiOp) <- NiOp[,1]
NiOp$Row.names<-NULL

#JV - this merges the abundance weighted mean value (NiOp) back in with the OTU table and then scales the whole thing and then promptly removes it... I thought it was the lines above, but below is what is the in the code

NiOp=merge(na.omit(NiOp), commm, by="row.names")
rownames(NiOp) <- NiOp[,1]
NiOp_s=NiOp[,-1]
NiOp_s=NiOp_s[,1, drop=FALSE]
NiOp=NiOp_s


NiOp.scal<- scale(NiOp)

# check that we get mean of 0 and sd of 1
apply(NiOp.scal, 2, mean)  # faster version of apply(scaled.dat, 2, mean)
apply(NiOp.scal, 2, sd)

#OTU niche distance matrix
spp.dist = as.matrix(dist(NiOp.scal))


# making sure the names on the phylogeny are ordered the same as the names in otu table
# the match phylo does subset the tree to the 10888 esvs, but this could be more streamlined and just subset the tree first
match.phylo.otu[[1]]; # tree
dim(match.phylo.otu[[2]]); match.phylo.otu[[2]][1:3, 1:6] ;

phylo.dist = cophenetic(match.phylo.otu[[1]]);   #Computes the cophenetic distances for a hierarchical clustering (intergroup dissimilarity)
phylo.dist = phylo.dist/max(phylo.dist) ; #Makes intergroup dissimilarities relative to the maximum? Gives a distance matrix for between-OTU phylogenetic distances

#Sort columns to match the niche distance matrix
phylo.dist<-phylo.dist[,order(colnames(phylo.dist))]
phylo.dist<-phylo.dist[order(rownames(phylo.dist)),]

#Check that niche distance matrix and phylogenetic distance matrix have OTUs in same order
table(colnames(spp.dist)==colnames(phylo.dist)) #all true
table(rownames(spp.dist)==rownames(phylo.dist)) #all true
```

Mantel test and plotting

```{r}
phylo.sig.correlog = mantel.correlog(spp.dist,
                                     phylo.dist,
                                     n.class=50,
                                     mult="bonferroni");

phylo.sig.correlog = mantel.correlog(spp.dist,
                                     phylo.dist,
                                     nperm = 1,
                                     mult="bonferroni");


plot(phylo.sig.correlog)

phylo.sig<-phylo.sig.correlog$mantel.res

env.plot=plot(phylo.sig[,c(1,3)],
                   xlab="Phylogenetic distance class", ylab="Mantel correlation") +lines(phylo.sig[,c(1,3)]) + points(phylo.sig[,c(1,3)], pch=21, bg="white", col="black", pty=4)+
        points(phylo.sig[,c(1,3)][phylo.sig[,5] < 0.05,], pch=21, bg="black", col="black", pty=4) + abline(h=0, lty=2, col="red") +
        title("Bottom water pH")

```

### a. Pearman code

```{r}
require(phyloseq)
require(ape)
require(dplyr)
library(caret)
library(picante)

```

Subset for the most abundant ESVs (10,888 ESVs) for computational tractability

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)

my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
```

could double check this if I wanted - but all looks fine. Maybe want to do with ESVs with 10+ reads

```{r}
#Load phylogenetic tree
phylo <- read.tree("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/sed_norm_fasta_29Aug2022_muscled.nwk")
#subset by the abundant 10888 ESVs
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), phylo)
tree <- physeq@phy_tree
# make sure only the 10888 ESVs are in the tree
table(tree$tip.label%in%unique_alldepths) #all TRUE
table(unique_alldepths%in%tree$tip.label) #all TRUE
rm(physeq)

#using just the 10888 most abundant ESVs
#match up with metadata
OTU<-as.data.frame(otu_table(my_subset))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),] #site by species

#order metadata
metadata<-metadata[order(rownames(metadata)),] 
#subset metadata
metadata.a <- metadata %>% select(depth, elevation_meters,max_lake_depth, water_sample_ph_bot, water_sample_do_bot, water_sample_t_bot,pH,n_perc, c_perc)

#make OTU and metadata match
table(rownames(OTU)==rownames(metadata)) #all true


#Predict the missing metadata (JVE: not very familiar with this part of John Pearman's code to predict missing metadata..)
set.seed(496)

metadata.1 <-
  preProcess(metadata.a,
             method = c("bagImpute"))

metadata.2  <- 
  predict(metadata.1 , metadata.a)


OTU_log <- log(OTU + 1) #JVE: unsure why log + 1

OTU_log.p <- match.phylo.comm(tree, OTU_log)$comm
tree_log.p <- match.phylo.comm(tree, OTU_log)$phy

phydist <- cophenetic(tree_log.p)

#phylogenetic distance
phydist_hel <- decostand(phydist, method="hellinger")

#from site x species to species x site
table <- t(OTU_log)

env.var.names<-colnames(metadata.2)

#loop through each environmental variable and create a list of each "final table"
all_env.vars_tab<-list()
q=1
for(q in 1:length(env.var.names)){

table_env<-rbind(metadata.2[,q],table)
rownames(table_env)[1]<-"env.var"

table_w <- t(table_env) #first column environmental variable 

headTable <- colnames(table_w) 
x <- headTable[1] # The first column is a constant env.var and is being compared to OTU


# Create the final_table empty

m <- matrix(0, nrow = 0, ncol = 3) 
final_table <- data.frame(m)
colnames(final_table)  <- c('ymax','d50', 'xmax') 

# the for loop, to get all otus 1 by 1 compared to env.var

for(i in seq(2,length(headTable),1)) #start at 2 because the 1 is the Eh_mv
{
  y <- headTable[i] # get otu name
  tablei <-  table_w[,c(x,y)] # extract 2 columns to create a new table
  
  head  <- colnames(tablei)
  
  ymax = which.max(tablei[,2]) # get the max relative abundance of the OTU
  
  MaxRow <- tablei[ymax,] # create a table with the line of the max OTU value
 
  tableiInf <- subset(tablei,tablei[,1]<= MaxRow[1]) #create two tables greater and lower than the x coord
  tableiSup <- subset(tablei,tablei[,1]>= MaxRow[1]) #of the max
  
  tableiInfno0 <- subset(tableiInf,tableiInf[,2]>(MaxRow[2]/20)) #remove the observations where the relative OTU abundance is 0
  tableiSupno0 <- subset(tableiSup,tableiSup[,2]>(MaxRow[2]/20)) #remove the observations where the relative OTU abundance is 0
  
  
  #Determine the lowest and highest Eh value where you find the OTU
  
  xInf <- min(tableiInfno0[,1])
  xSup <- max(tableiSupno0[,1])
  
  # calculate the d50
  
  d50  <- 0.68*abs(xSup-xInf)/2
  #JVE - why is this 0.68?
  
  # create the final table for the otu in the loop
  
  tablei_fin  <- matrix(c(MaxRow[2],d50,MaxRow[1]),ncol=3)
  rownames(tablei_fin)  <- c(head[2])
  colnames(tablei_fin)  <- c('ymax','d50', 'xmax')
  
  # add to the final_table the values for each otu 
  final_table <- rbind(final_table,tablei_fin)
  
  
}
#write the table and put into a list
write.table(final_table, paste("niche_", env.var.names[q], "_9Jan2023.txt", sep=""), sep="\t")
all_env.vars_tab[[q]]<-final_table
}

#remove all but the two objects needed for the mantel correlogs
rm(list=ls()[! ls() %in% c("all_env.vars_tab","phydist_hel")])
save.image("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")

```

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal 

```

#### i) run on Beartooth

phylogenetic_signal_1.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[1]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_1<-mantel.correlog(env_dist,phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_1.RData")

```

phylogenetic_signal_2.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[2]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_2<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_2.RData")

```

phylogenetic_signal_3.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[3]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_3<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_3.RData")

```

phylogenetic_signal_4.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[4]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_4<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_4.RData")

```

phylogenetic_signal_5.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[5]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_5<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_5.RData")

```

phylogenetic_signal_6.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[6]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_6<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_6.RData")

```

phylogenetic_signal_7.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[7]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_7<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_7.RData")

```

phylogenetic_signal_8.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[8]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_8<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_8.RData")

```

phylogenetic_signal_9.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[9]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_9<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_9.RData")

```

run_phylogenetic_signal.sh

```{bash}
#!/bin/bash
#SBATCH --job-name phylo_signal
#SBATCH --mem=20GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=phylo_signal_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal
date

srun Rscript phylogenetic_signal_1.R
echo "srun Rscript phylogenetic_signal_1.R"

srun Rscript phylogenetic_signal_2.R
echo "srun Rscript phylogenetic_signal_2.R"

srun Rscript phylogenetic_signal_3.R
echo "srun Rscript phylogenetic_signal_3.R"

srun Rscript phylogenetic_signal_4.R
echo "srun Rscript phylogenetic_signal_4.R"

srun Rscript phylogenetic_signal_5.R
echo "srun Rscript phylogenetic_signal_5.R"

srun Rscript phylogenetic_signal_6.R
echo "srun Rscript phylogenetic_signal_6.R"

srun Rscript phylogenetic_signal_7.R
echo "srun Rscript phylogenetic_signal_7.R"

srun Rscript phylogenetic_signal_8.R
echo "srun Rscript phylogenetic_signal_8.R"

srun Rscript phylogenetic_signal_9.R
echo "srun Rscript phylogenetic_signal_9.R"


echo "finished correlogs"
date

```

copy files back to computer

```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_1.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_2.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_3.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_4.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_5.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_6.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_7.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_8.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_9.RData /Users/jordanscheibe/Desktop


```

#### ii) Plot mantel correlograms

```{r}
env.var.names<-c("depth",               "elevation_meters"   , "max_lake_depth" ,    
"water_sample_ph_bot", "water_sample_do_bot", "water_sample_t_bot" ,
"pH"                ,  "n_perc"            ,  "c_perc"    )  
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_1.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_2.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_3.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_4.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_5.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_6.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_7.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_8.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_9.RData")

figs<-list()
figs[[1]]<-mc_result_1
figs[[2]]<-mc_result_2
figs[[3]]<-mc_result_3
figs[[4]]<-mc_result_4
figs[[5]]<-mc_result_5
figs[[6]]<-mc_result_6
figs[[7]]<-mc_result_7
figs[[8]]<-mc_result_8
figs[[9]]<-mc_result_9

i=1
pdf("Figures/Phylogenetic_signal_mantel_corr_24Jan2023.pdf", height = 8, width=10)
par(mfrow=c(3,3), mar=c(4,4,2,2))
for(i in 1:9){
plot(figs[[i]])
text(0.0025,max(figs[[i]]$mantel.res[,3], na.rm=T)*0.9, env.var.names[i])
}
dev.off()




```

## 21. iCAMP - Rerun Comm. Assembly

### a) ESVs with 10+ reads

#### i) subset ESVS with 10 reads

```{r}
require(phangorn)
require(msa)
require(Biostrings)
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

# 1. subset OTU table by ESVs with 10+ reads
OTU<-as.data.frame(otu_table(ps))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),]

table(colSums(OTU)<10)
#FALSE  TRUE 
#28316 63641 
mean(colSums(OTU)) #53 reads on average

lessthan10<-which(colSums(OTU)<10)
OTU_10plusreads<-OTU[,-lessthan10]
dim(OTU_10plusreads)
table(colSums(OTU_10plusreads)<10)# all FALSE
```

#### ii) subset fasta file

in local R

```{r}
#2. subset fasta file by the ESVs with 10+ reads (normalized reads)
seqs<-Biostrings::readDNAStringSet("zotus_nonchimeric.fa") #downloaded straight from the calder file folder
seqs@ranges@NAMES<-stringr::str_split(seqs@ranges@NAMES,pattern = ";", 3, simplify = T)[,1]

seqs_sub<-seqs[colnames(OTU_10plusreads)]
table(seqs_sub@ranges@NAMES %in% colnames(OTU_10plusreads))
table(colnames(OTU_10plusreads)%in%seqs_sub@ranges@NAMES)
rm(list=ls()[! ls() %in% c("OTU_10plusreads", "seqs_sub" )])
#save.image("WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData")

```

MSA is taking forever so try with old way (clustalO)

```{r}
load("WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData") #this is the subset file that has only ESVs with 10+ reads
require(Biostrings)
writeXStringSet(seqs_sub,filepath = "ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta", format = "fasta")
```

```{bash}
rsync /Users/jordanscheibe/Desktop/ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo
```

#### iii) make tree clustalo and fasttree

Ran this is command line with salloc, pasted output below for records

```{bash}
module load arcc/1.0 miniconda3/4.12.0
conda activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo

clustalo -i ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta -o tempfile_16Jan2023_muscled.fa -v --threads=6

FastTree -nt tempfile_16Jan2023_muscled.fa > ESVs_with10ormorereads_output_16Jan2023_muscled.nwk

```

(shotgun_env) [[jvonegge\@blog1](mailto:jvonegge@blog1){.email} maketree_clustalo]\$ salloc --mem=120GB --nodes=1 --cpus-per-task=6 --account=microbiome --time=1:00:00 salloc: Pending job allocation 6104675 salloc: job 6104675 queued and waiting for resources salloc: job 6104675 has been allocated resources salloc: Granted job allocation 6104675 salloc: Waiting for resource configuration salloc: Nodes m221 are ready for job [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ module load arcc/1.0 miniconda3/4.12.0 [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ conda activate shotgun_env cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo (shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo (shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ ls ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta (shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ clustalo -i ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta -o tempfile_16Jan2023_muscled.fa -v --threads=6 Using 6 threads Read 28316 sequences (type: DNA) from ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta Using 218 seeds (chosen with constant stride from length sorted seqs) for mBed (from a total of 28316 sequences) Calculating pairwise ktuple-distances... Ktuple-distance calculation progress done. CPU time: 494.62u 0.12s 00:08:14.74 Elapsed: 00:01:26 mBed created 448 cluster/s (with a minimum of 1 and a soft maximum of 100 sequences each) Distance calculation within sub-clusters done. CPU time: 94.02u 0.03s 00:01:34.05 Elapsed: 00:00:17 Guide-tree computation (mBed) done. Progressive alignment progress done. CPU time: 348.22u 1.90s 00:05:50.12 Elapsed: 00:04:25 Alignment written to tempfile_16Jan2023_muscled.fa

(shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ FastTree -nt tempfile_16Jan2023_muscled.fa \> ESVs_with10ormorereads_output_16Jan2023_muscled.nwk ML-NNI round 11: LogLk = -911140.600 NNIs 337 max delta 10.59 Time 656.25 (final) Optimize all lengths: LogLk = -911087.046 Time 669.69\
Total time: 748.59 seconds Unique: 28316/28316 Bad splits: 46/28313 Worst delta-LogLk 7.368

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI/iCAMP/qpen/10plus_ESVs


# Here I change name from what was used to make the phylogenetic tree - to make it more CLEAR, changed this also in my local computer files
mv WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_ESVswith10plusreads_Beartooth_13Jan2023.RData

```

#### v) qpen ESVs with 10+ reads

qpen_function.R

```{r}
require(phyloseq)
require(iCAMP)
require(ape)
load("../WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_ESVswith10plusreads_Beartooth_13Jan2023.RData")
comm<-OTU_10plusreads
tree<-read.tree("../ESVs_with10ormorereads_output_16Jan2023_muscled.nwk")
table(colnames(OTU_10plusreads)%in%tree[["tip.label"]]) #all TRUE
table(tree[["tip.label"]]%in%colnames(OTU_10plusreads)) #all TRUE


wd0="/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen"
nworker=6 # parallel computing thread number
rand.time=1000 # usually use 1000 for real data.
  

  # for a big dataset, pdist.big may be used
  save.wd="/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen/pdbig.qpen"
  print(save.wd)
  # please change to the folder you want to save the pd.big output.
  
  pd.big=pdist.big(tree = tree, wd=save.wd, nworker = nworker)
  qp2=qpen(comm=comm, pd=pd.big$pd.file, pd.big.wd=pd.big$pd.wd,
           pd.big.spname=pd.big$tip.label, tree=tree,
           rand.time=rand.time, nworker=nworker)
  setwd(wd0)
save.image("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
```

run_qpen_function.sh

```{bash}
#!/bin/bash
#SBATCH --job-name qpen_10plus
#SBATCH --mem=120GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=6
#SBATCH --account=microbiome
#SBATCH --output=qpen_10plus_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen

srun Rscript qpen_function.R
echo "srun Rscript qpen_function.R"

echo "finished qpen iCAMP - JVE"
date
```

```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen/WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData /Users/jordanscheibe/Desktop

```

#### vi) caluclate dniche

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_phyloseq_env_29Aug2022.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/dniche

```

doing this in the bash terminal:

```{bash}
module load gcc/12.2.0 r/4.2.2
```

dniche_function.R

```{r}
require(dplyr)
require(caret)
require(iCAMP)
require(ape)
require(phyloseq)
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("../WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_ESVswith10plusreads_Beartooth_13Jan2023.RData")
tree<-read.tree("../ESVs_with10ormorereads_output_16Jan2023_muscled.nwk")
table(colnames(OTU_10plusreads)%in%tree[["tip.label"]]) #all TRUE
table(tree[["tip.label"]]%in%colnames(OTU_10plusreads)) #all TRUE


#order metadata
metadata<-metadata[order(rownames(metadata)),] 
#subset metadata
metadata.a <- metadata %>% select(depth, elevation_meters,max_lake_depth, water_sample_ph_bot, water_sample_do_bot, water_sample_t_bot,pH,n_perc, c_perc)

#make OTU and metadata match
table(rownames(metadata.a)==rownames(OTU_10plusreads)) #all true

#Predict the missing metadata (JVE: not very familiar with this part of John Pearman's code to predict missing metadata..)
set.seed(496)

metadata.1 <-
  preProcess(metadata.a,
             method = c("bagImpute"))

metadata.2  <- 
  predict(metadata.1 , metadata.a)

table(rownames(metadata.2)==rownames(OTU_10plusreads)) #all true

table(is.na(metadata.a))

# FALSE  TRUE 
#  3685   617 
table(is.na(metadata.2))
# FALSE 
#  4302 

comm<-OTU_10plusreads
env<-metadata.2


 wd0=getwd()
  save.wd=paste0(tempdir(),"/pdbig.ps.bin")
  # please change to the folder you want to save the big niche difference matrix.
  
  nworker=2 # parallel computing thread number
  pd.big=pdist.big(tree = tree, wd=save.wd, nworker = nworker)
    
  niche.dif=dniche(env = env, comm = comm,
                   method = "niche.value", nworker = nworker,
                   out.dist=FALSE,bigmemo=TRUE,nd.wd = save.wd,
                   nd.spname.file="nd.names.csv")
  
  ds = 0.2 # setting can be changed to explore the best choice
  bin.size.limit = 12 # setting can be changed to explore the best choice.

  
  phylobin=taxa.binphy.big(tree = tree, pd.desc = pd.big$pd.file,
                           pd.spname = pd.big$tip.label, pd.wd = pd.big$pd.wd,
                           ds = ds, bin.size.limit = bin.size.limit,
                           nworker = nworker)
  sp.bin=phylobin$sp.bin[,3,drop=FALSE]
  
  sp.ra=colMeans(comm/rowSums(comm))
  abcut=3
  # by abcut, you may remove some species,
  # if they are too rare to perform reliable correlation test.
  
  
  commc=comm[,colSums(comm)>=abcut,drop=FALSE]
  dim(commc)
  spname.use=colnames(commc)
  
  binps=ps.bin(sp.bin = sp.bin,sp.ra = sp.ra,spname.use = spname.use,
               pd.desc = pd.big$pd.file, pd.spname = pd.big$tip.label,
               pd.wd = pd.big$pd.wd, nd.list = niche.dif$nd,
               nd.spname = niche.dif$names, ndbig.wd = niche.dif$nd.wd,
               cor.method = "pearson",r.cut = 0.1, p.cut = 0.05, min.spn = 5)
  setwd(wd0)

 save.image("dniche_psbin_ESV10plusreads_26Jan2023.Rdata")
```

run_dniche_function.sh

```{bash}
#!/bin/bash
#SBATCH --job-name dniche_10plus
#SBATCH --mem=50GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=2
#SBATCH --account=microbiome
#SBATCH --output=dniche_10plus_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/dniche

srun Rscript dniche_function.R
echo "srun Rscript dniche_function.R"

echo "finished dniche iCAMP - JVE"
date
```

### b) qpen results

#### 1) qpen output - ESVs with 10plus reads

###### a) start here

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(metagMisc)

#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","process")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"process[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)%in%metadata$samp_names) #all true
table(colnames(dfr)%in%metadata$samp_names) # all true

#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

table(colnames(dfr)==metadata$samp_names) # all true
table(colnames(dfr)==rownames(metadata_sub)) # all true 

daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))
table(colnames(daisy.mat)==colnames(dfr)) # all true
table(rownames(daisy.mat)==rownames(dfr)) # all true

daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

process<-melt(as.matrix(dfr), na.rm=T)

table(process$Var1==env_dist$Var1) # all true
table(process$Var2==env_dist$Var2) # all true 
process$env_dist<-env_dist$value

pairwise<-process #overwrite original file

names(pairwise)[1]<-"s1_samp_names"
pairwise$s1_samp_names<-as.character(pairwise$s1_samp_names)
names(pairwise)[2]<-"s2_samp_names"
pairwise$s2_samp_names<-as.character(pairwise$s2_samp_names)
names(pairwise)[3]<-"process"

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

pairwise<-merge(pairwise, metadata_s1, by="s1_samp_names")
pairwise<-merge(pairwise, metadata_s2, by="s2_samp_names")

#remove metadata notes
pairwise$s1_notes<-NULL
pairwise$s2_notes<-NULL
pairwise$s1_Notes_sed_water_wt<-NULL
pairwise$s2_Notes_sed_water_wt<-NULL
pairwise$s1_Notes_carbon_nitrogen<-NULL
pairwise$s2_Notes_carbon_nitrogen<-NULL
pairwise$s1_Notes._lake_sed_pH<-NULL
pairwise$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
pairwise$abs_cm<-rep(NA, nrow(pairwise))
i=1
for(i in 1:nrow(pairwise)){
  ifelse(pairwise$s1_lake_drive[i]==pairwise$s2_lake_drive[i],pairwise$abs_cm[i]<-abs(pairwise$s2_depth[i]-pairwise$s1_depth[i]),NA)
}

# calculate geographic distance

require(geosphere)
for(i in 1:nrow(pairwise)){
        pairwise$geo_dist[i]<-as.numeric(distm(data.frame(X = pairwise$s1_longitude[i], Y = pairwise$s1_latitude[i]),data.frame(X = pairwise$s2_longitude[i], Y = pairwise$s2_latitude[i])))
}

pairwise$geo_dist_km<-pairwise$geo_dist/1000

write.csv(pairwise, "qpen_10plusESVs_allcomparisons_7Feb2023.csv")
```

###### b) environmental distance within each zone

```{r}
require(stringr)
require(Hmisc)
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)

comps<-comps_backup

comps$d_val<-comps$env_dist

#make breaks for zone A
comps_tmp<-comps[comps$s1_zone=="A" & comps$s2_zone=="A",]

comps_tmp<-comps_tmp[comps_tmp$d_val!=0,]
comps_tmp$d_val_group_breaks<-cut2(comps_tmp$d_val, g=10)
levels(comps_tmp$d_val_group_breaks)
# [1] "[0.0162,0.117)" "[0.1167,0.163)" "[0.1632,0.210)" "[0.2097,0.240)" "[0.2399,0.276)"
#  [6] "[0.2759,0.323)" "[0.3234,0.384)" "[0.3835,0.454)" "[0.4543,0.540)" "[0.5399,0.722]"
breaks<-c(as.numeric(gsub("\\]","" ,gsub("\\[","",unlist(str_split(as.character(levels(comps_tmp$d_val_group_breaks)), pattern=",")))[c(seq(1, 20, 2),20)])))

#make sure first value is below the minimum env_distance measure that isn't 0
min(comps_backup[comps_backup$env_dist!=0,]$env_dist)
#[1] 0.01621537

# make a bottom break (0.016 - just below the lowest env_dist) that is above zero but below the first break
breaks<-c(0.016,breaks[2:11]) 

full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)

zone<-c("A","B","C")
j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

#breaks_labels<- levels(comps$d_val_group_breaks)
breaks_labels<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="env"))
  }
  }

full_output<-rbind(full_output,tally)
}
#check to make sure they add to one
range(full_output$n)
#[1]  171 2020 #11 jan 2023
mean(full_output$n)
#947.6364 #11 jan 2023


write.csv(full_output, "qpen_10plusESVs_envdist_8Feb2023.csv")

```

###### c) geographic distance

```{r}
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)
comps<-comps_backup


comps[comps$s1_lake_id==comps$s2_lake_id & comps$s1_lake_drive!=comps$s2_lake_drive,]$geo_dist_km<-0.001 #add one meter for cores in the same lake
comps$d_val<-comps$geo_dist_km
breaks<-c(seq(0,15,5), seq(100, 300, 200), seq(400,500,100))  ##nothing from  43.30254  174.6661
breaks<-c(0,0.1, breaks[2:8])
full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)


zone<-c("A","B","C")
j=1
for( j in 1:3) {
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

breaks_labels<- c("[0,0]",levels(cut(comps$d_val, breaks=breaks)))

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="geo"))
  }
  }
full_output<-rbind(full_output,tally)
#count the number of comparisons in each zone
}

range(full_output$n)
#[1]  75 2604 # 11 Jan 2023
mean(full_output$n)
#1158.222
print(breaks_labels)

write.csv(full_output, "qpen_10plusESVs_geodist_8Feb2023.csv")
```

###### d) combine the two dataframes

```{r}
geo_dist_tally<-read.csv("qpen_10plusESVs_geodist_8Feb2023.csv", header=T, row.names=1)
env_dist_tally<-read.csv("qpen_10plusESVs_envdist_8Feb2023.csv", header=T, row.names=1)
full_output<-rbind(geo_dist_tally,env_dist_tally)

full_output[full_output$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
full_output[full_output$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
full_output[full_output$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
full_output[full_output$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
full_output[full_output$process=="Undominated",]$process<-"C_Drift"


#add in color
full_output$zone_col<-rep('#018571',nrow(full_output))
full_output[full_output$zone=="B",]$zone_col<-"#C4AD79"
full_output[full_output$zone=="C",]$zone_col<-'#a6611a'
        


#add in xlab as dist column
full_output[full_output$dist=="env",]$dist<-'Environmental dissimilarity'
full_output[full_output$dist=="geo",]$dist<-'Distance (km)'

write.csv(full_output,"qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv")

#if count needed
count<-unique(full_output[,c("d_val_group","zone","n","dist")])
write.csv(count, "qpen_10plusESVs_count_summary_8Feb2023.csv")
```

###### e) plot env and geo dist

option to plot all five processes, or just the stochastic processes

```{r}
require(patchwork)
require(ggplot2)

full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
full_output$proportion<-full_output$percent
full_output$percent<-full_output$percent*100

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
#new_colors<-c("#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[1]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")

#full_output<-full_output[full_output$process%in%sort(unique(full_output$process))[3:5],]

ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
for(i in 1:2){
        sub2<-sub[sub$dist==unique(sub$dist)[i],]       
ifelse(j==3, 
       
       
       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) +
                theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       
       , 
       
       
              ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15))+ #change font size of legend title
        theme(axis.title.x=element_blank(),
        axis.text.x=element_blank()) +
        theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       )
        }}


#pdf("Figures/qpen/10plusESVs/Fig6_community_assembly_NOSELECTION_14Feb2023.pdf",height=7,width=7)
pdf("Figures/qpen/10plusESVs/Fig6_community_assembly_14Feb2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
  plot_layout(ncol = 2)
dev.off()


```

###### f) plot sediment distance

```{r}
require(Hmisc)
qpn<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)

#subset by samples within the same core
downcore<-qpn[is.na(qpn$abs_cm)==F & qpn$abs_cm<27 ,]


downcore$d_val<-abs(downcore$abs_cm)
downcore$d_val_group_breaks<-cut2(downcore$d_val, g=10)
downcore$d_val_group<-as.numeric(cut2(downcore$d_val, g=10))
levels(downcore$d_val_group_breaks)
break_labels<- c("0 to <2.5" ,"2.5 to <5" ,"5 to <7", "7 to <9" ,"9 to <11" ,"11 to <15"
,"15 to <18" ,"18 to <22" ,"22 to <26")
#these are put into 9 groups here instead of the requested 10, must be to make it even.

tmp<-downcore
tally<-data.frame(process=NULL,percent=NULL, d_val_group=NULL,n=NULL)
groups<-sort(unique(downcore$d_val_group))
i=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  processes<-as.data.frame(table(tmp2$process)/nrow(tmp2))
  colnames(processes)[1]<-"process"
  colnames(processes)[2]<-"percent"
  processes$process<-as.character(processes$process)
  processes$d_val_group<-rep(groups[i],nrow(processes))
  processes$n<-rep(nrow(tmp2),nrow(processes))
  tally<-rbind(tally,processes)
}
  
#write.csv(tally,"qpen_sedimentdistance_13Feb2023.csv")

tally$proportion<-tally$percent
tally$percent<-tally$percent*100

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

        ggplt <- ggplot(tally,            
                        aes(x = d_val_group,
                            y = percent,
                            color= process)) +  geom_line(linewidth=2) +geom_point(size=3) + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
                                                                                                                             labels=break_labels) + xlab(label = "Sediment distance (cm)")+ ylab(label = "Proportion (%)") + geom_line(linewidth=2) +   labs(color="Assembly process")    + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) + theme(axis.text.x = element_text(angle = 45,hjust=1))+ guides(color = guide_legend(override.aes = list(linewidth = 6))) + theme(legend.position="none")+
                theme(axis.text=element_text(color="black"))+
                theme(axis.text.x = element_text(angle = 60,hjust=1))+
                theme(text=element_text(size=14), #change font size of all text
                      axis.text=element_text(size=14), #change font size of axis text
                      axis.title=element_text(size=15), #change font size of axis titles
                      plot.title=element_text(size=14), #change font size of plot title
                      legend.text=element_text(size=14), #change font size of legend text
                      legend.title=element_text(size=15),
                      panel.grid.minor = element_blank(), 
                      panel.grid.major.x = element_blank()) #change font size of legend title 

pdf("Figures/qpen/10plusESVs/Fig6A_Continuous_EAP_sediment_distance_8Feb2023.pdf", height=2.9, width=3.3)
ggplt
dev.off()

unique(tally$n)
#[1] 372 352 302 283 239 403 163 236 190

```

#### 2) GAMS without 0

doesn't include comparisons within the same lake 8Feb2023 
the labeling for geographic distance is off (double check if going to put in supplementary figures)

```{r}
require(mgcv)
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")

full_output<-full_output[full_output$d_val_group!=0,] #GAMs without 0 (without comparisons from the same lake)
tmp1<-full_output[full_output$dist=="Environmental dissimilarity",]
tmp2<-full_output[full_output$dist=="Distance (km)",]
tmp2<-tmp2[tmp2$d_val_group!=1,]
full_output<-rbind(tmp2,tmp1)


model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_qpen_",unique(full_output$dist)[d],"_GAM_8Feb2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels[[d]])),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
        
}}
dev.off()
}
write.csv(model_results, "qPEN_CommunityAssembly_GAMS_without0_8Feb2023.csv")

```

sediment distance (with zero)

```{r}

tally<-read.csv("qpen_sedimentdistance_13Feb2023.csv", header=T,row.names=1)

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
break_labels<- c("0 to <2.5" ,"2.5 to <5" ,"5 to <7", "7 to <9" ,"9 to <11" ,"11 to <15"
,"15 to <18" ,"18 to <22" ,"22 to <26")

model_results<-data.frame(process=NULL, rsq=NULL, p=NULL)


require(mgcv)
#Plot the sediment depth GAM for each process
pdf("Figures/qpen/10plusESVs/Fig7_seddepth_GAM_14Feb2023.pdf", height=3, width=10)
par(mfrow=c(1,5),mar=c(6,4,2,1))
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="black", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels)),1), las=2, labels = break_labels)
title(xlab=c("","","Sediment distance (cm)","","")[i], cex.lab=2, line=4)
title(ylab=c("Proportion","","","","")[i], cex.lab=2)

Data<-data.frame(x=sub$d_val_group, y=sub$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=new_colors[i], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(new_colors[i], alpha.f = 0.40), border = NA)
        gam_res<-summary(Gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
        model_results<-rbind(model_results,data.frame(process=sort(unique(tally$process))[i], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
}
dev.off()

round(mean(model_results$rsq),digits=3)
```

#### 3) Stochastic vs. Deterministic

```{r}
require(patchwork)

full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

new_colors<-c("#3CB371","#6495ED")

break_labels<-list()
break_labels[[2]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[1]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
                     "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
                     "0.45 to <0.54" ,"0.54 to 0.72")


full_output$stoc_deter<-rep(NA,nrow(full_output))

full_output[full_output$process=="B_Homogeneous selection",]$stoc_deter<-"Determinisitic"
full_output[full_output$process=="A_Variable selection",]$stoc_deter<-"Determinisitic"
full_output[full_output$process=="E_Dispersal limitation",]$stoc_deter<-"Stochastic"
full_output[full_output$process=="D_Homogenizing dispersal",]$stoc_deter<-"Stochastic"
full_output[full_output$process=="C_Drift",]$stoc_deter<-"Stochastic"


i=1
j=1
k=1
d=1
stoch_deter<-data.frame(n=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL,stoc_deter=NULL)
for(d in 1:2){
        sub<-full_output[full_output$dist==c("Environmental dissimilarity","Distance (km)")[d],]
for(i in 1:length(unique(sub$d_val_group))){
        tmp<-sub[sub$d_val_group==sort(unique(sub$d_val_group))[i],]
        for(j in 1:3){
                tmp2<-tmp[tmp$zone==c("A","B","C")[j],]
        for(k in 1:2){
                tmp3<-tmp2[tmp2$stoc_deter==c("Determinisitic", "Stochastic")[k],]
                stoch_deter<-rbind(stoch_deter, data.frame(n=tmp3$n[1],percent=sum(tmp3$percent),d_val_group=tmp3$d_val_group[1], d_val_inclthisnumandbelow=tmp3$d_val_inclthisnumandbelow[1], zone=tmp3$zone[1],dist=tmp3$dist[1],stoc_deter=tmp3$stoc_deter[1]))
                                   
        }}}}
full_output<-stoch_deter


ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
        for(i in 1:2){
                sub2<-sub[sub$dist==unique(sub$dist)[i],]       
                ifelse(j==3, 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) +  geom_point(size=3)+ geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,0.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15)) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                       
                       , 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15))+ #change font size of legend title
                               theme(axis.title.x=element_blank(),
                                     axis.text.x=element_blank()) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                )
        }}


pdf("Figures/qpen/10plusESVs/Stochastic_deterministic_community_assembly_8Feb2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
        plot_layout(ncol = 2)
dev.off()

```

#### 4) Selection, dispersal, drift

```{r}
require(patchwork)

full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

new_colors<-c("#A9D18E","#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[2]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[1]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
                     "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
                     "0.45 to <0.54" ,"0.54 to 0.72")


full_output$stoc_deter<-rep(NA,nrow(full_output))

full_output[full_output$process=="B_Homogeneous selection",]$stoc_deter<-"A_Determinisitic"
full_output[full_output$process=="A_Variable selection",]$stoc_deter<-"A_Determinisitic"
full_output[full_output$process=="E_Dispersal limitation",]$stoc_deter<-"D_Dispersal limitation"
full_output[full_output$process=="D_Homogenizing dispersal",]$stoc_deter<-"C_Homogenizing dispersal"
full_output[full_output$process=="C_Drift",]$stoc_deter<-"B_Drift"


i=1
j=1
k=1
d=1
stoch_deter<-data.frame(n=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL,stoc_deter=NULL)
for(d in 1:2){
        sub<-full_output[full_output$dist==c("Environmental dissimilarity","Distance (km)")[d],]
for(i in 1:length(unique(sub$d_val_group))){
        tmp<-sub[sub$d_val_group==sort(unique(sub$d_val_group))[i],]
        for(j in 1:3){
                tmp2<-tmp[tmp$zone==c("A","B","C")[j],]
        for(k in 1:4){
                tmp3<-tmp2[tmp2$stoc_deter==c("A_Determinisitic", "B_Drift", "C_Homogenizing dispersal", "D_Dispersal limitation")[k],]
                stoch_deter<-rbind(stoch_deter, data.frame(n=tmp3$n[1],percent=sum(tmp3$percent),d_val_group=tmp3$d_val_group[1], d_val_inclthisnumandbelow=tmp3$d_val_inclthisnumandbelow[1], zone=tmp3$zone[1],dist=tmp3$dist[1],stoc_deter=tmp3$stoc_deter[1]))
                                   
        }}}}
full_output<-stoch_deter


ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
        for(i in 1:2){
                sub2<-sub[sub$dist==unique(sub$dist)[i],]       
                ifelse(j==3, 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) +  geom_point(size=3)+ geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15)) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                       
                       , 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15))+ #change font size of legend title
                               theme(axis.title.x=element_blank(),
                                     axis.text.x=element_blank()) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                )
        }}


pdf("Figures/qpen/10plusESVs/lightgreen_Deterministic_dispersal_drift_community_assembly_8Feb2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
        plot_layout(ncol = 2)
dev.off()

```

#### 5) CAP sediment characteristics

```{r}
load("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/Calder_vsearch/16S/WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/Calder_vsearch/16S/WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(metagMisc)

#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","process")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"process[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
table(colnames(dfr)==metadata$samp_names) # all true


#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("pH"
, "d_13_c"  
, "cn"
, "sulfur_perc"     
, "water_perc"     
, "protein_per") 

#pull out metadata

metadata_sub<-metadata[,names(metadata)%in%variables]

table(colnames(dfr)==rownames(metadata_sub)) # all true 


daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))
table(colnames(daisy.mat)==colnames(dfr)) # all true
table(rownames(daisy.mat)==rownames(dfr)) # all true


daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

env_dist_reexpand <- reshape(env_dist, direction="wide", idvar="Var1", timevar="Var2")
rownames(env_dist_reexpand)<-env_dist_reexpand$Var1
env_dist_reexpand$Var1<-NULL
colnames(env_dist_reexpand)<-str_split_fixed(colnames(env_dist_reexpand),"value[.]",n=2)[,2]

#figure out which ones were different
setdiff(rownames(env_dist_reexpand),colnames(env_dist_reexpand))
setdiff(colnames(env_dist_reexpand),rownames(env_dist_reexpand))
env_dist_reexpand$SV0226L<-rep(NA,nrow(env_dist_reexpand))
env_dist_reexpand[nrow(env_dist_reexpand)+1,] <- NA
rownames(env_dist_reexpand)[nrow(env_dist_reexpand)]<-"33_1_10_DNA"


env_dist_reexpand<-env_dist_reexpand[order(row.names(env_dist_reexpand)), ]
table(rownames(env_dist_reexpand)==colnames(env_dist_reexpand)) #all true

#how many NAs in each and overlap for metadata
#metadata_sub[,metadatacolnames(env_dist_reexpand)]
met<-metadata_sub[rownames(metadata_sub)%in% colnames(env_dist_reexpand),]
#pH, cn and percent water are present in most samples, but then half have d13c and half the sulfur and protein percent

#subset the process/dfr data frame by the comparisons in metadata
dfr<-dfr[colnames(env_dist_reexpand),colnames(env_dist_reexpand)]

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)==rownames(env_dist_reexpand)) #all true
table(colnames(dfr)==colnames(env_dist_reexpand)) #all true

#melt back to pairwise comparisons
env_dist<-melt(as.matrix(env_dist_reexpand))
process<-melt(as.matrix(dfr))


table(process$Var1==env_dist$Var1) # all true
table(process$Var2==env_dist$Var2) # all true 
process$env_dist<-env_dist$value
process<-process[complete.cases(process),]


pairwise<-process #overwrite original file

names(pairwise)[1]<-"s1_samp_names"
pairwise$s1_samp_names<-as.character(pairwise$s1_samp_names)
names(pairwise)[2]<-"s2_samp_names"
pairwise$s2_samp_names<-as.character(pairwise$s2_samp_names)
names(pairwise)[3]<-"process"

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

pairwise<-merge(pairwise, metadata_s1, by="s1_samp_names")
pairwise<-merge(pairwise, metadata_s2, by="s2_samp_names")

#remove metadata notes
pairwise$s1_notes<-NULL
pairwise$s2_notes<-NULL
pairwise$s1_Notes_sed_water_wt<-NULL
pairwise$s2_Notes_sed_water_wt<-NULL
pairwise$s1_Notes_carbon_nitrogen<-NULL
pairwise$s2_Notes_carbon_nitrogen<-NULL
pairwise$s1_Notes._lake_sed_pH<-NULL
pairwise$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
pairwise$abs_cm<-rep(NA, nrow(pairwise))
i=1
for(i in 1:nrow(pairwise)){
  ifelse(pairwise$s1_lake_drive[i]==pairwise$s2_lake_drive[i],pairwise$abs_cm[i]<-abs(pairwise$s2_depth[i]-pairwise$s1_depth[i]),NA)
}

# calculate geographic distance

require(geosphere)
for(i in 1:nrow(pairwise)){
        pairwise$geo_dist[i]<-as.numeric(distm(data.frame(X = pairwise$s1_longitude[i], Y = pairwise$s1_latitude[i]),data.frame(X = pairwise$s2_longitude[i], Y = pairwise$s2_latitude[i])))
}

pairwise$geo_dist_km<-pairwise$geo_dist/1000

write.csv(pairwise, "qpen_10plusESVs_sediment_characteristics_7Feb2023.csv")
```

plot all zones together

```{r}
require(Hmisc)
qpn<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/Calder_vsearch/16S/WYLakeSedMicrobes_old/qpen_10plusESVs_sediment_characteristics_7Feb2023.csv", header=T, row.names=1)

#subset by samples within the same core
#downcore<-qpn[is.na(qpn$abs_cm)==F & qpn$abs_cm<27 ,]
downcore<-qpn

downcore$d_val<-downcore$env_dist
downcore$d_val_group_breaks<-cut2(downcore$d_val, g=10)
downcore$d_val_group<-as.numeric(cut2(downcore$d_val, g=10))
levels(downcore$d_val_group_breaks)
break_labels<- c("0 to <0.03" ,"0.03 to <0.05" ,"0.05 to <0.06", "0.06 to <0.08" ,"0.08 to <0.10" ,"0.10 to <0.13"
,"0.13 to <0.15" ,"0.15 to <0.20" ,"0.20 to <0.28", "0.28 to 1")
# [ includes ( up to 

tmp<-downcore
tally<-data.frame(process=NULL,percent=NULL, d_val_group=NULL,n=NULL)
groups<-sort(unique(downcore$d_val_group))
i=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  processes<-as.data.frame(table(tmp2$process)/nrow(tmp2))
  colnames(processes)[1]<-"process"
  colnames(processes)[2]<-"percent"
  processes$process<-as.character(processes$process)
  processes$d_val_group<-rep(groups[i],nrow(processes))
  processes$n<-rep(nrow(tmp2),nrow(processes))
  tally<-rbind(tally,processes)
}
  

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

        ggplt <- ggplot(tally,            
                        aes(x = d_val_group,
                            y = percent,
                            color= process)) +  geom_line(linewidth=2) +geom_point(size=3) + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
                                                                                                                             labels=break_labels) + xlab(label = "Sediment dissimilarity")+ ylab(label = "Proportion") + geom_line(linewidth=2) +   labs(color="Assembly process")    +theme(axis.ticks.x = element_blank()) + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.75) + theme(axis.text.x = element_text(angle = 45,hjust=1))+ guides(color = guide_legend(override.aes = list(linewidth = 6))) +
                theme(axis.text=element_text(color="black"))+
                theme(axis.text.x = element_text(angle = 60,hjust=1))+
                theme(text=element_text(size=14), #change font size of all text
                      axis.text=element_text(size=14), #change font size of axis text
                      axis.title=element_text(size=15), #change font size of axis titles
                      plot.title=element_text(size=14), #change font size of plot title
                      legend.text=element_text(size=14), #change font size of legend text
                      legend.title=element_text(size=15),
                      panel.grid.minor = element_blank(), 
                      panel.grid.major.x = element_blank()) #change font size of legend title 

pdf("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/Calder_vsearch/16S/WYLakeSedMicrobes_old/Figures/qpen/10plusESVs/SFig3_sediment_characteristics_10July2023.pdf", height=3, width=6)
ggplt
dev.off()

unique(tally$n)
#[1] 372 352 302 283 239 403 163 236 190
#for some reason the unique tally is different and only reporting two numbers now...  4277 4276
but still plotting the same figure

```

#### 6) t-test for differences within and across lakes

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

#indicate within or across lakes
full_output$inlake<-rep("no", nrow(full_output))
full_output[full_output$d_val_group%in%c(0,1) & full_output$dist=="Distance (km)",]$inlake<-"yes"
full_output[full_output$d_val_group==0 & full_output$dist=="Environmental dissimilarity",]$inlake<-"yes"



i=1
k=2
t_result<-data.frame(dist=NULL, process=NULL, pval=NULL)
for(i in 1:2){
        sub<-full_output[full_output$dist==unique(full_output$dist)[i],]
for(k in 1:5){
        sub2<-sub[sub$process==unique(sub$process)[k],]
        within_lake<-sub2[sub2$inlake=="yes",]
        across_lake<-sub2[sub2$inlake=="no",]
       result<-t.test(across_lake$percent, within_lake$percent, var.equal = F)
      t_result<-rbind(t_result,data.frame(dist=sub2$dist[1], process=sub2$process[1], pval=result$p.value))
}}

t_result$sig<-rep(NA, nrow(t_result))

t_result[t_result$pval<0.05,]$sig<-"sig"
t_result[t_result$pval>=0.05,]$sig<-"not sig"

max(t_result[t_result$sig=="sig",]$pval)
#[1] 0.04340025
mean(t_result[t_result$sig=="sig",]$pval)
#[1] 0.01167037

```

#### 7) mantel tests of RCBray values

These all need to be dist objects

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(vegan)


#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","RC")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"RC[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)%in%metadata$samp_names) #all true
table(colnames(dfr)%in%metadata$samp_names) # all true

RCbray<-as.dist(dfr)

#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
metadata_sub<-metadata[,names(metadata)%in%variables]


table(colnames(dfr)==metadata$samp_names) # all true
table(colnames(dfr)==rownames(metadata_sub)) # all true 

env_dist <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))
table(labels(RCbray)==labels(env_dist))


# geographic distance

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
geo_dist<-as.dist(dist_m_output)

#check to make sure all labels match!
table(labels(RCbray)==labels(env_dist)) #all true
table(labels(RCbray)==labels(geo_dist)) #all true

mantel(RCbray, env_dist)
mantel(RCbray, geo_dist)

#add partial mantel test, spatially structured environmental variables
mantel.partial(RCbray, env_dist, geo_dist, method = "pearson", permutations = 999)
mantel.partial(RCbray, geo_dist, env_dist, method = "pearson", permutations = 999)
```

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = env_dist)

Mantel statistic r: 0.246 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.00917 0.01263 0.01589 0.01806 Permutation: free Number of permutations: 999

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = geo_dist)

Mantel statistic r: 0.2047 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0124 0.0165 0.0197 0.0241 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = env_dist, zdis = geo_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.2433 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0103 0.0130 0.0147 0.0179 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = geo_dist, zdis = env_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.2014 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0120 0.0157 0.0193 0.0256 Permutation: free Number of permutations: 999

#### 8) stats of community assembly processes

```{r}
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
round(qp2$ratio, digits=3)*100
#   Heterogeneous.Selection Homogeneous.Selection Dispersal.Limitation
# 1                    26.1                  39.2                 31.4
#   Homogenizing.Dispersal Undominated num.pair
# 1                    1.3         2.1 11400300

#selection
0.2610984 +0.3918757 
# 0.6529741
#round 65.3%

#dispersal
0.3137286 + 0.01266633 +  0.02063104
#0.347026
#round 34.7%
```

fig 6 A (sed distance)

```{r}
full_output<-read.csv("qpen_sedimentdistance_13Feb2023.csv", header=T,row.names=1)
i=1
for( i in 1:5){
sub<-full_output[full_output$d_val_group==1 & full_output$process==sort(unique(full_output$process))[i],]
print(sub$process[1])
print(signif(sub$percent, digits=3)*100)
}

i=1
for( i in 1:5){
sub<-full_output[full_output$d_val_group%in%c(8,9) & full_output$process==sort(unique(full_output$process))[i],]
print(sub$process[1])
print(signif(sub$percent, digits=3)*100)
}

```

Within an individual sediment core, samples \<= 2.5 cm apart were either homogenized (69.9%) or differentiated by selection (5.9%), regardless of horizon. As the distance between samples increased, homogenous selection declined and variable selection increased (Fig. 6, A). At sediment distances \>18 cm (18-22 and 22-26 cm), where all pairs of samples were compared across zones, variable selection predominated (34.4-43.2%), Homogenizing dispersal was highest between comparisons 0-7 cm apart (15.9-18.9%), and declined to 4.7% with increasing sediment distance. Dispersal limitation increased with sediment distance, reaching 26.8% at 22-26 cm. Drift played a minor role (\<10%) across all sediment distances.

figure 6 B and C

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)



#1) same lake (environment)

#Homogenous selection
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="B_Homogeneous selection", ]$percent), digits=3)*100


#variable selection
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="A_Variable selection", ]$percent), digits=3)*100


#disp lim
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="E_Dispersal limitation", ]$percent), digits=3)*100


#homog disp
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="D_Homogenizing dispersal", ]$percent), digits=3)*100


# [1] 60.3
# [1] 9.5
# [1] 8
# [1] 19.8

#or signif

# [1] 60.3
# [1] 9.51
# [1] 8.01
# [1] 19.8
```

Manuscript text:

Homogenous selection was dominant among comparisons within the same horizon in any given lake (mean across horizons, 60.3%), while variable selection accounted for 9.5%, homogenizing dispersal 19.8%, and dispersal limitation 8.0% of such intra-lake, intra-horizon comparisons (Fig. 6, B and C).

inter-lake comparisons (averaged across environmental distance and geographic distance)

```{r}

'%!in%' <- function(x,y)!('%in%'(x,y))

for( i in 1:5){

sub<-rbind(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%!in%c(0) & full_output$process==sort(unique(full_output$process))[i],]
      , 
      full_output[full_output$dist=="Distance (km)" & full_output$d_val_group%!in%c(0,1) & full_output$process==sort(unique(full_output$process))[i],])

print(sub$process[1])
print(signif(mean(sub$percent),digits=3)*100)
print(signif(range(sub$percent),digits=3)*100)
}


```

[1] "A_Variable selection" [1] 16.9 [1] 4.02 34.60

[1] "B_Homogeneous selection" [1] 47.3 [1] 23.2 71.7

[1] "C_Drift" [1] 2.85 [1] 0.0 11.1

[1] "D_Homogenizing dispersal" [1] 1.29 [1] 0.00 7.23

[1] "E_Dispersal limitation" [1] 31.6 [1] 15.8 46.3

As environmental dissimilarity and geographic distance increased with inter-lake comparisons, homogenizing dispersal declined while variable selection and dispersal limitation increased, however homogeneous selection was often the most dominant community assembly process in all horizons. Homogeneous selection ranged from 23.2-71.7% (avg. 47.3%), variable selection 4.0-34.6% (avg. 16.9%), and dispersal limitation 15.8-46.3% (avg. 31.6%). Homogenizing dispersal (mass effects) and drift acting alone was negligible (avg. 1.3 and 2.9%, respectively) across comparisons of abiotic lake environments and geographic distances.

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
'%!in%' <- function(x,y)!('%in%'(x,y))
z=1
i=1
for( i in 1:5){
sub<-rbind(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%!in%c(0) & full_output$process==sort(unique(full_output$process))[i],]
      , 
      full_output[full_output$dist=="Distance (km)" & full_output$d_val_group%!in%c(0,1) & full_output$process==sort(unique(full_output$process))[i],])

for(z in 1:3){
    sub2<-sub[sub$zone==c("A","B","C")[z],]    


print(sub2$process[1])
print(sub2$zone[1])
print(signif(mean(sub2$percent),digits=3)*100)
print(signif(range(sub2$percent),digits=3)*100)
}}
```

[1] "B_Homogeneous selection" [1] "A" [1] 37.7

[1] "B_Homogeneous selection" [1] "B" [1] 44.8

[1] "B_Homogeneous selection" [1] "C" [1] 59.4

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
'%!in%' <- function(x,y)!('%in%'(x,y))
z=1
for(z in 1:3){
    sub<-full_output[full_output$zone==c("A","B","C")[z],]  

sub2<-rbind(sub[sub$dist=="Environmental dissimilarity" & sub$d_val_group%!in%c(0) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],]
      , 
      sub[sub$dist=="Distance (km)" & sub$d_val_group%!in%c(0,1) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],])

 
print(sub2$zone[1])
print(signif(range(sub2$percent),digits=3)*100)
}
```

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
'%!in%' <- function(x,y)!('%in%'(x,y))
i=1
    sub<-full_output[full_output$zone=="C",]
sub2<-rbind(sub[sub$dist=="Environmental dissimilarity" & sub$d_val_group%!in%c(0) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],]
      , 
      sub[sub$dist=="Distance (km)" & sub$d_val_group%!in%c(0,1) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],])

hs_de<-NULL
hs_vs<-NULL
for (i in 1:length(unique(sub2$d_val_group))){
 sub3<-sub2[sub2$d_val_group==sort(unique(sub2$d_val_group))[i],]

hs_de<-c(hs_de,sub3[sub3$process=="B_Homogeneous selection",]$percent/ sub3[sub3$process=="E_Dispersal limitation",]$percent)
hs_vs<-c(hs_vs,sub3[sub3$process=="B_Homogeneous selection",]$percent/ sub3[sub3$process=="A_Variable selection",]$percent)
    
}
        print(sub3$zone[1])
          print("hs_de")
print(signif(mean(hs_de),digits=3))   
print(signif(range(hs_de),digits=3)) 
          print("hs_vs")
print(signif(mean(hs_vs),digits=3))  
print(signif(range(hs_vs),digits=3)) 

```

[1] "C" [1] "hs_de" [1] 2.19 [1] 1.26 4.53 [1] "hs_vs" [1] 6.71 [1] 3.34 14.20

#### 9) community assembly of deeper samples

```{r}
require(ggplot2)

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)
comps<-comps_backup
comps$d_val<-comps$abs_cm
comps<-comps[is.na(comps$d_val)==F,]
comps<-comps[comps$d_val>26,] # 119 comparisons, so no many compared to the other ones

tmp2<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL)
proc<-unique(comps_backup$process)
for(p in 1:5){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2)))
  }
tally$group<-rep("deep", nrow(tally))

EAP_plot<-ggplot(tally, aes(x = group, y = percent , fill = process)) +
    geom_bar(stat="identity") + scale_fill_manual(values=new_colors,labels= c("Variable selection", "Homogeneous selection", "Drift", "Homogenizing dispersal", "Dispersal limitation" )) 
pdf("Figures/qpen/10plusESVs/SFig7_deep_community_assembly.pdf", height=4, width=4)
EAP_plot
dev.off()
```

## 22. trends in trophic status of 8 lakes

```{r}
troph<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/JordansLakesTrophicStatus.csv", header=T,row.names=1)


require(ggplot2)
my_date <- as.POSIXct("10/14/20", format="%m/%d/%y")
as.numeric(my_date)
tmp$yday

gg<-ggplot(troph, aes(x=as.numeric(as.POSIXct(date, format="%m/%d/%y")), y=pred_numeric, col=lake_name)) + geom_point() + geom_line() +   facet_wrap(vars(lake_name)) + xlab("date") + ylab("trophic status (1: olig 3: eutro)") + geom_smooth(method = "lm", se = FALSE)


gg
```

## 23. shared ESVs and shared families

will all reads

```{r}
require(Matrix)
require(metagMisc)
require(phyloseq)
require(reshape2)
require(cluster)
require(stringr)
require(geosphere)

load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

#calculate
shared_esvs <- phyloseq_num_shared_otus(ps)


#shared ESVs
shared<-as.matrix(shared_esvs[["shared"]])
shared[upper.tri(shared, diag = T)] <- NA
shared<-melt(shared,na.rm=TRUE)

colnames(shared)[3]<-"shared"

#nonshared ESVs
nonshared<-as.matrix(shared_esvs[["nonshared_total"]])
nonshared[upper.tri(nonshared, diag = T)] <- NA
nonshared<-melt(nonshared,na.rm=TRUE)
colnames(nonshared)[3]<-"nonshared"

table(nonshared$Var1==shared$Var1) #all T
table(nonshared$Var2==shared$Var2) #all T

shared$nonshared<-nonshared$nonshared
rm(nonshared)
shared$percent<-shared$shared/(shared$shared+shared$nonshared)


#now add in environmental distances
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(as.matrix(shared_esvs[["shared"]])))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))

daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

table(shared$Var1==env_dist$Var1) # all true
table(shared$Var2==env_dist$Var2) # all true 
shared$env_dist<-env_dist$value
 #overwrite original file

names(shared)[1]<-"s1_samp_names"
shared$s1_samp_names<-as.character(shared$s1_samp_names)
names(shared)[2]<-"s2_samp_names"
shared$s2_samp_names<-as.character(shared$s2_samp_names)

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

shared<-merge(shared, metadata_s1, by="s1_samp_names")
shared<-merge(shared, metadata_s2, by="s2_samp_names")

#remove metadata notes
shared$s1_notes<-NULL
shared$s2_notes<-NULL
shared$s1_Notes_sed_water_wt<-NULL
shared$s2_Notes_sed_water_wt<-NULL
shared$s1_Notes_carbon_nitrogen<-NULL
shared$s2_Notes_carbon_nitrogen<-NULL
shared$s1_Notes._lake_sed_pH<-NULL
shared$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
shared$abs_cm<-rep(NA, nrow(shared))
i=1
for(i in 1:nrow(shared)){
  ifelse(shared$s1_lake_drive[i]==shared$s2_lake_drive[i],shared$abs_cm[i]<-abs(shared$s2_depth[i]-shared$s1_depth[i]),NA)
}


# calculate geographic distance


for(i in 1:nrow(shared)){
        shared$geo_dist[i]<-as.numeric(distm(data.frame(X = shared$s1_longitude[i], Y = shared$s1_latitude[i]),data.frame(X = shared$s2_longitude[i], Y = shared$s2_latitude[i])))
}

shared$geo_dist_km<-shared$geo_dist/1000


#write.csv(shared,"sharedESVs_7Feb2023.csv")
```

```{r}
#comps_backup<-read.csv("sharedESVs_7Feb2023.csv",header=T,row.names=1)

#comps_backup$percent<-comps_backup$percent*100

zone<-c("A","B","C")
comps<-comps_backup
pdf("Figures/PercentSharedESVS_7Feb2023.pdf", width=7, height=18)
par(mfrow=c(7,3), mar=c(4,4,3,2))

j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Environmental dissimilarity", ylim=c(0,40),main=c("Redox", "Transition", "Depauperate")[j])
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$env_dist!=0,]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Environmental dissimilarity",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$geo_dist_km!=0,]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)", ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}}


j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$geo_dist_km>100,]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", xlab="Environmental dissimilarity",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}

j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$geo_dist_km>100,]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)", ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}
}

#sediment
comps<-comps_backup[is.na(comps_backup$abs_cm)==F & comps_backup$abs_cm<27 ,]
plot(comps$abs_cm, comps$percent, ylab="Shared ESVs (%)", xlab="Sediment distance (cm)", pch=16, col="#00000050",main="All horizons, individual cores")
abline(lm(comps$percent~comps$abs_cm), col="lightgreen", lwd=2)
if(summary(lm(comps$percent~comps$abs_cm))$coefficients[2,4]<0.05){
        text(x=(max(comps$abs_cm)*.95), max(comps$percent)*0.95,"*" ,cex=3)
}

dev.off()
```

## 24. Make ESV table with taxonomy and actual sequences for Eric Capo

```{r}
tax_tab<-as.data.frame(as.matrix(tax_table(ps)))
identical(otu_tab_10knorm, as.data.frame(as.matrix(otu_table(ps)))) #true
setdiff(rownames(tax_tab),rownames(otu_tab_10knorm)) # none
setdiff(rownames(otu_tab_10knorm),rownames(tax_tab)) # none
ESV_taxa<-merge(tax_tab,otu_tab_10knorm, by="row.names")
rownames(ESV_taxa)<-ESV_taxa$Row.names
ESV_taxa$Row.names<-NULL

seqs<-Biostrings::readDNAStringSet("zotus_nonchimeric.fa") #downloaded straight from the calder file folder
seqs@ranges@NAMES<-stringr::str_split(seqs@ranges@NAMES,pattern = ";", 3, simplify = T)[,1]

seqs_sub<-seqs[rownames(otu_tab_10knorm)]
df_seqs_sub<-as.data.frame(seqs_sub)
names(df_seqs_sub)[1]<-"sequence"
table(rownames(df_seqs_sub) %in% rownames(ESV_taxa))
table(rownames(ESV_taxa)%in%rownames(df_seqs_sub))

ESV_taxa_seq<-merge(df_seqs_sub,ESV_taxa, by="row.names")
rownames(ESV_taxa_seq)<-ESV_taxa_seq$Row.names
ESV_taxa_seq$Row.names<-NULL

write.csv(ESV_taxa_seq, "20230316_Seq_Taxa_ESV.csv")

```

## 25. esvs unique to a core

```{r}


require(metagMisc)
require(Matrix)

lake<-merge_samples(ps_tr, "lake_name")
lake

shared_esvs <- phyloseq_num_shared_otus(lake)

shared<-as.matrix(shared_esvs[["shared"]])
ESV<-as.data.frame(t(as.matrix(lake@otu_table@.Data)))


lake_taxa_present<-list()
i=1
for(i in 1:36){
        sub<-ESV[,i]
sub2<-as.data.frame(cbind(rownames(ESV),sub))
names(sub2)<-c("ESV","abundance")
which<-sub2[which(sub2$abundance!=0),]
listoftaxa<-which$ESV
lake_taxa_present[[i]]<-listoftaxa
}

num_unique<-NULL
for(j in 1:36){ 
        sub<-lake_taxa_present
        sub[[j]]<-NULL
        allelse<-unique(unlist(sub))
num_unique<-c(num_unique,table(lake_taxa_present[[j]]%in% allelse)[1])
}
range(num_unique/91957)
mean(num_unique/91957)
```


```{r}
#order OTUS to match the comps table
dfr <- reshape(comps[,names(comps)%in% c("s2_samp_names", "s1_samp_names","env_dist")], direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"env_dist[.]",n=2)[,2]
setdiff(rownames(metadata), colnames(dfr))
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
setdiff(rownames(metadata), rownames(dfr))
rownames(dfr)[478]<-"33_1_10_DNA"

#calculate Bray-Curtis similarity between all samples
comm.dist <- 1 - vegdist(OTU)
#convert to pairwise comparisons by making half of the distance matrix into NAs
comm.dist.mat<-as.matrix(comm.dist)
comm.dist.mat[upper.tri(comm.dist.mat, diag = T)] <- NA
comm.dist.mat<-comm.dist.mat[order(match(rownames(comm.dist.mat),colnames(dfr))),]
comm.dist.mat<-comm.dist.mat[order(match(colnames(comm.dist.mat),rownames(dfr))),]


comm<-reshape2::melt(comm.dist.mat, na.rm=T)

#check that these names match in comps dataframe
table(comps$s1_samp_names==comm$Var1) # all true
table(comps$s2_samp_names==comm$Var2) # all true 
comps$env_dist<-comm$value

```



Decontaminating using Eric Capos method


From what Eric Capo says: 
Based on the number of DNA reads found in the molecular inventories from the sampling, DNA extraction and PCR controls, 

we removed the XX OTUs with DNA reads in blank molecular inventories with sums equal or higher to 2 % of the total number of DNA reads in molecular inventories from the samples

otus with blank sums >= 2% of the total number of reads in the samples 


Including the blank with 42k reads
```{r}
load("DataCleaning/2024-01-16_TemporaryDataCleaning_Part2_sumtechrep.Rdata")

blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)

otu_table_no_blanks<-otu_table[, -blanks]
blanks_otu_table<-otu_table[, blanks]
rm(blanks)

blank_otu_reads<-as.data.frame(rowSums(blanks_otu_table))
names(blank_otu_reads)<-"blank_reads"
blank_otu_reads$otu<-rownames(blank_otu_reads)
rownames(blank_otu_reads)<-NULL

sample_reads<-as.data.frame(rowSums(otu_table_no_blanks))
names(sample_reads)<-"sample_reads"
sample_reads$otu<-rownames(sample_reads)
rownames(sample_reads)<-NULL

table(sample_reads$otu==blank_otu_reads$otu) # all true

decontam_tab<-cbind(sample_reads,blank_otu_reads)
decontam_tab[,4]<-NULL
decontam_tab$percent_blanks_of_samples<-decontam_tab$blank_reads/decontam_tab$sample_reads
decontam_tab$decontaminant<-ifelse(decontam_tab$percent_blanks_of_samples>=0.02,TRUE,FALSE)

table(decontam_tab$decontaminant)
#  FALSE   TRUE 
# 117993   2513 

contaminants<-decontam_tab[which(decontam_tab$decontaminant==TRUE),]$otu

# what percent are these contaminants of the entire dataset? 
sum(colSums(otu_table_no_blanks[rownames(otu_table_no_blanks)%in%contaminants,]))
# 148521 reads - 16 Jan 2024
sum(colSums(otu_table_no_blanks[rownames(otu_table_no_blanks)%in%contaminants,]))/sum(colSums(otu_table_no_blanks))
# [1] 0.00343798 # proportion of reads removed, SO SMOL!

```

Without the blank with 42K reads
```{r}
load("DataCleaning/2024-01-16_TemporaryDataCleaning_Part2_sumtechrep.Rdata")

blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)

otu_table_no_blanks<-otu_table[, -blanks]
blanks_otu_table<-otu_table[, blanks]
blanks_otu_table<-blanks_otu_table[,colnames(blanks_otu_table)!="JC_Blank2"] # removing the blank with 42k reads
rm(blanks)

blank_otu_reads<-as.data.frame(rowSums(blanks_otu_table))
names(blank_otu_reads)<-"blank_reads"
blank_otu_reads$otu<-rownames(blank_otu_reads)
rownames(blank_otu_reads)<-NULL


sample_reads<-as.data.frame(rowSums(otu_table_no_blanks))
names(sample_reads)<-"sample_reads"
sample_reads$otu<-rownames(sample_reads)
rownames(sample_reads)<-NULL

table(sample_reads$otu==blank_otu_reads$otu) # all true

decontam_tab<-cbind(sample_reads,blank_otu_reads)
decontam_tab[,4]<-NULL
decontam_tab$percent_blanks_of_samples<-decontam_tab$blank_reads/decontam_tab$sample_reads
decontam_tab$decontaminant<-ifelse(decontam_tab$percent_blanks_of_samples>=0.02,TRUE,FALSE)

table(decontam_tab$decontaminant)
#  FALSE   TRUE 
# 120467     39 

contaminants<-decontam_tab[which(decontam_tab$decontaminant==TRUE),]$otu

# what percent are these contaminants of the entire dataset? 
sum(colSums(otu_table_no_blanks[rownames(otu_table_no_blanks)%in%contaminants,]))
#only [1] 1090 reads
sum(colSums(otu_table_no_blanks[rownames(otu_table_no_blanks)%in%contaminants,]))/sum(colSums(otu_table_no_blanks))
# [1] 2.523144e-05 # proportion of reads removed, SO SMOL!

```





## 6. Island Biogeography for poster

Merge in the lake area measurements from Google Earth Pro polygons, plot whether lake depth increases with lake area. Generally true, but some exceptions of large shallow lakes. 
```{r}
lkarea<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/metadata_lakearea_27May2023.csv",header=T)

meta_merged<-merge(metadata,lkarea,by="lake_name")

#plot lake area by lake depth

uniq_lake<- meta_merged %>% select(lake_name,lakearea_squaremeters,max_lake_depth)
uniq_lake<-unique(uniq_lake)
plot(uniq_lake$lakearea_squaremeters,uniq_lake$max_lake_depth, pch=16, xlab="Lake area (sq m)",ylab="Max. lake depth")

```


```{r}
ESV<-as.data.frame(otu_table(ps))
ESV<-t(ESV)

#calculate the number of ESVs for each sediment core
LkDr_ESVs<-data.frame(lake_drive=NULL, num_samples=NULL,num_ESVs=NULL)
i=1
for(i in 1:length(unique(metadata$lake_drive))){
        #for each lake_drive, grab all the sample names in metadata
        samples<-metadata[metadata$lake_drive==unique(metadata$lake_drive)[i],]$samp_names
        #pull out number of ESVs from the table 
        sub_ESV<-as.data.frame(ESV[rownames(ESV)%in%samples,])  
        colsumsESV<-as.data.frame(colSums(sub_ESV))
       LkDr_ESVs<-rbind(LkDr_ESVs,data.frame(lake_drive=unique(metadata$lake_drive)[i], num_samples=length(samples),num_ESVs=colSums(colsumsESV != 0)))
}

#this didn't work if just one sample so updated the last two manually

LkDr_ESVs[47,3]<-2003
LkDr_ESVs[48,3]<-3570

```

Something to think about is that some lakes have MORE sediment samples, can plot this
```{r}
plot(LkDr_ESVs$num_samples, LkDr_ESVs$num_ESVs, pch=16, ylab="Total ESVs per core", xlab="Sediment samples")
#cores must have at least 5 samples

#remove lake_drives with less than 5 samples
LkDr_ESVs<-LkDr_ESVs[LkDr_ESVs$num_samples>4,]

#merge with lake metadata
uniq_lake<- meta_merged %>% select(lake_name,lakearea_squaremeters,max_lake_depth, lake_drive, mountain_range)
uniq_lake<-unique(uniq_lake)

#merge with lake metadata
isl_bio<-merge(LkDr_ESVs,uniq_lake, by="lake_drive")

isl_bio$col<-as.factor(isl_bio$mountain_range)
isl_bio$col<-as.numeric(isl_bio$col)
isl_bio$lakearea_squarekm<-isl_bio$lakearea_squaremeters/1000000
```

plot number of taxa with lake depth, lake size, and lake depth x size
```{r}
cols<-met.brewer(name="Egypt", n=4, type="discrete")
cols<-c("#000000",cols[1],cols[4],cols[2],cols[3])
full_cols<-cols
#cols<-paste(cols,"99", sep="")
point_types<-c(16,17,18,19,20)



pdf("Figures/ESVs_lakearea_29May2023.pdf", height=4,width=5)
par(mar=c(4,6,2,2))
expression<-expression(Lake ~ area ~ (km^2))
plot(isl_bio$lakearea_squarekm,isl_bio$num_ESVs, pch=16, cex=2, ylab="Exact sequence variants \n(ESVs) per core" ,col=cols[isl_bio$col], xlab=expression)
legend("topright",title="Mountain range", c("Beartooth" , "Bighorn"  ,  "Snowy" ,     "Wind River"), bty="n",pt.cex=2, col=full_cols, pch=16)
dev.off()

pdf("Figures/ESVs_lakevolume_29May2023.pdf", height=4,width=5)
par(mar=c(4,6,2,2))
expression<-expression(Lake ~ volume ~ (m^3))
plot((isl_bio$lakearea_squaremeters * isl_bio$max_lake_depth),isl_bio$num_ESVs, cex=2,pch=16, ylab="Exact sequence variants \n(ESVs) per core",col=cols[isl_bio$col], xlab=expression)
legend("topright",title="Mountain range", c("Beartooth" , "Bighorn"  ,  "Snowy" ,     "Wind River"), bty="n",pt.cex=2, col=full_cols, pch=16)
dev.off()
```




```{r}
plot(isl_bio$max_lake_depth,isl_bio$num_ESVs,  ylab="ESVs per core", xlab="Lake depth (m)", col=cols[isl_bio$col],pch=point_types[isl_bio$col])
```


