---
title: "WY Lake Sediment Prokaryotic Community Analyses"
author: "Jordan Von Eggers"
date: "20 Apr 2023"
output: html_document
editor_options: 
  chunk_output_type: console
---

#request resources in Beartooth

```{bash}
salloc --mem=80GB --nodes=1 --cpus-per-task=1 --account=microbiome --time=2:00:00
```

General notes: - Round lake didn't have a bottom water temperature recorded, but surface water was 20.1 degrees and the lake was 1.5 m so it is likely the bottom water temperature isn't too much different (around 20 deg C), so I used 20.1 as the temperature - mV for sample SR20122L: changed from 1121.6 to 121.6 since likely an error (pH matches perfectly and the surrounding measurements are between 107 and 134) -- but not a problem when looking at the ps object data, since this sample must have been removed for low reads or something - Only one truly replicated sample (LB0208L1 and LB0208L2)

# Part I: Extract files and assign taxonomy

## 1. Grab files off Beartooth supercomputer

copy the bash file used to make the files above! (from Alex Buerkle) move to folder: /Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/original files off Teton

```{bash}
rsync jvonegge@teton.uwyo.edu:/project/microbiome/data/seq/psomagen_17sep20_novaseq2/otu/run_slurm_mkotu.pl /Users/jordanscheibe/Desktop

#from: run_slurm_mkotu.pl - this is from april 28 (I assume) so likely the final one! and all the files are here so I can conclude it is this one
foreach my $tfmdirectory (sort keys %allprojectsotu){
    # print $tfmdirectory, " -- " , $allprojectsotu{$tfmdirectory}, "\n";
    $job = '';
    unless(-e $allprojectsotu{$tfmdirectory}){ # this is the otu directory
	mkpath $allprojectsotu{$tfmdirectory};
    }

    $job .= "cd $allprojectsotu{$tfmdirectory};\n"; # this is the otu directory
    $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --derep_fulllength - --threads 32 --output uniqueSequences.fa --sizeout;\n";
    $job .= "vsearch --cluster_unoise uniqueSequences.fa --relabel 'otu' --sizein --sizeout --consout zotus.fa --minsize 8 ;\n";
    $job .= "vsearch --uchime3_denovo zotus.fa --nonchimeras zotus_nonchimeric.fa --threads 32;\n";
    $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --usearch_global - --db zotus_nonchimeric.fa --otutabout - --id 0.99 --threads 32 | sed 's/^#OTU ID/OTUID/' > 'otutable';\n";
    $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --search_exact - --db zotus_nonchimeric.fa --otutabout - --threads 32 | sed 's/^#OTU ID/OTUID/' > 'otutable.esv';\n";
   $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --search_exact - --db zotus_nonchimeric.fa --biomout otu.esv.biom.json --threads 32\n";
    push @jobarray, $job, 
}

```

copy orginal files from the Micro Project to my folders but just grab: 1. otutable.esv 2. zotus_nonchimeric.fa

```{bash}
cd /project/microbiome/data/seq/psomagen_17sep20_novaseq2/otu/16S/Calder
cp * /project/seddna/jvonegge/WY_lake_microbes/16S/original_files
```


```{r}
rsync -r jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/original_files /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes

```

move all the other files aside from those two into an "other" folder

## 2. Assigning taxonomy


```{r}
rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/silva_nr99_v138.1_wSpecies_train_set.fa.gz jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/reference_database
PrayingMantus@0613@
```

Using the maintained datasets from DADA2 hosted on Zenodo

McLaren, Michael R., & Callahan, Benjamin J. (2021). Silva 138.1 prokaryotic SSU taxonomic training data formatted for DADA2 [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4587955

First, read in the fasta file with sequences, remove three duplicates (that aren't in the OTU table) and assign taxonomy using DADA2
```{r}

require(dada2)
require(Biostrings)
require(stringr)
fasta <- readDNAStringSet("OriginalFiles/zotus_nonchimeric.fa")

# I know there are 3 duplicates in this file, so checking if they are in the OTU table (they aren't), then deleting them
dups<-as.data.frame(fasta[duplicated(fasta)])

dups<-as.data.frame(fasta[which(as.character(fasta)%in% dups$x)])
removefromfasta<-rownames(dups)[4:6]

#quickly read in the OTU table - when we subset by these duplicates, we find that the duplicated sequences aren't in here, so not a problem, we will just remove them from the zotus_nonchimeric.fa file
# otu_table<-read.delim("OriginalFiles/otutable.esv", header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
# dups$centroid<-str_split(rownames(dups),pattern = ";", 3, simplify = T)[,1]
# dup_otu<-otu_table[which(otu_table$OTUID %in% dups$centroid),]
rm(otu_table)
rm(dups)
rm(dup_otu)

fasta<-fasta[!names(fasta) %in% removefromfasta]
rm(removefromfasta)

#Assign taxonomy
#This command takes a few hours
#with species
 

# #without species (just to genus)
# tax_table <- assignTaxonomy(fasta, "silva_nr99_v138.1_train_set.fa.gz")
# ESVseq<-data.frame(seq=as.character(fasta), names=names(fasta))
# ESVseq$ESV<-str_split(ESVseq$names,pattern = ";", 3, simplify = T)[,1]
# ESVseq$names<-NULL
# 
# tax_table<-merge(tax_table,df, by="seq")
# tax_table$seq<-NULL
# rownames(tax_table)<-tax_table$ESV
# tax_table$ESV<-NULL
# write.csv(tax_table,"Silva138_assignedTaxonomy_toGenus_27Apr2023.csv")
```

runR_assignTaxonomy_dada2.sh
```{bash}

#!/bin/bash
#SBATCH --job-name assign_tax_dada2
#SBATCH --mem=100GB
#SBATCH --time=3-00:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=assign_tax_dada2_%A.out
hostname; date
module load miniconda3/4.12.0
source activate dada2_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/assign_taxonomy/dada2
srun Rscript assignTaxonomy_dada2.R
echo "srun Rscript assignTaxonomy_dada2.R"
source deactivate dada2_env
echo "finished assigntax JV"
date
```



assignTaxonomy_dada2.R
```{r}
require(dada2)
require(Biostrings)
require(stringr)
fasta <- readDNAStringSet("/project/seddna/jvonegge/WY_lake_microbes/16S/original_files/zotus_nonchimeric.fa")

removefromfasta<-c("centroid=otu50237;seqs=1;size=53","centroid=otu52917;seqs=1;size=48", "centroid=otu55415;seqs=1;size=44")
fasta<-fasta[!names(fasta) %in% removefromfasta]
rm(removefromfasta)

tax_table <- assignTaxonomy(fasta, "/project/seddna/jvonegge/WY_lake_microbes/16S/reference_database/silva_nr99_v138.1_wSpecies_train_set.fa.gz")
write.csv(tax_table,"Silva138_assignedTaxonomy_toSpecies_temp1_28Apr2023.csv")
ESVseq<-data.frame(seq=as.character(fasta), names=names(fasta))
ESVseq$ESV<-str_split(ESVseq$names,pattern = ";", 3, simplify = T)[,1]
ESVseq$names<-NULL

tax_table<-merge(tax_table,df, by="seq")
tax_table$seq<-NULL
rownames(tax_table)<-tax_table$ESV
tax_table$ESV<-NULL
write.csv(tax_table,"Silva138_assignedTaxonomy_toSpecies_28Apr2023.csv")

```
This all assigned fine to the "Silva138_assignedTaxonomy_toSpecies_temp1_28Apr2023.csv" object and then I got an error afterwards in the outfile because of the merging (didn't change the tax_table column to "seq"), see below for correction


copy back to computer and add in the taxa names by merging by sequenes
```{r}
require(dada2)
require(Biostrings)
require(stringr)
fasta <- readDNAStringSet("OriginalFiles/zotus_nonchimeric.fa")
removefromfasta<-c("centroid=otu50237;seqs=1;size=53","centroid=otu52917;seqs=1;size=48", "centroid=otu55415;seqs=1;size=44")
fasta<-fasta[!names(fasta) %in% removefromfasta]
rm(removefromfasta)

tax_table<-read.csv("AssignTaxonomy/Silva138_assignedTaxonomy_toSpecies_temp1_28Apr2023.csv", header=T)
colnames(tax_table)[1]<-"seq"
ESVseq<-data.frame(seq=as.character(fasta), names=names(fasta))
ESVseq$ESV<-str_split(ESVseq$names,pattern = ";", 3, simplify = T)[,1]
ESVseq$names<-NULL

tax_table<-merge(tax_table,ESVseq, by="seq")
tax_table$seq<-NULL
rownames(tax_table)<-tax_table$ESV
tax_table$ESV<-NULL
#write.csv(tax_table,"AssignTaxonomy/FinalTaxFile/Silva138_assignedTaxonomy_toSpecies_2May2023.csv")
```
Put these into an "AssignTaxonomy" folder and then the final one in the "FinalTaxFile"

# Part 2: Data Cleaning

note: if going to rerun, make sure that there continues to have the object 'lessthan1' be 'integer (empty)' on lines 869, 1068 and 1206

## 1. remove unassigned, or incorrect OTUs

### a. read in taxa table

```{r}
tax_table<-read.csv("AssignTaxonomy/FinalTaxFile/Silva138_assignedTaxonomy_toSpecies_2May2023.csv", header=T,row.names=1)
table(tax_table$Kingdom) # showing NO Eukaryotes
unique(tax_table$Kingdom) # when the cell is empty there is an NA
```

### b. remove unassigned domain, chloroplast, and mitochondira

```{r}
#unassigned taxa at the kingdom/domain level (column 1)
table(is.na(tax_table[,1]))

#  FALSE   TRUE 
# 123794    111 
# with the old Silva v123 database 2427 were unassigned at the kingdom/domain level, now only 111 (great!)
df<-as.data.frame(tax_tab_silva)
unassigned_domain<-which(is.na(df$Kingdom))


df1<-df[-unassigned_domain,]

table(tax_table[,]=="Chloroplast")
#  FALSE   TRUE 
# 504979   1013
#all of these are Cyanobacteria!

table(tax_table$Kingdom=="Chloroplast")
table(tax_table$Phylum=="Chloroplast")
table(tax_table$Class=="Chloroplast")
table(tax_table$Order=="Chloroplast") # only here
table(tax_table$Family=="Chloroplast")
table(tax_table$Genus=="Chloroplast")
table(tax_table$Species=="Chloroplast")

table(tax_table[,]=="Mitochondria")
#  FALSE   TRUE 
# 504925   1067

table(tax_table$Kingdom=="Mitochondria")
table(tax_table$Phylum=="Mitochondria")
table(tax_table$Class=="Mitochondria")
table(tax_table$Order=="Mitochondria") 
table(tax_table$Family=="Mitochondria") #only here
table(tax_table$Genus=="Mitochondria")
table(tax_table$Species=="Mitochondria")

#Mitochondria is only found for 
# Bacteria
# Proteobacteria
# Alphaproteobacteria
# Rickettsiales
# Mitochondria
# but maybe they evolved together? not sure if I should throw out: https://doi.org/10.1016/S0014-5793(01)02618-7



#remove the rest of the objects
rm(df3)
rm(df2)
rm(df1)
rm(df)

rm(unassigned_domain)
```

### @count_track

```{r}
nrow(tax_tab_silva)
# [1] 119879 - Aug29
```

### c. remove : in taxomony table

```{r}

```

cleaned taxa table (from NAs, chloroplast and mitochondria, with the colons removed is now called taxa_tab)

## 2. read in OTU table

from folder on Teton, now Beartooth /project/microbiome/data/seq/psomagen_17sep20_novaseq2/otu/16S/Calder

```{r}
otu_table<-read.delim("OriginalFiles/otutable.esv", header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
#change the OTU name to be a row name
rownames(otu_table)<-otu_table$OTUID
otu_table$OTUID<-NULL

# #a double check that there are no NAs in the table - this takes awhile so checked once. 
# table(is.na(otu_table[,]))
#  FALSE 
# 161770302
```

### a. sed only and remove samples

1.  remove all water samples H24blank H3BL00_3 H59BL01_1 H6BL01_3 H64BL00_1 H66BL00_2 H67BL07_2 H7BL01_3 H70BL10_2 H72BL10_1 H73BL07_1 H74BL01_2 H9BL10_3 H29CL00_1 H31CL01_2 H34CL01_1 H35CL07_3 H36CL07_2 H43CL00_3 H46CL16_1 H48CL00_2 H51CL16_3 H55CL16_2 H61CL01_3 H65CL07_1 H11EG03_3 H12EG01_2 H13EG00_1 H14EG06_2 H15EG00_3 H16EG01_3 H17EG01_1 H18EG03_2 H19EG06_3 H20EG00_2 H21EG03_1 H22EG06_1 H5HL00_1 H10HW00_1 H4LW00_1 H68LL00_2 H38LO08_3 H39LO08_1 H40LO01_3 H44LO08_2 H45LO01_3 H50LO05_1 H52LO00_3 H53LO05_3 H54LO05_2 H56LO00_1 H57LO01_1 H69LO00_2 H2LS00_1 H58ML00_2 H1RL00_1 H25SV06_3 H28SV04_2 H30SV01_3 H32SV00_1 H33SV04_1 H41SV04_3 H47SV00_3 H49SV01_1 H60SV01_2 H62SV06_1 H63SV00_2 H71SV06_2 H26SR00_1 H23SG10_1 H27SG00_1 H37SG01_1 H42SG15_1
2.  remove samples that should not have been included RL0100L2 #should not have been sequenced RL0104L #should not have been sequenced SM0100L #private land, cannot include SM0104L #private land, cannot include SV0100L #should not have sequenced first core SV0104L #should not have sequenced first core TL0100L2 #biased productivity (near septic) TL0104L #biased productivity (near septic) LL0404L #out of order since core starts at 38 cm LW0100 \# duplicated sample, clearly wrong LWL0104L #sampled poorly 2017, should not have been sequenced LWL100L #sampled poorly 2017, should not have been sequenced

```{r}
require(stringr)
OTU_names<-unique(names(otu_table))

water<-c("H24blank",
"H3BL00_3",
"H59BL01_1",
"H6BL01_3",
"H64BL00_1",
"H66BL00_2",
"H67BL07_2",
"H7BL01_3",
"H70BL10_2",
"H72BL10_1",
"H73BL07_1",
"H74BL01_2",
"H9BL10_3",
"H29CL00_1",
"H31CL01_2",
"H34CL01_1",
"H35CL07_3",
"H36CL07_2",
"H43CL00_3",
"H46CL16_1",
"H48CL00_2",
"H51CL16_3",
"H55CL16_2",
"H61CL01_3",
"H65CL07_1",
"H11EG03_3",
"H12EG01_2",
"H13EG00_1",
"H14EG06_2",
"H15EG00_3",
"H16EG01_3",
"H17EG01_1",
"H18EG03_2",
"H19EG06_3",
"H20EG00_2",
"H21EG03_1",
"H22EG06_1",
"H5HL00_1",
"H10HW00_1",
"H4LW00_1",
"H68LL00_2",
"H38LO08_3",
"H39LO08_1",
"H40LO01_3",
"H44LO08_2",
"H45LO01_3",
"H50LO05_1",
"H52LO00_3",
"H53LO05_3",
"H54LO05_2",
"H56LO00_1",
"H57LO01_1",
"H69LO00_2",
"H2LS00_1",
"H58ML00_2",
"H1RL00_1",
"H25SV06_3",
"H28SV04_2",
"H30SV01_3",
"H32SV00_1",
"H33SV04_1",
"H41SV04_3",
"H47SV00_3",
"H49SV01_1",
"H60SV01_2",
"H62SV06_1",
"H63SV00_2",
"H71SV06_2",
"H26SR00_1",
"H23SG10_1",
"H27SG00_1",
"H37SG01_1",
"H42SG15_1")

remv<-c("LWL0104L",
"LWL100L",
"LW0100",
"LL0404L",
"RL0100L2",
"RL0104L",
"SV0100L",
"SV0104L",
"SM0100L",
"SM0104L",
"TL0100L2",
"TL0104L")

remove<-c(remv,water)
rm(remv)
rm(water)
positions_remove<-NULL
for(i in 1:length(remove)){
   positions_remove<-c(positions_remove, grep(remove[i], OTU_names))    
}

#check that only the samples you want are removed, (i.e. not LW0100L on accident)
remove_df<-otu_table[,positions_remove]
samp_names_rm<-str_split(names(remove_df),pattern = "_", 4, simplify = T)[,4]
keep<-grep(setdiff(samp_names_rm,remove), OTU_names)  #only one sample (LW0100L) that is in the removed list that shouldn't be!
positions_remove_new<-positions_remove[!positions_remove%in%keep]
table(positions_remove_new%in%keep)
table(positions_remove_new%in%positions_remove)


positions_remove<-positions_remove_new
rm(remove_df)
rm(samp_names_rm)
rm(keep)
rm(positions_remove_new)


#overwrite otu_table with columns (samples) removed
otu_table_backup<-otu_table #make backup
otu_table[,positions_remove]<-list(NULL)

#check this worked
OTU_names<-unique(names(otu_table))
positions_remove<-NULL
i=1
for(i in 1:length(remove)){
   positions_remove<-c(positions_remove, grep(remove[i], OTU_names))    
}
OTU_names[positions_remove]
#only two left are the LW0100L samples which means it worked

rm(OTU_names)
rm(remove)
rm(positions_remove)
rm(i)
rm(otu_table_backup) #delete backup

```

### @count_track

starting, with chloroplast, mitochondria, and NAs included no water, or samples that shouldn't have been sequenced 11 blanks included here - not the water blank (that was removed) removed OTUS with no reads updated 29 aug 2022

```{r}
#starting number of reads in the OTU table including blanks
sum(colSums(otu_table))
#[1] 45453337 - 29Aug

#avg number of reads per sample before filtering with blanks
sum(colSums(otu_table))/length(otu_table)
#[1] 40153.12  - 29Aug

#starting number of OTUs, first check if any OTUs are 0
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

greaterthan0<-which(reads_per_otu$reads>0)
otu_table_zero_rm<-otu_table[greaterthan0,]

#check this
reads_per_otu<-as.data.frame(rowSums(otu_table_zero_rm))

#overwrite otu table
otu_table<-otu_table_zero_rm

rm(otu_table_zero_rm)
rm(reads_per_otu)
rm(greaterthan0)

#number of OTUS
nrow(otu_table) 
#[1] 122444  - 29Aug

```

added this september 2nd to calculate: total number of reads (blanks removed), kept mitochondria, chloroplast and NAs also for reads per sample (555 samples without blanks)

!!!did not keep this going forward, skip this section if rerunning!!!!

```{r}
#initiate object to store column number containing blanks
blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)

#remove blanks from otu_table  & calculate reads
otu_table_no_blanks<-otu_table[, -blanks]
num_read<-as.data.frame(colSums(otu_table_no_blanks))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


#otu table with blanks removed
#total reads
sum(colSums(otu_table_no_blanks))
#45407617 - 2 Sept 2022
#average number of reads per sample
sum(colSums(otu_table_no_blanks))/(length(otu_table_no_blanks)/2)
#[1] 81815.53- 2 Sept 2022


reads_per_otu<-as.data.frame(rowSums(otu_table_no_blanks))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

greaterthan0<-which(reads_per_otu$reads>0)
otu_table_zero_rm<-otu_table_no_blanks[greaterthan0,]
nrow(otu_table_zero_rm)
#[1] 122443 - 2 Sept 2022



#BLANKS BEFORE REMOVING OTUS REMOVED FROM TAXONOMY



#subset blanks from otu_table & calculate reads
blank_data<-otu_table[, blanks]
blank_reads<-as.data.frame(colSums(blank_data))
names(blank_reads)<-"reads"
blank_reads$sample<-rownames(blank_reads)
rownames(blank_reads)<-NULL
summary(blank_reads$reads) 
   #  Min.  1st Qu.   Median     Mean  3rd Qu.     Max.  - 2 Sept
   # 26.00    36.75    48.50  2078.18   106.75 30828.00 


#reads in blanks
sum(colSums(blank_data))
#[1] 45720 - 2 Sept
#number of OTUs
tmp<-as.data.frame(rowSums(blank_data))
tmp<-tmp[tmp$`rowSums(blank_data)`>0,]
length(tmp)
#[1]  8950 - 2 Sept 
#22 samples

# how much is that one sample of total reads
(13537+ 30828)/45720 #2 Sept
rm(tmp)
rm(blanks)
rm(otu_table_no_blanks)
rm(blank_data)
rm(num_read)


```

### b. remove OTUs that were removed from the taxonomy table

```{r}
keep<-rownames(taxa_tab)
otus<-rownames(otu_table)
removefromotutab<-setdiff(otus,keep)
table(otus%in%keep)
table(row.names(otu_table) %in% removefromotutab)

otu_table2<-otu_table[!(row.names(otu_table) %in% removefromotutab),]
#overwrite OTU table
otu_table<-otu_table2

rm(keep)
rm(otus)
rm(removefromotutab)
rm(otu_table2)
```

### @count_track

otu table with blanks and chloroplast/mitochondria/domainNA removed as wel as water and bad samples, blanks kept updated aug 29, 2022

```{r}
#number of reads in the OTU table including blanks
sum(colSums(otu_table))
#[1] 43081685 - 29Aug
#number of OTUS
nrow(otu_table)
#[1] 118730 - 29Aug
#avg number of reads per sample
sum(colSums(otu_table))/length(otu_table)
#[1] 38058.03 - 29Aug

#number of samples with blanks included
ncol(otu_table)
#[1] 1132 - 29Aug

```

### c. remove blanks to assess reads

updated 29 aug 2022

```{r}
#initiate object to store column number containing blanks
blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)

#subset blanks from otu_table & calculate reads
blank_data<-otu_table[, blanks]
blank_reads<-as.data.frame(colSums(blank_data))
names(blank_reads)<-"reads"
blank_reads$sample<-rownames(blank_reads)
rownames(blank_reads)<-NULL
summary(blank_reads$reads)
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. - 29Aug
 #   0.0     2.0     4.0  1937.8    11.5 29455.0 

#remove blanks from otu_table  & calculate reads
otu_table_no_blanks<-otu_table[, -blanks]
num_read<-as.data.frame(colSums(otu_table_no_blanks))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


rm(blank_reads)



```

### @count_track (blanks and OTU with blanks removed before further filtering) - this is without chloroplast/NA/mitochondira

updated aug 29, 2022

```{r}
#otu table with blanks removed
#total reads
sum(colSums(otu_table_no_blanks))
# [1] 43039053 - 29Aug
#average number of reads per sample
sum(colSums(otu_table_no_blanks))/length(otu_table_no_blanks)
# [1] 38773.92 - 29Aug

#number of OTUs with otus wirh zero removed
tmp<-as.data.frame(rowSums(otu_table_no_blanks))
tmp<-tmp[tmp$`rowSums(otu_table_no_blanks)`>0,]
length(tmp)
#[1] 118730 - 29Aug
rm(tmp)
#number of (duplicated) samples
#1132-22 = 1110


#reads in blanks
sum(colSums(blank_data))
#[1] 42632 - 29Aug
#number of OTUs
tmp<-as.data.frame(rowSums(blank_data))
tmp<-tmp[tmp$`rowSums(blank_data)`>0,]
length(tmp)
#[1]  8787 - 29Aug
#22 samples
rm(tmp)
rm(blanks)
rm(otu_table_no_blanks)
rm(blank_data)
rm(num_read)

```

## 3. remove OTUs with low reads

at this point you should just have the two data frames in your environment: 1. taxa_tab 2. otu_table (taxonomy filtered, water and bad samples removed, blanks included)

### a. plot otus per read count

```{r}
#do on otu_table with blanks, with taxonomy filtering done on otu table
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

#make a table to see the counts of each otu
table_no.out_per.read<-as.data.frame(table(reads_per_otu$reads))
table_no.out_per.read$Var1<-as.numeric(as.character(table_no.out_per.read$Var1))
colnames(table_no.out_per.read)<-c("reads","no.otus")

#plot all to see that this is just happening within the first 100 reads
#fig02a
pdf("Figures/Number_otu_per_read_notpruned_29Aug2022.pdf")
plot(table_no.out_per.read$reads,table_no.out_per.read$no.otus,main="number of otus per read count", xlab="reads", ylab="number of otus")
dev.off()
#plot the subset of 0-20 reads
#fig02b
pdf("Figures/Number_otu_per_read_to100_notpruned_29Aug2022.pdf")
plot(table_no.out_per.read$reads[1:100],table_no.out_per.read$no.otus[1:100],main="number of otus per read count", xlab="reads", ylab="number of otus")
dev.off()


rm(table_no.out_per.read)
rm(reads_per_otu)
```

Decision: remove OTUs with less than 10 reads calculate number of OTUs and reads

### b. remove otus with low reads & @count_track

Chose 10 reads, citation: Urrutia-Cordero, P., Langenheder, S., Striebel, M., Eklöv, P., Angeler, D. G., Bertilsson, S., ... Hillebrand, H. (2021). Functionally reversible impacts of disturbances on lake food webs linked to spatial and seasonal dependencies. Ecology, 102(4). <https://doi.org/10.1002/ecy.3283>

```{r}
# output values for unpruned otu table with blanks
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan10<-which(reads_per_otu$reads<10)
otu_table_lessthan10removed<-otu_table[lessthan10,]
sum(colSums(otu_table_lessthan10removed))
nrow(otu_table_lessthan10removed)
# [1] 134028 - 29Aug #reads removed
# [1] 18085 - 29Aug #otus removed

# @count_track
otu_table_10<-otu_table[-lessthan10,]
sum(colSums(otu_table_10))
nrow(otu_table_10)
# [1] 42947657 - 29Aug #reads retained
# [1] 100645 - 29Aug #otus retained

otu_table<-otu_table_10


rm(lessthan10)
rm(otu_table_lessthan10removed)
rm(reads_per_otu)
rm(otu_table_10)

```

### c. plot reads by OTUs for each sample

```{r}
#for filtered otu table - not pruned (otu_table)
#calculate number of reads per sample
reads<-as.data.frame(colSums(otu_table))
names(reads)<-"reads"

#calculate number of otus per sample
samples<-names(otu_table)
table(rownames(reads)==samples) # make sure this is TRUE

OTUs<-NULL
for(i in 1:ncol(otu_table)){
        tmp<-as.data.frame(otu_table[,i])
        tmp2<-tmp[tmp>0,]
        OTUs<-c(OTUs, length(tmp2))
}
rm(i)
rm(tmp)
rm(tmp2)
rm(samples)


tmp3<-as.data.frame(cbind(reads$reads,OTUs))
names(tmp3)<-c("reads3","otus3")
tmp3$sample<-rownames(reads)

#fig3b
pdf("Figures/Reads_otus_bysamp_notpruned_10plusreadsperOTU_29Aug2021.pdf")
plot(OTUs,reads$reads, main="number of reads and otus for each sample", ylab="reads")
dev.off()


rm(OTUs)
rm(reads)
rm(tmp3)

```

Note - this suggest that I should normalizing (verses pruning at something like 500 or more) since the number of OTUs increases with then number of reads.

## 4. remove samples with low reads

### a. plot number of reads per sample

updated 22 Aug 2022

```{r}
### with blanks and otus with less than 10 reads removed and taxonomy filtered####
num_read<-as.data.frame(colSums(otu_table))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL

#fig01a
par(mar=c(2,4,2,2))
pdf("Figures/fig01a_boxplot_reads_per_sample_29Aug2022.pdf")
boxplot(num_read$reads, main="total reads per sample before pruning")
dev.off()

summary(num_read$reads)
  # Min. 1st Qu.  Median    Mean 3rd Qu.    Max. - 29Aug
  #     0    9752   31103   37940   56291  178805 
#fig01b
par(mar=c(2,4,2,2))
pdf("Figures/fig01b_histogram_reads_per_sample_29Aug2022.pdf")
hist(num_read$reads, main="total reads per sample before pruning", xlab="number of reads", breaks=150)
dev.off()

#fig01c
par(mar=c(2,4,2,2))
pdf("Figures/fig01c_histogram_low_reads_per_sample_29Aug2022.pdf")
hist(num_read[num_read$reads<25000,]$reads, xlab="number of reads", main="total reads per sample before pruning", breaks=150)
dev.off()

#fig01d
par(mar=c(2,4,2,2))
pdf("Figures/fig01d_histogram_lower_reads_per_sample_29Aug2022.pdf")
hist(num_read[num_read$reads<10000,]$reads, xlab="number of reads", main="total reads per sample before pruning", breaks=150)
dev.off()

#fig01e
par(mar=c(2,4,2,2))
pdf("Figures/fig01e_histogram_lowest_reads_per_sample_29Aug2022.pdf")
hist(num_read[num_read$reads<5000,]$reads, xlab="number of reads", main="total reads per sample before pruning", breaks=150)
dev.off()

#fig01f
pdf("Figures/fig01f_hist_samples_per_readslessthan1000_29Aug2022.pdf")
hist(as.data.frame((num_read[num_read$reads<1000,]$reads))[,1],breaks = 100,xlab="number of reads", ylab="freq of samples", main="number of samples by read number (<1000)")
dev.off()

#fig01g
pdf("Figures/fig01g_hist_samples_per_readslessthan1000_29Aug2022.pdf")
hist(as.data.frame((num_read[num_read$reads<1000,]$reads))[,1],breaks = 50,xlab="number of reads", ylab="freq of samples", main="number of samples by read number (<1000)")
dev.off()


```

numbers of samples lost with pruning reran aug 22, 2022

```{r}

#this is with the blanks included, OTUs with less than 10 reads removed, and taxonomy filtering
#200 samples with less than 1000 reads... looking to see where I should cut as the "low number of reads"
#this is calculated with blanks removed already
length(num_read[num_read$reads<1000,]$reads)
length(num_read[num_read$reads<500,]$reads)
length(num_read[num_read$reads<200,]$reads)
length(num_read[num_read$reads<150,]$reads)
length(num_read[num_read$reads<100,]$reads)
#removing samples with <100 reads, we remove 97 samples

# [1] 152
# [1] 136
# [1] 117
# [1] 107
# [1] 97

rm(num_read)
```

### b. prune samples \@ 100

choice is to remove samples with less than 100 reads

```{r}
# Rerun the number of reads per sample using the OTU table with blanks included (otu_table)
num_read<-as.data.frame(colSums(otu_table))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


# prune at 100 reads
prune<-which(num_read$reads<100)
# 97 duplicated samples will be removed

otu_table_pruned<-otu_table[,-prune]
ncol(otu_table_pruned)
# 1035 - 29Aug duplicated samples remaining

#check those columns were removed! 
remove<-num_read[prune,] # all of these are less than 100
sum(remove$reads)
#2418 -29Aug reads removed with pruning
num_read_pruned<-as.data.frame(colSums(otu_table_pruned)) # all over 100 
sum(num_read_pruned$`colSums(otu_table_pruned)`)
#42945239 -29Aug reads after pruning (see below for count track, these match)

#remove OTUs that are now zero 
reads_per_otu<-as.data.frame(rowSums(otu_table_pruned))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan1<-which(reads_per_otu$reads<1)
#none less than one

#rewrite otu table
otu_table<-otu_table_pruned

rm(lessthan1)
rm(otu_table_pruned)
rm(reads_per_otu)
rm(remove)
rm(num_read_pruned)
rm(prune)
rm(num_read)
```

### @count_track

updated 29 aug 2022

```{r}
#reads in pruned otu table
sum(colSums(otu_table))
#number of otus
nrow(otu_table)
#number of duplicated samples
ncol(otu_table)
# [1] 42945239 - 29Aug
# [1] 100645 - 29Aug
# [1] 1035 - 29Aug

```

## 5. correlate technical replicates

### a. correlate technical replicates

this chunk of code you put in the otu table you want to try with and then rerun for each variation.

```{r}
otu_tab_derep <- otu_table


#Pull out the barcode names from the current sample names containing locus ID, forward MID, reverse MID, and Library number.
#split by "_"" 4 times, save as character matrix and extract the 4th slot.
samp_names<-str_split(names(otu_tab_derep),pattern = "_", 4, simplify = T)[,4]
length(unique(samp_names))
#599 unique samples


#I also could go look for the specific tag... could ask kevin if this is TAG jumping?? 
names(otu_tab_derep)<-samp_names

# 1. counts with zeros ####
cor_df<-list()
for(i in 1:length(samp_names)){
        for(j in 1:length(samp_names)){
                if(samp_names[i] == samp_names[j] & i!=j & i>j){
                        tmp<-cbind(otu_tab_derep[i], otu_tab_derep[j])
                        cor_stat <-data.frame(cor(tmp[1], tmp[2])) 
                        cor_df<-c(cor_df, cor_stat)
                }
        }
}

#results: 
#unlist to a dataframe containing sample ID and the correlation among technical replicates.
cor_df <- data.frame(SampleID = rep(names(cor_df), sapply(cor_df, length)), correlation = unlist(cor_df))
#Sort correlations to look at lowest ones.
cor_df %>% arrange(correlation,SampleID)

#502 unique samples (not duplicated)

# 70% cutoff        
low_cor_samp<-cor_df[cor_df$correlation<0.7,]
# 18 samples


rm(tmp)
rm(cor_df)
rm(cor_stat)
#rm(low_cor_samp)
rm(i)
rm(j)
rm(otu_tab_derep)
```

### b. look at samples individually

I did this once, and the results are in an excel file, from there, I selected the samples that needed to be removed after summing technical replicates!

for reads correlated less than 70%, kept samples whose smaller reads were less than 25% of the greater reads updated 29 aug 2022

```{r}
tmp <- otu_table

samp_names2<-str_split(names(tmp),pattern = "_", 4, simplify = T)[,4]

reads<-as.data.frame(colSums(tmp))
names(reads)<-"reads"
reads$sample<-rownames(reads)
rownames(reads)<-NULL
reads$samp_name<-str_split(reads$sample,pattern = "_", 4, simplify = T)[,4]

xremoved<-str_remove(low_cor_samp$SampleID, "[X]")


select<-which(reads$samp_name%in%xremoved)
cor70<-reads[select,]


rm(reads)
rm(tmp)
rm(cor70)
rm(select)
rm(xremoved)
rm(samp_names2)
rm(low_cor_samp)
rm(samp_names)
```

sample1 sample2 correlation reads1 reads2 percent 33_1\_0_DNA 33_1\_0_DNA 0.41812768 324 1322 25% 35_2\_6_DNA 35_2\_6_DNA 0.66788087 447 1040 43% 37_1\_2_DNA 37_1\_2_DNA 0.52398164 104 521 20% 39_1\_18_DNA 39_1\_18_DNA 0.31364043 164 652 25% 44_1\_12_DNA 44_1\_12_DNA 0.64393483 161 132952 0% 46_1\_2_DNA 46_1\_2_DNA 0.69341167 155 168144 0% BL0126L BL0126L 0.52864848 348 3941 9% BL0132L BL0132L 0.61811876 150 16985 1% FB0104L FB0104L 0.34169897 204 283 72% LB0212L LB0212L 0.54820268 114 545 21% LL0104L LL0104L 0.11559503 129 160 81% LL0112L LL0112L 0.53879839 114 287 40% LS0120L LS0120L 0.06579572 49014 53193 92% SR0102L SR0102L 0.58216221 420 676 62% SR0104L SR0104L 0.36381059 159 1577 10% SR0114L SR0114L 0.66456196 239 23419 1% SV0408L SV0408L 0.5970406 229 235 97%

### c. sum technical replicates

Only the ones that have good correlation, taxonomy filtered, otus with less than 10 reads removed, samples with less than 100 reads removed

```{r}

otu_tab_derep <- otu_table

samp_names<-str_split(names(otu_tab_derep),pattern = "_", 4, simplify = T)[,4]

#use the sample names to compare which ones were removed 
length(unique(samp_names))
# 532 - 29Aug

#just to check that the new sample names match up, I created a new dataframe. I check this three lines below. 
otu_table2<-otu_tab_derep

#rename columns in otu_table2 with the extracted info in samp_names
names(otu_table2)<-samp_names
table(names(otu_table2)==str_split(names(otu_tab_derep),pattern = "_", 4, simplify = T)[,4])
length(unique(names(otu_table2)))
otu_table3 <- rowsum(t(otu_table2), group = colnames(otu_table2), na.rm = T)

otu3_df<-as.data.frame(t(otu_table3))
otu3_df<-otu3_df[,order(names(otu3_df))]
otu2_df<-otu_table2[,order(names(otu_table2))]

#check to make sure they are added right!
otu2_df[1:10,1:6]
otu3_df[1:10,1:5]

summed_otu_table<-otu3_df

#remove extra dataframes
rm(otu_table2)
rm(otu_table3)
rm(otu2_df)
rm(otu3_df)
rm(samp_names)

#remove uncorrelated samples!!!!
remove<-c("35_2_6_DNA",
"39_1_18_DNA",
"FB0104L",
"LL0104L",
"LL0112L",
"LS0120L",
"SR0102L",
"SV0408L")


where<-which(names(summed_otu_table)%in% remove) 
table(names(summed_otu_table)[where] %in% remove)
summed_otu_table_goodcorronly<-summed_otu_table[,-where]
table(names(summed_otu_table_goodcorronly)%in% remove)
#rewrite otu_table
otu_table<-summed_otu_table_goodcorronly

#remove OTUs with zeros now
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan1<-which(reads_per_otu$reads<1)
#no OTUs less than 1


rm(lessthan1)
rm(reads_per_otu)
rm(where)
rm(summed_otu_table_goodcorronly)
rm(remove)
rm(otu_tab_derep)
rm(summed_otu_table)

```

## 6. remove blanks

In this, since most of the blanks were removed, there would only be one blank used to do decontamination (if we rarefied at around 10k again)

```{r}
blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)



#subset blanks from summed_otu_table & calculate reads
blank_data<-as.data.frame(otu_table[, blanks])
rownames(blank_data)<-rownames(otu_table)
blank_reads<-as.data.frame(colSums(blank_data))

	
colSums(blank_data)
 # JC_Blank2 JC7_Blank2 - 29Aug
 #     42082        132 



#overwrite otu table and check blanks removed
otu_table<-otu_table[,-blanks]

blanks<- c()
for (i in 1:ncol(otu_table)){
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)


rm(blanks)
rm(blank_data)
rm(blank_reads)

```

### @count_track

removed blank reads this is: before normalization 1. mictochondria/chloroplast/NA removed 2. OTUs with less than 10 reads removed 3. samples with less than 100 reads removed 4. technical replicates correlated and summed 5. blanks removed updated 29 aug 2022

```{r}
sum(colSums(otu_table))
nrow(otu_table)
ncol(otu_table)

# [1] 42795778 - 29Aug
# [1] 100645  - 29Aug
# [1] 522  - 29Aug


```

## 7. Write OTU and taxa CSVs before normalization

updated 29 aug 2022

```{r}
# write.csv(otu_table,file="otu_table_notnorm_29Aug2022.csv",row.names = T)
# write.csv(taxa_tab,file="tax_tab_notnorm_29Aug2022.csv",row.names = T)
# 
# save.image("WyLakeMicrobes_otu_tax_notnorm_env_29Aug2022.RData")
```

## 8. normalize

using the rrarefy() function: Function rrarefy generates one randomly rarefied community data frame or vector of given sample size. The sample can be a vector giving the sample sizes for each row. If the sample size is equal to or smaller than the observed number of individuals, the non-rarefied community will be returned. The random rarefaction is made without replacement so that the variance of rarefied communities is rather related to rarefaction proportion than to the size of the sample. updated 29 aug 2022

```{r}
require(vegan)

#performing this on pruned at 100, otus with at least 10 reads (from FIRST and only cut in step 3), taxonomy cleaned

otu_to_normalize<-otu_table
num_read<-as.data.frame(colSums(otu_to_normalize))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL

belowthreshold<-which(num_read$reads<10000)
length(belowthreshold)
#44 samples below 10,000 reads

#look at how many samples removed if the threshold is 5000
belowthreshold5000<-which(num_read$reads<5000)
length(belowthreshold5000)
# this is 40, so almost the same, so will go to 10K
rm(belowthreshold5000)

otu_to_normalize_10k<-otu_to_normalize[,-belowthreshold]
ncol(otu_to_normalize_10k)
# 478 - 29Aug samples remaining

#check those columns were removed! 
remove<-num_read[belowthreshold,] # all of these are between 100 and 10,000
num_read_otu_to_normalize_10000<-as.data.frame(colSums(otu_to_normalize_10k)) # all over 10,000 reads

rm(remove)
rm(num_read_otu_to_normalize_10000)
rm(belowthreshold)
rm(num_read)

#remove OTUs that are now zero (only one)
reads_per_otu<-as.data.frame(rowSums(otu_to_normalize_10k))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan1<-which(reads_per_otu$reads<1)
#no OTUs with reads less than 1
rm(lessthan1)
rm(reads_per_otu)





#rarefy!
S <- specnumber(otu_to_normalize_10k) 
length(S)
#[1] 100645 - 29Aug # observed number of species
(raremax <- min(rowSums(t(otu_to_normalize_10k)))) #minimum reads to normalize to (10285 - 29Aug)
set.seed(061319)
rrare<-rrarefy(t(otu_to_normalize_10k), sample = raremax)
df<-as.data.frame(t(rrare))


pdf("Figures/Rarefied_data_10k_29Aug2022.pdf")
plot(S, specnumber(t(rrare)), xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
dev.off()


num_read<-as.data.frame(colSums(df))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL

# count the number of species after rarefaction
otu_counts<-as.data.frame(rowSums(df))
names(otu_counts)<-"reads"
otu_counts$OTU<-rownames(otu_counts)
rownames(otu_counts)<-NULL
zeroOTU<-which(otu_counts$reads<1)
length(zeroOTU)
# [1] 8688 - 29Aug OTUS that are zero after rarefying.

otu_tab_10knorm<-df[-zeroOTU,]


#check you removed the right OTUS! 

check1<-as.data.frame(rowSums(otu_tab_10knorm))

rm(check1)
rm(df)
rm(num_read)
rm(otu_counts)
rm(otu_to_normalize)
rm(raremax)
rm(S)
rm(zeroOTU)
rm(rrare)

#keep the non-normalized OTU table that is cut off at 10k just in case
# write.csv(otu_to_normalize_10k, "otu_to_normalize_10k_29Aug2022.csv", row.names=T)
# write.csv(otu_tab_10knorm, "otu_tab_10knorm_29Aug2022.csv", row.names=T)
# save.image("WyLakeMicrobes_otu_tax_10knorm_env_29Aug2022.RData")
```

### \@ count_track rarefied_final

```{r}
(sum(colSums(otu_tab_10knorm)))
nrow(otu_tab_10knorm)
ncol(otu_tab_10knorm)
# [1] 4916230 - 29Aug reads
# [1] 91957 - 29Aug OTUs
# [1] 478 - 29Aug samples

#percent of samples removed
1-(478/555)

#number of ESVs retained
91957/122444
#[1] 0.7510127
```

### a. ordinate and procrustes

reran 29 aug 2022

```{r}
require(vegan)
#compared samples greater than 10,000 reads (otu_normalize_10000) and the rarefied or normalized object (final_normalized)

#calculate vegdist
notnorm.dist <- vegdist(wisconsin(t(otu_to_normalize_10k)))
norm.dist <- vegdist(wisconsin(t(otu_tab_10knorm)))

mds.notnorm <- monoMDS(notnorm.dist)
mds.norm <- monoMDS(norm.dist)

procrust <- procrustes(mds.notnorm, mds.norm)
procrust


summary(procrust)

pdf("Figures/Procrustes_error_29Aug2022.pdf",height=10, width=20)
plot(procrust)
dev.off()


plot(procrust, kind=2)
residuals(procrust)
summary(procrust)
eigenvals(procrust)
plot(procrust, kind=2)
residuals(procrust)


rm(notnorm.dist)
rm(norm.dist)
rm(mds.notnorm)
rm(mds.norm)
rm(procrust)
rm(otu_to_normalize_10k)
rm(otu_table)

```

# Part III: Create Phyloseq

## 1. Update and save the metadata file

```{r}
#update the EG sample name
metadata<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/Wy Lake Microbes_old/Metadata_30July2022.csv",header=T,row.names=1)
rownames(metadata)[which(rownames(metadata)=="EG0318")]<-"EG0318L"
metadata[metadata$cleanedID=="EG0318",]$cleanedID<-"EG0318L"
metadata[metadata$samp_names=="EG0318",]$samp_names<-"EG0318L"


metadata_old<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/metadata/current/metadata_31Jan2022.csv",header=T, row.names = 1)
metadata_old$samp_names<-rownames(metadata_old)
metadata_old$water_sample_do_avg<-NULL
metadata_old$water_sample_ph_avg<-NULL
metadata_old$water_sample_t_avg<-NULL
metadata_old$bin_depth<-NULL
metadata$bin_depth<-NULL
table(names(metadata_old)==names(metadata))


add<-metadata_old[rownames(metadata_old)%in%c("BN0105L", "ML0104L"),]
metadata<-rbind(metadata,add)
rm(add)
rm(metadata_old)

#do some old checks from old RMD file
metadata[metadata$lake_id=="SR",]$max_lake_depth # should be 4.3
# all good, cut in half
metadata[metadata$lake_id=="BN" & metadata$sample_type=="sediment",]$depth
metadata[metadata$lake_id=="NB" & metadata$sample_type=="sediment",]$depth
metadata[metadata$lake_id=="LS" & metadata$sample_type=="sediment",]$depth

# add in depth bins
sort(unique(metadata$depth))
metadata$bin_depth<-metadata$depth
metadata[metadata$depth==1,]$bin_depth<-2
metadata[metadata$depth==2.5,]$bin_depth<-2
metadata[metadata$depth==3,]$bin_depth<-4
metadata[metadata$depth==5,]$bin_depth<-6
metadata[metadata$depth==7,]$bin_depth<-8
metadata[metadata$depth==9,]$bin_depth<-10
metadata[metadata$depth==11,]$bin_depth<-12
metadata[metadata$depth==13,]$bin_depth<-14
metadata[metadata$depth==15,]$bin_depth<-16
metadata[metadata$depth>26,]$bin_depth<-100
sort(unique(metadata$bin_depth))

#write.csv(metadata,"metadata_29Aug2022.csv")
```

## 2. Make phyloseq

```{r}
load("WyLakeMicrobes_otu_tax_10knorm_env_29Aug2022.RData")
metadata<-read.csv("metadata_29Aug2022.csv",header=T,row.names = 1)

#change East Glacier mislabeled sample from EG0308L2 to EG0318
names(otu_tab_10knorm)[which(names(otu_tab_10knorm)=="EG0308L2")]<-"EG0318L"
names(otu_table)[which(names(otu_table)=="EG0308L2")]<-"EG0318L"
names(otu_to_normalize_10k)[which(names(otu_to_normalize_10k)=="EG0308L2")]<-"EG0318L"

#check metadata and otu names match 
table(rownames(metadata)%in%names(otu_tab_10knorm)) # all true

#check only sediment
table(metadata$sample_type)

#put in "Unassigned" in blanks in taxa tab
tax_tab<-as.data.frame(taxa_tab)
tax_tab[tax_tab==""]<-"Unassigned"
tax_tab <-as.matrix(tax_tab)
#covert taxa table to phyloseq sub-object
taxa_tab <- tax_table(tax_tab)

#make sure removed
table(taxa_tab@.Data=="") # all false

#delete non-normalized otu tables
rm(otu_table)
rm(otu_to_normalize_10k)

#convert metadata and otu table to phyloseq sub-objects
samp_dat <- sample_data(metadata)
otu_tab_norm <- otu_table(otu_tab_10knorm, taxa_are_rows = T)
ps <- phyloseq(otu_tab_norm, samp_dat, taxa_tab)

#remove phyloseq sub-objects
rm(samp_dat)
rm(otu_tab_norm)
rm(taxa_tab)

#transform sample data
ps_tr <- transform_sample_counts(ps, function(x) x / sum(x))
#save final phyloseq object
#save.image("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
```

# Part III: Data Analysis

## 0. password

```{bash}
PrayingMantus@0613@
```

## 1. Load data

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
```

## 2. Stats for manuscript

Updated Sept 2, 2022

```{r}
# what is the total number of cores and lakes?
length(unique(metadata$lake_id)) # 36 lakes
length(unique(metadata$lake_drive)) # 48 cores 

#average and range of sediment depth for cores? 
lake_drives<-unique(metadata$lake_drive)
maxd<-NULL
for(i in 1:length(lake_drives)){
        tmp<-metadata[metadata$lake_drive==lake_drives[i],]
       maxd <-c(maxd,max(tmp$depth))
}
mean(maxd)
#21.83333
max(maxd)
#88
min(maxd)   
#2

min(metadata$max_lake_depth)
max(metadata$max_lake_depth)
#0.5-19m

#how many lakes greater than or equal to 26 cm deep? 
table(maxd>=26)
22/(26+22)
#0.4583333 of the lakes were at or greater than 26 cm 

# how many samples are less than or equal to 26 cm
nrow(metadata[metadata$depth<=26,])
# 429 samples

# how many samples greater than 26 cm
478-429
#49 - 6 sept 2022

meta_sed_26<-metadata[metadata$depth<=26,]

# JVE notes - I need to say how many samples are measured in ALL samples, not just 26 cm 

# #how many samples <=26 have elemental measurements?
# sub<-meta_sed_26[is.na(meta_sed_26$d_13_c)==FALSE,]
# nrow(sub)
# #121 samples
# 
# #how many cores and samples have C:N data
# length(unique(ps@sam_data[is.na(ps@sam_data$cn)==FALSE,]$lake_id))
# # 28
# #samples less than 26 cm that have CN data
# nrow(meta_sed_26[is.na(meta_sed_26$cn)==FALSE,])
# # 262 samples have C:N, C%, N%
# 
# #how many cores have d13C data
# length(unique(ps@sam_data[is.na(ps@sam_data$d_13_c)==FALSE,]$lake_id))
# # 13
# #how many samples <=26 have elemental measurements?
# nrow(meta_sed_26[is.na(meta_sed_26$d_13_c)==FALSE,])
# #121 samples

# so INSTEAD - did this:
# all the true
table(is.na(metadata$pH)==F)
#278

table(is.na(metadata$cn)==F)
#282

table(is.na(metadata$d_13_c)==F)
#130



#where are those from?
unique(ps@sam_data[is.na(ps@sam_data$d_13_c)==FALSE,]$mountain_range)
#just the Snowies

#Values for elemental measurements for results section



print("surface")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$n_perc, na.rm=T),digits=2)

print("replacement")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$n_perc, na.rm=T),digits=2)

print("depauperate")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$n_perc, na.rm=T),digits=2)

print("replacement and depauperate")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$c_perc, na.rm=T),digits=3)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$n_perc, na.rm=T),digits=2)

# Sept 6
# [1] "surface"
# [1] 13.09
# [1] 1.34
# [1] "replacement"
# [1] 11.46
# [1] 1.02
# [1] "depauperate"
# [1] 11.34
# [1] 0.95
# [1] "replacement and depauperate"
# [1] 11.40
# [1] 0.98


# a positive shift in carbon isotopic composition (δ13C) of occurred from depths 0-4 cm
sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(0),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -30.0454 - 6Sept
sd(sub$d_13_c)
#[1] 2.124595 - 6 sept


sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(4),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
sd(sub$d_13_c)
#[1] 4.420052 - 6 sept
(-30.0454) - (-25.2764)
#-4.769 (round to 4.77%) - 6 sept

sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -24.65961 - 6 sept
sd(sub$d_13_c)
#[1] 4.301652  - 6 sept

sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(24,26),]
nrow(sub)
#10 samples at depths 24 and 26 with isotopic data
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -26.081  - 6 sept
sd(sub$d_13_c)
#[1] 2.139629  - 6 sept


# range of elevations
range(metadata$elevation_meters,na.rm=T) #NAs are the blanks that don't have elevations
#2032 3350
#subset for 26 cm cutoff elevation
range(meta_sed_26$elevation_meters,na.rm=T) 
#2032 3350 (same)

#range of temperatures

#these are the same for the 26 cm and less samples and the entire dataset
range(metadata$water_sample_t_bot, na.rm=T)
#3.8 20.1
#these are the same for the 26 cm and less samples and the entire dataset
range(metadata$water_sample_t_surf, na.rm=T)
#11.3 20.7
range(metadata$water_sample_ph_bot, na.rm=T)
#5.39 9.68
range(metadata$water_sample_ph_surf, na.rm=T)
#6.83 9.63
range(metadata$water_sample_do_bot, na.rm=T)
#-0.06 12.14
range(metadata$water_sample_do_surf, na.rm=T)
#3.14 8.23

# Started with 555 samples
# ended with 478 samples

# How many lakes cored in 2018 and 2017
sub2017<-metadata[metadata$year_sample=="2017",]
length(unique(sub2017$lake_name))
sub2018<-metadata[metadata$year_sample=="2018",]
length(unique(sub2018$lake_name))

#how many cores from the Snowies
sub<-metadata[metadata$mountain_range=="Snowy",]
length(unique(sub$lake_name))

#looking for how many lakes were cored more than once (eight)
sort(unique(sub$lake_drive))

#bighorn
sub<-metadata[metadata$mountain_range=="Bighorn",]
length(unique(sub$lake_name))
sub<-metadata[metadata$mountain_range=="Beartooth",]
length(unique(sub$lake_name))
sub<-metadata[metadata$mountain_range=="Wind River",]
length(unique(sub$lake_name))


#create supplementary table 1

ST1<-metadata[,names(metadata)%in%c("lake_id"                
, "drive"                  
,"lake_name"              
,"lake_number"            
, "year_sample"            
, "mountain_range"         
, "latitude"               
, "longitude"              
, "elevation_meters"       
, "max_lake_depth"
,"water_sample_depth_surf"
, "water_sample_ph_surf"   
,"water_sample_do_surf"   
, "water_sample_t_surf"    
, "water_sample_depth_bot" 
,"water_sample_ph_bot"    
, "water_sample_do_bot"    
,"water_sample_t_bot")]

ST1<-unique(ST1)

write.csv(ST1,"SupplementaryTable1.csv")


#what are the top 3 ESVs?

#what percent of reads are archaea?
arch<-subset_taxa(ps, Kingdom=="Archaea")
sum(rowSums(arch@otu_table))/sum(rowSums(ps@otu_table))
#[1] 0.172937

# additional generalizations between shallow, warm and cool, deep lakes

#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata

require(GGally)

metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

cor(metadata_sub)

which(is.na(metadata_sub[3]==T))
metadata_sub[which(is.na(metadata_sub[3]==T)),]

ggpairs(metadata_sub)
```

## 3. Null models (BNTI and RCBray)

### a. Phylogenetic tree

create Phylogenetic tree on Teton

```{r}
library(phylotools)
library(readr)

uniqueOTUs_split<- phylotools::read.fasta("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/original files off Teton/uniqueOTUs_16S.fa")
uniqueOTUs_split$OTU <- paste(rep(">centroid=",nrow(uniqueOTUs_split)),str_split(uniqueOTUs_split$seq.name, pattern="_",2,simplify=T)[,2],sep="")
uniqueOTUs_split$OTU <- str_split(uniqueOTUs_split$OTU, pattern=";",3,simplify=T)[,1]

centroids <- rownames(as.data.frame(ps@otu_table@.Data))
centroids <- paste(rep(">",length(centroids)),centroids,sep="")
sed_norm_subset<-uniqueOTUs_split[uniqueOTUs_split$OTU%in%centroids,]
sed_norm_subset$seq.name<-NULL
names(sed_norm_subset)<-c("seq","OTU")
sed_norm_subset$even <- seq(2,by=2, len=nrow(sed_norm_subset))
sed_norm_subset$odd <- seq(1,by=2, len=nrow(sed_norm_subset))

table(sort(centroids,decreasing=T)==sort(sed_norm_subset$OTU,decreasing=T))

odd<-sed_norm_subset[,c(2,4)]
names(odd)<-c("item","number")
even<-sed_norm_subset[,c(1,3)]
names(even)<-c("item","number")
all<-rbind(odd,even)
all<-all[order(all$number),]
all$number<-NULL
write_lines(all$item,file = "sed_norm_fasta_29Aug2022.txt")
```

on teton copy the file over

```{bash}
rsync /Users/jordanscheibe/Desktop/sed_norm_fasta_29Aug2022.txt jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree

#rename as a fasta file
mv sed_norm_fasta_29Aug2022.txt sed_norm_fasta_29Aug2022.fa
```

create bash file for muscle and fastTree clustalo_fasttree.sh

```{bash}
#!/bin/bash
#SBATCH --job-name clustalo_fasttree
#SBATCH --mem=512GB
#SBATCH --time=20:00:00
#SBATCH --account=microbiome
#SBATCH --output=clustalo_fasttree_%A.out
hostname; date
module load miniconda3/4.3.30 gcc/7.3.0
source activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree

echo "clustalo -i sed_norm_fasta_29Aug2022.fa -o sed_norm_fasta_29Aug2022_muscled.fasta -v"
clustalo -i sed_norm_fasta_29Aug2022.fa -o sed_norm_fasta_29Aug2022_muscled.fasta -v
echo "finish muscle"

echo "FastTree -nt sed_norm_fasta_29Aug2022_muscled.fasta > sed_norm_fasta_29Aug2022_muscled.nwk"
FastTree -nt sed_norm_fasta_29Aug2022_muscled.fasta > sed_norm_fasta_29Aug2022_muscled.nwk
echo "finished FastTree"
source deactivate shotgun_env
date
```

copy to computer

```{bash}
rsync jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/sed_norm_fasta_29Aug2022_muscled.nwk /Users/jordanscheibe/Desktop/
```

### b. BNTI on Teton

### i. subset by the top 10K ESVs

ps and ps_tr (transformed) are very similar, so I use the ESVs from ps_tr since thats likely more accurate, and then subset from the ps (untransformed table)

```{r}
require(ape)
#how many samples greater than 26 cm
nrow(ps_tr@sam_data[ps_tr@sam_data$bin_depth==100,])
# 49 samples - 30 aug

all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)
#3500 = 10888 OTUs - 30Aug

#compare with the top 5,000 and 10,000 most abundant centroids across all samples
means <- as.data.frame(rowMeans(ps_tr@otu_table))
names(means)<-"val"
means$otu<-rownames(means)
avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:5000] 
table(avgabundtax %in% unique_alldepths)
# TRUE 
# 5000 
table(unique_alldepths %in% avgabundtax)
# FALSE  TRUE 
#  5888  5000 - 30Aug

avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:10000]
table(avgabundtax %in% unique_alldepths)
# FALSE  TRUE 
#  1065  8935 - 30Aug
table(unique_alldepths %in% avgabundtax)
# FALSE  TRUE 
#  1953  8935 - 30Aug

#what percent of the entire dataset are these 10,888 ESVs? 
tmp<-as.data.frame(otu_table(ps_tr))
tmp2<-tmp[rownames(tmp)%in%unique_alldepths,]
sum(tmp2)/sum(tmp)
#  0.8720786 - 30Aug2022 10888 ESVs included here make up 87.21% of the 91957 total ESVs
rm(tmp2)
rm(tmp)



my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
tree<-read.tree("sed_norm_fasta_29Aug2022_muscled.nwk")
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), tree)
table(sort(physeq@phy_tree[["tip.label"]])==sort(unique_alldepths)) #all true
tree_dist <- cophenetic(physeq@phy_tree)

rm(my_subset)
rm(means)
rm(ps)
rm(ps_tr)
rm(tree)
rm(unique_alldepths)
rm(ps_tmp)
rm(cm)
rm(i)
rm(all_depths)
rm(avgabundtax)
rm(metadata)
rm(otu_tab_10knorm)
rm(tax_tab)

#save.image("WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData")
```

copy to Teton server

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI


BNTI_32cores_abundwt_10kESVs.R
require(phyloseq)
require(MicEco)
load("WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData")
BNTI<-ses.comdistnt2(t(physeq@otu_table),tree_dist, cores=32, abundance.weighted = TRUE)
save.image("WyLakeMicrobes_env_30Aug2022_finalBNTI_abundwt_10888ESVs.RData")


runR_32_abundwt_10kESVs.sh
#!/bin/bash
#SBATCH --job-name BNTI_BDC_abundwt_10k
#SBATCH --mem=120GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=32
#SBATCH --account=microbiome
#SBATCH --output=BNTI_BDC_abundwt_10kESVs%A.out
hostname; date
module load miniconda3/4.3.30 gcc/7.3.0 r/4.0.5-py27
source activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/BNTI
module load r/4.0.5-py27
srun Rscript BNTI_32cores_abundwt_10kESVs.R
echo "srun Rscript BNTI_32cores_abundwt_10kESVs.R"
source deactivate shotgun_env
echo "finished BNTI"
date
```

## c. RCBray on Teton

```{r}
RC_Bray_Wisnoski.R
load("WyLakeMicrobes_env_30Aug2022_fortetonBNTI_bindepthcentroids_10888ESVs.RData")
#load required packages
require(phyloseq)
require(vegan)

#pull out OTU table
OTUs<-t(as.data.frame(physeq@otu_table))
# Calculate abundance-weighted Raup-Crick dissimilarities
regional.abunds <- t(as.matrix(colSums(OTUs)))
regional.relabunds <- decostand(regional.abunds, method = "total")
occupancy.probs <- t(as.matrix(colSums(decostand(OTUs, method = "pa")) / nrow(OTUs)))
site.abunds <- rowSums(OTUs)
site.rich <- specnumber(OTUs)
a <- regional.relabunds * occupancy.probs
# Create a null community based on Stegen et al. 2015
set.seed(47405)

# stochastic community assembly nulls


nullcom.rcabund <- function(OTUs, stand = "total", distance = "bray", nsims = 999){
  
  # create output object
  r <- nrow(OTUs)
  c <- ncol(OTUs)
  spec.vec <- 1:ncol(OTUs)
  RCbray.nulls <- array(NA, c(r, r, nsims))
  
  
  for(i in 1:nsims){
    # if(i == 1) pb <- progress_bar$new(total = nsims, force = T)
    # pb$update(ratio = i/nsims)
    
    null.comm <- OTUs * 0
    # for first simulation:
    for(row.i in 1:nrow(null.comm)){
      #print(paste("run :", i, " -> ", row.i, " : ", site.abunds[row.i], " inds"))
      
      while(rowSums(null.comm)[row.i] < site.abunds[row.i]){
        
        
        # choose a species based on its occupancy
        local.specs <- sample(x = spec.vec, size = site.rich[row.i],
                              prob = as.vector(occupancy.probs), replace = FALSE)
        
        local.probs <- decostand(t(as.matrix(regional.abunds[,local.specs])), method = "total")
        
        local.inds <- sample(x = local.specs, size = site.abunds[row.i],
                             prob = as.vector(local.probs), replace = TRUE)
        
        local.abunds <- rle(sort(local.inds))
        
        # add an individual to the local community
        null.comm[row.i, local.abunds$values] <- local.abunds$lengths
      }
    }
    null.bray <- as.matrix(vegdist(decostand(null.comm, method = stand), method = distance))
    RCbray.nulls[,,i] <- null.bray
  }
  return(RCbray.nulls)
}

RCbray_output<-nullcom.rcabund(OTUs=OTUs, stand = "total", distance = "bray", nsims = 999)
save.image("WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData")


run_RC_Bray_Wisnoski.sh
#!/bin/bash
#SBATCH --job-name RC_Bray_W_10K
#SBATCH --mem=110GB
#SBATCH --time=2-00:00:00
#SBATCH --account=microbiome
#SBATCH --output=RC_Bray_W_10K_%A.out
hostname; date
module load miniconda3/4.3.30 gcc/7.3.0 r/4.0.5-py27
source activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/BNTI
module load r/4.0.5-py27
srun Rscript RC_Bray_Wisnoski.R
echo "srun Rscript RC_Bray_Wisnoski.R"
source deactivate shotgun_env
echo "finished BNTI"
date

```

## 4. Hill's number

### a. ran on rarefied data

```{r}
require(iNEXT)
bio1 <- as.matrix(ps@otu_table@.Data)
bio2 <- split(bio1, as.numeric(rep(1:ncol(bio1), each = nrow(bio1))))
bio3 <- iNEXT(bio2, q=0, datatype="abundance") 

# select asymptotic estimates 
bio4 <- bio3$AsyEst
bio5 <- subset(bio4, bio4$Diversity =="Species richness")  
bio6 <- subset(bio4, bio4$Diversity =="Shannon diversity")  
bio7 <- subset(bio4, bio4$Diversity =="Simpson diversity")  

#create a table with diversity estimates
bio8 <- data.frame(cbind(bio5$Observed, bio6$Observed, bio7$Observed))
rownames(bio8) <- colnames(bio1)
colnames(bio8) <- c("Species_Richness", "Shannon_Diversity","Simpson_Dominance")
head(bio8)
write.table(bio8,file="hillnumbers_norm_endpoint_6Sept2022.txt",sep="\t",row.names=T, col.names=T)
hill_norm<-bio8

rm(bio1)
rm(bio2)
rm(bio3) 
rm(bio4)
rm(bio5)
rm(bio6)
rm(bio7)
rm(bio8)
```

### b. regression of richness and depth

```{r}
require(mgcv)
#load environment and Hill's number file. 
meta_sed_26<-metadata[metadata$depth<=26,]
hills<-read.delim("hillnumbers_norm_endpoint_6Sept2022.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")

lm<-lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth)
plot(meta_sed_26_hills$depth, meta_sed_26_hills$Species_Richness)
abline(lm)

summary(lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Shannon_Diversity~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Simpson_Dominance~meta_sed_26_hills$depth))

#tried this with a GAM too, but not much different 
Data<-data.frame(x=meta_sed_26_hills$depth,y=meta_sed_26_hills$Species_Richness)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=3), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)

```

Call: lm(formula = meta_sed_26_hills$Species_Richness ~ meta_sed_26_hills$depth)

Residuals: Min 1Q Median 3Q Max -1829.58 -394.87 -16.98 432.42 1375.02

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 2608.585 47.284 55.168 \<2e-16 ***meta_sed_26_hills\$depth -30.408 3.482 -8.732 \<2e-16***

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 554.2 on 427 degrees of freedom Multiple R-squared: 0.1515, Adjusted R-squared: 0.1495 F-statistic: 76.25 on 1 and 427 DF, p-value: \< 2.2e-16

Call: lm(formula = meta_sed_26_hills$Shannon_Diversity ~ meta_sed_26_hills$depth)

Residuals: Min 1Q Median 3Q Max -877.93 -233.20 -27.67 216.85 1036.11

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 892.845 27.068 32.985 \< 2e-16 ***meta_sed_26_hills\$depth -14.226 1.993 -7.137 4.13e-12***

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 317.2 on 427 degrees of freedom Multiple R-squared: 0.1066, Adjusted R-squared: 0.1045 F-statistic: 50.93 on 1 and 427 DF, p-value: 4.134e-12

Call: lm(formula = meta_sed_26_hills$Simpson_Dominance ~ meta_sed_26_hills$depth)

Residuals: Min 1Q Median 3Q Max -267.24 -86.48 -22.33 68.79 448.47

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 269.7218 10.4390 25.838 \< 2e-16 ***meta_sed_26_hills\$depth -4.0058 0.7688 -5.211 2.94e-07***

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 122.3 on 427 degrees of freedom Multiple R-squared: 0.05978, Adjusted R-squared: 0.05758 F-statistic: 27.15 on 1 and 427 DF, p-value: 2.936e-07

GAM RESULTS Family: gaussian Link function: identity

Formula: y \~ s(x, k = 3)

Parametric coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 2268.14 26.76 84.77 \<2e-16 \*\*\*

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms: edf Ref.df F p-value\
s(x) 1 1 76.25 \<2e-16 \*\*\*

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) = 0.15 Deviance explained = 15.2% GCV = 3.0854e+05 Scale est. = 3.071e+05 n = 429

## 5. Top phyla and families

### a. Caluclate top phyla

went back and changed to Phylum without the kingdom and phylum pasted together, since tax_glom knows how to keep those separate

```{r}
require(stringr)
require(pivottabler)
require(reshape)

#subset samples less than or equal to 26
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)

#Change the Phylum column to include kingdom as well
#tax_tmp<-as.data.frame(ps_tr_26@tax_table@.Data)
#tax_tmp$Phylum_only<-tax_tmp$Phylum
#tax_tmp$Phylum<-paste(tax_tmp$Kingdom,tax_tmp$Phylum,sep="@")
#tax_tmp[tax_tmp$Phylum=="Archaea@Unknown_Order",]$Phylum<-"Archaea@Unassigned"
#ps_tr_26@tax_table@.Data<-as.matrix(tax_tmp)
ps_tr_26_phy<-tax_glom(ps_tr_26, taxrank = "Phylum",NArm = FALSE)
ps_melt<-psmelt(ps_tr_26_phy)
#write.csv(ps_melt, "allphyla_29Sept2022.csv")
ps_melt<-read.csv("allphyla_29Sept2022.csv", header=T, row.names=1)

#which phyla go over a threshold on AVERAGE for each bin depth
    pt <- PivotTable$new()
    pt$addData(ps_melt)
    pt$addRowDataGroups("bin_depth") 
    pt$addColumnDataGroups("Phylum") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    
    
    phy_avg_abund <- pt$asDataFrame()
    phy_avg_abund$bin_depth<-rownames(phy_avg_abund)
    phy_avg_abund<-phy_avg_abund[-which(phy_avg_abund$bin_depth=="Total"),]
    phy_avg_abund$Total<-NULL
    phy_avg_abund$bin_depth <- as.numeric(phy_avg_abund$bin_depth)
    phy_avg_abund <- melt(phy_avg_abund, id.vars = "bin_depth")
    
    rm(pt)
    
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.05),]$variable)))
#9
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.1),]$variable)))
#4
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.08),]$variable)))
#4
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.075),]$variable)))
#6
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.07),]$variable)))
#8


subphy<-unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.07),]$variable))
### this is the one we used! 
# [1] "Bacteroidetes"                      "Chloroflexi"                       
# [3] "Cyanobacteria"                      "Euryarchaeota"                     
# [5] "Miscellaneous_Crenarchaeotic_Group" "Planctomycetes"                    
# [7] "Proteobacteria"                     "Verrucomicrobia" 
#same if we added the domain in the string
# [1] "Archaea@Euryarchaeota"                     
# [2] "Archaea@Miscellaneous_Crenarchaeotic_Group"
# [3] "Bacteria@Bacteroidetes"                    
# [4] "Bacteria@Chloroflexi"                      
# [5] "Bacteria@Cyanobacteria"                    
# [6] "Bacteria@Planctomycetes"                   
# [7] "Bacteria@Proteobacteria"                   
# [8] "Bacteria@Verrucomicrobia"  
#same as before, updated 6 Sept 2022, checked again sept 27

# what percent are these 8 phyla in the entire dataset?

length(unique(ps_melt$samp_names))
#429 samples
sum(ps_melt[ps_melt$Phylum%in%subphy,]$Abundance)/length(unique(ps_melt$samp_names))
#[1] 0.6392306 - 6 sept 2022 and again 29 sept 2022

```

### b. Family abundances

redoing not on Teton

```{r}
require(stringr)
require(pivottabler)
require(reshape)
require(phyloseq)
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

#only taxa <= 26 cm
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)

#subset by the top 8 phyla
subphy<-c("Euryarchaeota" , "Miscellaneous_Crenarchaeotic_Group"
,"Bacteroidetes" ,"Chloroflexi"                      
, "Cyanobacteria", "Planctomycetes"                   
,"Proteobacteria" ,"Verrucomicrobia")
ps_tr_26_subphy<-subset_taxa(ps_tr_26, Phylum%in%subphy)


#change taxonomy that is unknown
tax_tmp<-as.data.frame(ps_tr_26_subphy@tax_table@.Data)
row_names<-rownames(tax_tmp)

tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Order", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Family", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Phylum", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("Unknown_Class", "Unassigned", x)}))
tax_tmp <- data.frame(lapply(tax_tmp, function(x) {gsub("unidentified", "Unassigned", x)}))
#make a string with all of the assignments up to Family
rownames(tax_tmp)<-row_names
ps_tr_26_subphy@tax_table@.Data<-as.matrix(tax_tmp)
length(unique(ps_tr_26_subphy@tax_table[,5]))
#[1] 235
temp2<-as.data.frame(ps_tr_26_subphy@tax_table@.Data)

temp2$alltax_tofam<-paste(temp2$Kingdom,temp2$Phylum,temp2$Class,temp2$Order,temp2$Family,sep="@")
length(unique(temp2$alltax_tofam))
#391
#melt<-psmelt(ps_tr_26_subphy)

ps_tr_26_fam<-tax_glom(ps_tr_26_subphy, taxrank = "Family",NArm = FALSE)
length(unique(rownames(ps_tr_26_fam@tax_table)))
#391
length(unique(ps_tr_26_fam@tax_table[,5]))
#235 - so it correctly conglomerated by family with unique phyla-order

melt<-psmelt(ps_tr_26_fam)
#write.csv(melt, "family_abundances_27sept2022.csv")

```

### i. calculate top 10 families in each bin depth

```{r}
require(pivottabler)
require(reshape)
fam<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/family_abundances_27sept2022.csv",header=T, row.names=1)

fam$alltax_tofam<-paste(fam$Kingdom,fam$Phylum,fam$Class,fam$Order,fam$Family,sep="@")

#which family has a relative abundance greater than 2% on average in any in depth less than or equal to 26 cm
    pt <- PivotTable$new()
    pt$addData(fam)
    pt$addColumnDataGroups("alltax_tofam")
    pt$addRowDataGroups("bin_depth")
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance, na.rm=T)")
    pt$renderPivot()
    pt_df <- pt$asDataFrame()
    pt_df$Total<-NULL
    pt_df<-pt_df[-which(rownames(pt_df)=="Total"),] 
    
    data_long <- as.data.frame(pt_df)    # Reshape data from wide to long
    data_long$bin_depth <- as.numeric(rownames(data_long))
    data_long <- melt(data_long, id.vars = "bin_depth")
    data_long <-data_long[order(data_long$value, decreasing=T),]
        
    fam_GTX<-data_long[which(data_long$value>0.02),]
    length(unique(as.character(fam_GTX$variable))) #16
   families<-sort(unique(as.character(fam_GTX$variable)))
#  [1] "Archaea@Euryarchaeota@Thermoplasmata@Thermoplasmatales@Marine_Benthic_Group_D_and_DHVEG-1"
#  [2] "Archaea@Miscellaneous_Crenarchaeotic_Group@Unassigned@Unassigned@Unassigned"              
#  [3] "Bacteria@Bacteroidetes@Bacteroidetes_vadinHA17@Unassigned@Unassigned"                     
#  [4] "Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"                         
#  [5] "Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned"                                    
#  [6] "Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned"                                        
#  [7] "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned"                                    
#  [8] "Bacteria@Cyanobacteria@Cyanobacteria@SubsectionI@FamilyI"                                 
#  [9] "Bacteria@Cyanobacteria@Cyanobacteria@SubsectionIII@FamilyI"                               
# [10] "Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned"                                   
# [11] "Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned"                           
# [12] "Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned"                         
# [13] "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae"              
# [14] "Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned"                           
# [15] "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae"            
# [16] "Bacteria@Verrucomicrobia@OPB35_soil_group@Unassigned@Unassigned"                          
    
#what percentage are these 16 families in the top 8 phyla
sum(fam[fam$alltax_tofam%in%families,]$Abundance, na.rm=T)/sum(fam$Abundance, na.rm=T) 
 #0.620541 -27 sept 2022

#what percent are these 16 families in the entire dataset
sum(fam[fam$alltax_tofam%in%families,]$Abundance, na.rm=T)/429 
#0.3966688 -27 sept 2022
```

### ii. SFig6 plot the top 17 families

this is where I looked to see which phyla should be split into families. All 17 of these families have greater than 2% relative abundance across the dataset.

```{r}
require(stringr)
require(RColorBrewer)
require(pivottabler)
fam<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/family_abundances_27sept2022.csv",header=T, row.names=1)

fam$Phylum <- gsub("Miscellaneous_Crenarchaeotic_Group", "Bathyarchaeota",  fam$Phylum)
fam$Phylum <- gsub("Chloroflexi", "Chloroflexota", fam$Phylum)
fam$Phylum <- gsub("Bacteroidetes", "Bacteroidota", fam$Phylum)
fam$Class <- gsub("Bacteroidetes", "Bacteroidota", fam$Class)
fam$Family <- gsub("Marine_Benthic_Group_D_and_DHVEG-1", "MBG-D_and_DHVEG-1", fam$Family)

fam$alltax_tofam<-paste(fam$Kingdom,fam$Phylum,fam$Class,fam$Order,fam$Family,sep="@")

 families<-c("Archaea@Euryarchaeota@Thermoplasmata@Thermoplasmatales@Marine_Benthic_Group_D_and_DHVEG-1"
,"Archaea@Miscellaneous_Crenarchaeotic_Group@Unassigned@Unassigned@Unassigned"              
,"Bacteria@Bacteroidetes@Bacteroidetes_vadinHA17@Unassigned@Unassigned"                     
,"Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"                         
,"Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned"                                    
,"Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned"                                        
, "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned"                                    
,"Bacteria@Cyanobacteria@Cyanobacteria@SubsectionI@FamilyI"                                 
, "Bacteria@Cyanobacteria@Cyanobacteria@SubsectionIII@FamilyI"                               
, "Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned"                                   
, "Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned"                           
, "Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned"                         
, "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae"              
,"Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned"                           
, "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae"            
,"Bacteria@Verrucomicrobia@OPB35_soil_group@Unassigned@Unassigned")   


 
families<-as.data.frame(families)
names(families)<-"alltax_tofam"
families$Kingdom<-str_split_fixed(families$alltax_tofam,"@",5)[,1]
families$Phylum<-str_split_fixed(families$alltax_tofam,"@",5)[,2]
families$Class<-str_split_fixed(families$alltax_tofam,"@",5)[,3]
families$Order<-str_split_fixed(families$alltax_tofam,"@",5)[,4]
families$Family<-str_split_fixed(families$alltax_tofam,"@",5)[,5]

families$Phylum <- gsub("Miscellaneous_Crenarchaeotic_Group", "Bathyarchaeota",  families$Phylum)
families$Phylum <- gsub("Chloroflexi", "Chloroflexota", families$Phylum)
families$Phylum <- gsub("Bacteroidetes", "Bacteroidota", families$Phylum)
families$Class <- gsub("Bacteroidetes", "Bacteroidota", families$Class)
families$Family <- gsub("Marine_Benthic_Group_D_and_DHVEG-1", "MBG-D_and_DHVEG-1", families$Family)
families$alltax_tofam<-paste(families$Kingdom,families$Phylum,families$Class,families$Order,families$Family,sep="@")


unique(families$Phylum)

fam$kingphy<-paste(fam$Kingdom,fam$Phylum, sep=": ")
fam$kingphy<-as.factor(fam$kingphy)

fam$kinphycol<-PNWColors::pnw_palette("Bay",8)[fam$kingphy]


temp<-fam
temp<-temp[temp$alltax_tofam%in%families$alltax_tofam,]


w=1
l=1
pdf("Figures/SF6_FamilyRA_top16_diffscale_27Sept2022.pdf",height=15,width=25)
par(mfrow=c(4,4), mar=c(3,3,5,3))
for (w in 1:length(families$alltax_tofam)){
  temp2<-temp[temp$alltax_tofam==families$alltax_tofam[w],]
  lake_drives<-unique(temp2$lake_drive)
  plot(temp2$bin_depth,temp2$Abundance, xlim=c(0,26), main=paste(temp2$Kingdom[1],temp2$Phylum[1],temp2$Class[1],paste("\n",temp2$Order[1],sep=""),temp2$Family[1], sep=", "), cex.axis=1.5, cex.main=2 ,bty="n",col="white", type="l", ylab="Relative abundance (%)", xlab="Depth (cm)")
for(l in 1:length(lake_drives)){
  temp3<-temp2[temp2$lake_drive==lake_drives[l],]
  temp3<-temp3[order(temp3$bin_depth,decreasing = F),]
  temp3<-temp3[,names(temp3)%in%c("bin_depth","Abundance"),]
  temp3<-na.omit(temp3) # removing all NAs and connecting the lines 
  lines(temp3$bin_depth,temp3$Abundance, col="gray")
}
    pt <- PivotTable$new()
    pt$addData(temp2)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = F),]
    lines(meanabund$bin_depth,meanabund$Abundance,col=temp2$kinphycol[1], lwd=7)
}
dev.off()


```

### iii. combine phylum and family

```{r}
#only taxa <= 26 cm
allphy<-read.csv("allphyla_29Sept2022.csv", header=T, row.names=1)
allfam<-read.csv("family_abundances_27sept2022.csv",header=T, row.names=1)

#subset by the top 8 phyla
top8phy<-allphy[allphy$Phylum%in%c("Euryarchaeota" , "Miscellaneous_Crenarchaeotic_Group","Bacteroidetes" ,"Chloroflexi", "Cyanobacteria", "Planctomycetes","Proteobacteria" ,"Verrucomicrobia"),]

table(names(allfam)[1:56]==names(allphy)[1:56])# all of this is true 

#which phyla are NOT broken up
phyla_notbrokenup<-c("Euryarchaeota" ,"Miscellaneous_Crenarchaeotic_Group", "Bacteroidetes", "Cyanobacteria" ,"Verrucomicrobia")

set1<-top8phy[top8phy$Phylum%in%phyla_notbrokenup,]

#phyla broken up
allfam$tax<-paste(allfam$Kingdom,allfam$Phylum,allfam$Class, allfam$Order, allfam$Family, sep="@")

split_into_fam_nosum<-sort(c("Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned", "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae",
 "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae",
                             "Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae",
                             "Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned",
                             "Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned",
                             "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned"))
set2<-allfam[allfam$tax%in%split_into_fam_nosum,]
table(sort(unique(set2$tax))==sort(split_into_fam_nosum))

# #Ones to sum#
# # 1. Proteobacteria
# # 2. Chloroflexi
# # 3. Planctomycetes other 
# # 4. Planctomycetes - Phycisphaerae (class) 
#                              "Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned"
#                              "Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned"
#                              "Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned"

# 1. Proteobacteria - other
sub<-allfam[allfam$Phylum=="Proteobacteria",]
sub2<-sub[sub$tax!="Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned" & sub$tax!="Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae",]

#this is summing all the rest of the Proteobacteria families for each depth and lake drive
df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Proteobacteria@Undifferentiated_Proteobacteria")
    df<-rbind(df,tempdf)
}

set3<-df


# 2. Chloroflexi - other
sub<-allfam[allfam$Phylum=="Chloroflexi",]
sub2<-sub[sub$tax!="Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae" & sub$tax!="Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned" & sub$tax!="Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned"  & sub$tax!="Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned",]

df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Undifferentiated_Chloroflexi")
    df<-rbind(df,tempdf)
}

set3<-rbind(set3,df)


# 3. Planctomycetes - class Phycisphaerae
sub<-allfam[allfam$Phylum=="Planctomycetes",]
sub2<-sub[sub$tax=="Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned" | sub$tax=="Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned" | sub$tax=="Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned",]

df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Planctomycetes@Phycisphaerae")
    df<-rbind(df,tempdf)
}

set3<-rbind(set3,df)

# 4. Planctomycetes - other
sub<-allfam[allfam$Phylum=="Planctomycetes",]
sub2<-sub[sub$tax!="Bacteria@Planctomycetes@Phycisphaerae@MSBL9@Unassigned" & sub$tax!="Bacteria@Planctomycetes@Phycisphaerae@ODP1230B30.09@Unassigned" & sub$tax!="Bacteria@Planctomycetes@Phycisphaerae@Phycisphaerales@Unassigned" & sub$tax!="Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae",]

df<-data.frame(Sample=NULL,Abundance=NULL,tax=NULL)
i=1
for(i in 1:length(unique(sub2$Sample))){
        temp<-sub2[sub2$Sample==unique(sub2$Sample)[i],]
    tempdf<-data.frame(Sample=unique(sub2$Sample)[i],Abundance=sum(temp$Abundance),tax="Bacteria@Undifferentiated_Planctomycetes")
    df<-rbind(df,tempdf)
}

set3<-rbind(set3,df)

#make set3 look like the rest of the allphy and allfam

forset3<-allphy
forset3$Abundance<-NULL
forset3$Phylum<-NULL
forset3$Kingdom<-NULL
forset3$OTU<-NULL
forset3<-unique(forset3)
table(sort(unique(set3$Sample))==sort(forset3$Sample)) #all true

set4<-merge(set3,forset3,by="Sample")
table(set4$Sample%in%set3$Sample)
table(set4$Abundance%in%set3$Abundance)
table(set4$tax%in%set3$tax)

names(set4)
names(set1)

set4$tax2<-set4$tax
set4$tax<-NULL
names(set4)[54]<-"tax"

set1$OTU<-NULL
set1$tax<-paste(set1$Kingdom,set1$Phylum, sep="@")
set1$Kingdom<-NULL
set1$Phylum<-NULL

set2$OTU<-NULL
set2$Kingdom<-NULL
set2$Phylum<-NULL
set2$Class<-NULL
set2$Order<-NULL
set2$Family<-NULL

table(names(set1)==names(set4)) #all true
table(names(set1)==names(set2)) #all true

allset<-rbind(set1,set2,set4) #combine set 1, 2, and 4 (not 3)

write.csv(allset, "Top_phyla_and_families_14Oct2022.csv")
```

### iv. setup for fig. 3

```{r}
require(vegan)
require(rioja)

load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
#sed char summary
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
OTU<-as.data.frame(ps_tr_26@otu_table@.Data)
meta_sed_26<-metadata[metadata$samp_names%in%names(OTU),]

hills<-read.delim("hillnumbers_norm_endpoint_6Sept2022.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")
meta_sed_26_hills$log_cn<-log(meta_sed_26_hills$cn)

depthbins<-sort(unique(meta_sed_26_hills$bin_depth),decreasing=F)

summary<-data.frame(bin_depth=NULL,
                    rich_mean=NULL,rich_sd=NULL,
                    shan_mean=NULL,shan_sd=NULL,
                    simp_mean=NULL,simp_sd=NULL,
                    c13_mean=NULL,c13_sd=NULL,
                    c_perc_mean=NULL,c_perc_sd=NULL,
                    n_mean=NULL,n_sd=NULL,
                    cn_mean=NULL,cn_sd=NULL)
i=1
for(i in 1:length(depthbins)){
  tmp<-meta_sed_26_hills[meta_sed_26_hills$bin_depth==depthbins[i],]
  tmp2<-data.frame(bin_depth=depthbins[i],
                   rich_mean=mean(tmp$Species_Richness,na.rm=T),rich_sd=sd(tmp$Species_Richness,na.rm=T),
                   shan_mean=mean(tmp$Shannon_Diversity,na.rm=T),shan_sd=sd(tmp$Shannon_Diversity,na.rm=T),
                   simp_mean=mean(tmp$Simpson_Dominance,na.rm=T),simp_sd=sd(tmp$Simpson_Dominance,na.rm=T),
                    c13_mean=mean(tmp$d_13_c,na.rm=T),c13_sd=sd(tmp$d_13_c,na.rm=T),
                    c_perc_mean=mean(tmp$c_perc,na.rm=T),c_perc_sd=sd(tmp$c_perc,na.rm=T),
                    n_mean=mean(tmp$n_perc,na.rm=T),n_sd=sd(tmp$n_perc,na.rm=T),
                   cn_mean=mean(tmp$cn,na.rm=T),cn_sd=sd(tmp$cn,na.rm=T))
  summary<-rbind(summary,tmp2)
}


# average CONISS across all samples
OTU<-as.data.frame(ps_tr@otu_table@.Data)
OTU<-t(OTU)

OTU_meta<-as.matrix(ps_tr@sam_data)
OTU_meta<-as.data.frame(OTU_meta)
OTU_meta$bin_depth<-as.numeric(OTU_meta$bin_depth)

#create a mean relative abundance of each ESV for each depth
mean_abundances<-data.frame()
i=1
centimeters<-c(0,2,4,6,8,10,12,14,16,18,20,22,24,26)
  sub<-OTU[rownames(OTU)%in%(OTU_meta[OTU_meta$bin_depth==centimeters[1] & is.na(OTU_meta$bin_depth)==FALSE,]$samp_names),]
  mean_abundances<-as.data.frame(colMeans(sub))
  colnames( mean_abundances)[1]<-centimeters[i]
for(i in 2:length(centimeters)){
  sub<-OTU[rownames(OTU)%in%(OTU_meta[OTU_meta$bin_depth==centimeters[i] & is.na(OTU_meta$bin_depth)==FALSE,]$samp_names),]
  means<-as.data.frame(colMeans(sub))
  colnames(means)[1]<-centimeters[i]
  mean_abundances<-cbind(mean_abundances,means)
}

tma<-as.matrix(t(mean_abundances))
dist<-vegdist(tma)
  clust<-chclust(dist)

domtax<-read.csv("Top_phyla_and_families_14Oct2022.csv",header=T,row.names=1)
lake_drives<-unique(domtax$lake_drive)
```

### v. Fig 3. Dominant Groups

top of plot

```{r}
require(pivottabler)
unique_tax<-c("Bacteria@Proteobacteria@Undifferentiated_Proteobacteria",
          "Bacteria@Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"   ,
                            "Archaea@Euryarchaeota", 
               "Archaea@Miscellaneous_Crenarchaeotic_Group",  
                        "Bacteria@Planctomycetes@Phycisphaerae" )

zone_colors<- c(rep('#018571',1),rep('#C4AD79',2),rep('#a6611a',2))
i=1
l=1
pdf("Figures/Phyla_family_lines_26cm_topcolumn_14Oct2022.pdf", width=13, height=4)
par(mfrow=c(1,13), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)
for(i in 1:(length(unique_tax))){
  temp<-domtax[domtax$tax==unique_tax[i],]
  temp<-temp[order(temp$bin_depth,decreasing=T),]
  pt <- PivotTable$new()
    pt$addData(temp)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = T),]
    plot(temp$Abundance,temp$bin_depth,col="white",ylim=c(26,0),xlim=c(0,max(meanabund$Abundance)*1.8),bty="n",ylab="",xlab="",yaxt="n",cex.axis=1.6)

    for(l in 1:length(lake_drives)){
  temp2<-temp[temp$lake_drive==lake_drives[l],]  
  temp2<-temp2[order(temp2$bin_depth,decreasing = F),]
  lines(temp2$Abundance,temp2$bin_depth, col="gray")}
    lines(meanabund$Abundance,meanabund$bin_depth, col=zone_colors[i],lwd=4)
}


seq<-seq(2,14,2)
lab<-c("Species\nrichness","Shannon\ndiversity","Simpson\ndiversity","d13C", "%C","%N","C:N")
lines<-c(rep(4.5,3), rep(3.5,4))
sed_col<-c(rep("#607B8B",3), rep("#616161",4))
i=1

par(mar=c(6,0.5,1,1), xpd=TRUE)

  sum_temp<-summary[,c(1,seq[i],seq[i]+1)]
  sum_temp$sdp<-sum_temp[,2]+sum_temp[,3]
  sum_temp$sdm<-sum_temp[,2]-sum_temp[,3]
  plot(sum_temp[,2],sum_temp$bin_depth,ylim=c(26,0),xlim=c(min(sum_temp$sdm),max(sum_temp$sdp)),type="l",bty="n", yaxt="n", ylab="",col="white",lwd=2, xlab="", cex.axis=1.6)
    mtext(lab[i],side=1,line=lines[i], cex=1.1)
polygon(x = c(sum_temp$sdp,rev(sum_temp$sdm)),  # X-Coordinates of polygon 
        y = c(sum_temp$bin_depth, rev(sum_temp$bin_depth)),    # Y-Coordinates of polygon
        col = paste(sed_col[i],"50",sep=""), 
        border=paste(sed_col[i],"50",sep="")) 
lines(sum_temp[,2],sum_temp$bin_depth, col=sed_col[i],lwd=3)
  dev.off()
  
pdf("Figures/Phyla_family_lines_26cm_topcolumn_2nd_14Oct2022.pdf", width=13, height=4)  
seq<-seq(2,14,2)
lab<-c("Species\nrichness","Shannon\ndiversity","Simpson\ndiversity","d13C", "%C","%N","C:N")
lines<-c(rep(4.5,3), rep(3.5,4))
sed_col<-c(rep("#607B8B",3), rep("#616161",4))
i=2


par(mfrow=c(1,13), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)

for(i in 2:length(seq)){
  sum_temp<-summary[,c(1,seq[i],seq[i]+1)]
  sum_temp$sdp<-sum_temp[,2]+sum_temp[,3]
  sum_temp$sdm<-sum_temp[,2]-sum_temp[,3]
  plot(sum_temp[,2],sum_temp$bin_depth,ylim=c(26,0),xlim=c(min(sum_temp$sdm),max(sum_temp$sdp)),type="l",bty="n", yaxt="n", ylab="",col="white",lwd=2, xlab="", cex.axis=1.6)
    mtext(lab[i],side=1,line=lines[i], cex=1.1)
polygon(x = c(sum_temp$sdp,rev(sum_temp$sdm)),  # X-Coordinates of polygon 
        y = c(sum_temp$bin_depth, rev(sum_temp$bin_depth)),    # Y-Coordinates of polygon
        col = paste(sed_col[i],"50",sep=""), 
        border=paste(sed_col[i],"50",sep="")) 
lines(sum_temp[,2],sum_temp$bin_depth, col=sed_col[i],lwd=3)
}
 dev.off()
   
```

bottom of plot

```{r}
zone_colors<- c(rep('#018571',5),rep('#C4AD79',3),rep('#a6611a',3))
unique_tax <-c(     "Bacteria@Cyanobacteria", 
                    "Bacteria@Bacteroidetes", 
               "Bacteria@Verrucomicrobia" ,
               "Bacteria@Planctomycetes@Planctomycetacia@Planctomycetales@Planctomycetaceae",
               "Bacteria@Chloroflexi@KD4-96@Unassigned@Unassigned", 
               "Bacteria@Proteobacteria@Deltaproteobacteria@Syntrophobacterales@Syntrophaceae" ,
              "Bacteria@Undifferentiated_Planctomycetes",
               "Bacteria@Chloroflexi@Unassigned@Unassigned@Unassigned",
                "Bacteria@Proteobacteria@Deltaproteobacteria@Sva0485@Unassigned",
               "Bacteria@Undifferentiated_Chloroflexi"     ,
               "Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned")

table(unique_tax%in%domtax$tax)

pdf("Figures/Phyla_family_lines_26cm_bottomcolumn_14Oct2022.pdf", width=13, height=4)
par(mfrow=c(1,13), mar=c(5.5,6,1.5,0))
plot( 0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(5.5,0.5,1.5,1), xpd=TRUE)
for(i in 1:(length(unique_tax))){
  temp<-domtax[domtax$tax==unique_tax[i],]
  temp<-temp[order(temp$bin_depth,decreasing=T),]
  pt <- PivotTable$new()
    pt$addData(temp)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = T),]
    plot(temp$Abundance,temp$bin_depth,col="white",ylim=c(26,0),xlim=c(0,max(meanabund$Abundance)*1.8),bty="n",ylab="",xlab="",yaxt="n",cex.axis=1.6)

    for(l in 1:length(lake_drives)){
  temp2<-temp[temp$lake_drive==lake_drives[l],]  
  temp2<-temp2[order(temp2$bin_depth,decreasing = F),]
  lines(temp2$Abundance,temp2$bin_depth, col="gray")}
  if(meanabund$Abundance>0.05){points(((max(meanabund$Abundance)*1.8)/2),-1, pch=8, lwd=1.5,cex=2)}
    lines(meanabund$Abundance,meanabund$bin_depth, col=zone_colors[i],lwd=4)
}

par(mar=c(5.5,2,1.5,1))
 plot(clust, xvar=as.numeric(clust[["labels"]]),ylim=c(0,3.2), xaxt="n",hang=-1, cex=1.6, cex.lab=1.6, cex.main=1.8, horiz=TRUE, x.rev=TRUE) 
  mtext("Distance",side=1,line=3, cex=1.1)
  mtext("CONISS", side=3, line=0, cex=1.1)
  axis(side=1,at=c(0,1,2,3),labels=c(0,1,2,3), cex.axis=1.6)
dev.off()

```

## 6. NMDS

```{r}
require(colorspace)
require(vegan)
ps_table <- data.frame(otu_table(ps_tr))
ps_table <-t(ps_table)
sd_sed <- data.frame(sample_data(ps_tr))

ord_baseR<-metaMDS(ps_table, distance = "bray")
# Ran 6 Sept 2022
# Run 0 stress 0.1711495 
# Run 1 stress 0.195905 
# Run 2 stress 0.2022131 
# Run 3 stress 0.192464 
# Run 4 stress 0.1985516 
# Run 5 stress 0.1963562 
# Run 6 stress 0.202012 
# Run 7 stress 0.2018219 
# Run 8 stress 0.189256 
# Run 9 stress 0.1951161 
# Run 10 stress 0.187484 
# Run 11 stress 0.1948257 
# Run 12 stress 0.1979875 
# Run 13 stress 0.1976616 
# Run 14 stress 0.4193573 
# Run 15 stress 0.1938638 
# Run 16 stress 0.1873383 
# Run 17 stress 0.1927334 
# Run 18 stress 0.200088 
# Run 19 stress 0.1954923 
# Run 20 stress 0.1938701 
# *** No convergence -- monoMDS stopping criteria:
#      2: no. of iterations >= maxit
#     14: stress ratio > sratmax
#      4: scale factor of the gradient < sfgrmin

fig<-ordiplot(ord_baseR, type="points")

NMDS1<-fig$sites[,1]
NMDS2<-fig$sites[,2]

fig_sites<-as.data.frame(fig[["sites"]])
table(fig_sites$NMDS2==NMDS2) #same for both NMDS1 and NMDS2


fig_sites$samp_names<-rownames(fig_sites)
fig_sites$samp_names<-gsub("X","",as.character(fig_sites$samp_names))
table(names(as.data.frame(ps_tr@otu_table))==fig_sites$samp_names) # all true 

#add in metadata to the dataframe with the NMDS
ord_df<-merge(fig_sites,sd_sed, by="samp_names")

#8c510a darker brown
#a6611a # dark brown
#dfc27d
#80cdc1
#018571 # dark teal

darkertan<-darken("#dfc27d", 0.1)
#C4AD79

ord_df$zone_col<-rep('#01857180',nrow(ord_df))
ord_df[ord_df$bin_depth%in%c(6,8,10,12),]$zone_col<-"#C4AD7980"
ord_df[ord_df$bin_depth%in%c(14,16,18,20,22,24,26),]$zone_col<-'#a6611a80'
ord_df[ord_df$depth>26,]$zone_col<-'#00000080' #black

zone_colors<- c('#018571','#dfc27d','#a6611a','#000000')

sd_sed$bottom_water_temp<-sd_sed$water_sample_t_bot
sd_sed$lake_depth<-sd_sed$max_lake_depth
sd_sed$sediment_depth<-sd_sed$depth

#if you want first axis reversed!
ord_df$NMDS1<-(ord_df$NMDS1*(-1))

ord.fit <- envfit(ord_baseR ~  sediment_depth + lake_depth + bottom_water_temp,  data = sd_sed,  perm = 1000, na.rm = TRUE)
ord.fit[["vectors"]][["arrows"]][1,1]<-(ord.fit[["vectors"]][["arrows"]][1,1]*(-1))

#save.image("NMDS_6Sept2022.Rdata")
```

### ii. plot NMDS

```{r}
load("NMDS_6Sept2022.Rdata")

pdf("NMDS_6Sept2022_legend.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
```

### iii. plot with additional variables (just to check)

```{r}
ord.fit <- envfit(ord_baseR ~  sediment_depth + lake_depth + bottom_water_temp +water_sample_ph_surf +water_sample_ph_bot+water_sample_do_bot + water_sample_t_surf + water_sample_do_surf + elevation_meters,  data = sd_sed,  perm = 1000, na.rm = TRUE)
ord.fit[["vectors"]][["arrows"]][1,1]<-(ord.fit[["vectors"]][["arrows"]][1,1]*(-1))

pdf("NMDS_allvar_13Sept2022.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit,p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()


# which variables are correlated?
meta_sub<-metadata[,names(metadata)%in%c("elevation_meters","water_sample_ph_surf", "water_sample_do_surf", "water_sample_t_surf", "water_sample_ph_bot", "water_sample_do_bot", "water_sample_t_bot", "max_lake_depth" , "latitude"),]
meta_sub<-unique(meta_sub)

cor_results <- cor(meta_sub[1:length(names(meta_sub))], method = "spearman")
x <- which(cor_results > 0.6, arr.ind=TRUE)
x[] <- colnames(cor_results)[x]
rownames(x) <- NULL
x
#only surface water temperature and latitude 

```

### iv. Adonis testing - done 6 Sept 2022

```{r}
require(vegan)
require(parallelDist)
load("NMDS_6Sept2022.Rdata")

ps_table <- data.frame(otu_table(ps_tr)) 
names(ps_table)<-colnames(ps_tr@otu_table)
ps_table<-as.data.frame(t(ps_table))

sd <- data.frame(sample_data(ps_tr)) 
#JVE - first adding surface water temperature for round lake bottom water temperature since the lake it 1.5 m deep
sd[sd$lake_id==40,]$water_sample_t_bot<-20.1


#subset the sd to just the variables we are looking at
sd <- sd[names(sd)%in%c("water_sample_t_bot","max_lake_depth","depth")]
table(is.na(sd)) #make sure there are no NAs

# you must remove any samples with NAs prior to running this analysis 
table(rownames(ps_table)%in%rownames(sd))
adonis_table<-data.matrix(ps_table) 
adonis_dist<-parDist(adonis_table, method = "bray") 
adonis(adonis_dist ~ depth+  water_sample_t_bot + max_lake_depth, data = sd)  
```

Call: adonis(formula = adonis_dist \~ depth + water_sample_t\_bot + max_lake_depth, data = sd)

Permutation: free Number of permutations: 999

Terms added sequentially (first to last)

                    Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)    

depth 1 9.707 9.7074 27.915 0.05236 0.001 ***water_sample_t\_bot 1 7.267 7.2669 20.897 0.03920 0.001*** max_lake_depth 1 3.583 3.5826 10.302 0.01932 0.001 \*\*\* Residuals 474 164.835 0.3478 0.88912\
Total 477 185.392 1.00000

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### iv. Adonis testing - done 28 Sept 2022

```{r}
require(vegan)
require(parallelDist)
load("NMDS_6Sept2022.Rdata")

ps_table <- data.frame(otu_table(ps_tr)) 
names(ps_table)<-colnames(ps_tr@otu_table)
ps_table<-as.data.frame(t(ps_table))

sd <- data.frame(sample_data(ps_tr)) 
#JVE - first adding surface water temperature for round lake bottom water temperature since the lake it 1.5 m deep
sd[sd$lake_id==40,]$water_sample_t_bot<-20.1


sd$zone<-rep("A",nrow(sd))
sd[sd$bin_depth%in%c(6,8,10,12),]$zone<-"B"
sd[sd$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
sd[sd$bin_depth==100,]$zone<-"D"
sd$zone<-as.factor(sd$zone)

#subset the sd to just the variables we are looking at
sd <- sd[names(sd)%in%c("water_sample_t_bot","max_lake_depth","zone")]
table(is.na(sd)) #make sure there are no NAs

# you must remove any samples with NAs prior to running this analysis 
table(rownames(ps_table)%in%rownames(sd))
adonis_table<-data.matrix(ps_table) 
adonis_dist<-parDist(adonis_table, method = "bray") 
adonis(adonis_dist ~ zone+  water_sample_t_bot + max_lake_depth, data = sd)  
```

Call: adonis(formula = adonis_dist \~ zone + water_sample_t\_bot + max_lake_depth, data = sd)

Permutation: free Number of permutations: 999

Terms added sequentially (first to last)

                    Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)    

zone 3 13.452 4.4840 13.159 0.07256 0.001 ***water_sample_t\_bot 1 7.384 7.3843 21.670 0.03983 0.001*** max_lake_depth 1 3.719 3.7186 10.913 0.02006 0.001 \*\*\* Residuals 472 160.837 0.3408 0.86755\
Total 477 185.392 1.00000

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### iiv. plot each zone separately

```{r}
#ord_df_backup<-ord_df
#temp<-ord_df #reversed the whole dataset to temp to subset from
zone<-c("replacement","surface","depauperate","deep_depauperate")
zone_colors<-unique(temp$zone_col)
i=1
for(i in 1:4){
pdf(paste("Figures/NMDS_",zone[i],"_May2022.pdf", sep=""), width=5, height=4.5)
ord_df<-temp[temp$zone_col==zone_colors[i],]
    par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
#plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
}
```

### ix. points by mountain range

```{r}
ord_df<-ord_df_backup
ord_df$mtn_pch<-rep(NA,nrow(ord_df))
ord_df[ord_df$mountain_range=="Snowy",]$mtn_pch<-21 #circle
ord_df[ord_df$mountain_range=="Beartooth",]$mtn_pch<-22 #square
ord_df[ord_df$mountain_range=="Wind River",]$mtn_pch<-23 #diamond
ord_df[ord_df$mountain_range=="Bighorn",]$mtn_pch<-24 #triangle
ord_df$mtn_pch<-as.numeric(ord_df$mtn_pch)

pdf("Figures/NMDS_points_29Apr2022.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col="black",bg=ord_df$zone_col, pch=ord_df$mtn_pch, xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
#legend("top",inset=c(0.3,-0.13),xpd=TRUE,bty ="n",legend = c("                    ","                       ","                       ","                       "), col=zone_colors, pch=16, pt.cex = 3,cex=1,ncol=4)
#plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()

```

## 7. Pairwise comparisons

```{r}
require(vegan)
require(geosphere)
require(simba)

#used transformed data
OTU<-as.data.frame(ps_tr@otu_table@.Data)
OTU<-t(OTU)

#calculate Bray-Curtis similarity between all samples
comm.dist <- 1 - vegdist(OTU)

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),rownames(OTU))),]
table(rownames(OTU)==rownames(metadata)) #all true

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
dist_m_output<-as.dist(dist_m_output)


#transform all distance matrices into db format using liste function in simba
coord.dist.ls<-liste(dist_m_output, entry="geo_dist")
comm.dist.ls<-liste(comm.dist, entry="comm")
#check the names of these match

table(coord.dist.ls[,1]==comm.dist.ls[,1]) #both true
table(coord.dist.ls[,2]==comm.dist.ls[,2])

#create df with similarity of community and distance
comps<-data.frame(comm.dist.ls)
comps$geo_dist<-coord.dist.ls$geo_dist
names(comps)<-c("s1_samp_names","s2_samp_names","comm","geo_dist")
comps$s1_samp_names<-as.character(comps$s1_samp_names)
comps$s2_samp_names<-as.character(comps$s2_samp_names)

#add zone into metadata
metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

#merge in metadata to table
metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

comps<-merge(comps, metadata_s1, by="s1_samp_names")
comps<-merge(comps, metadata_s2, by="s2_samp_names")

#remove metadata notes
comps$s1_notes<-NULL
comps$s2_notes<-NULL
comps$s1_Notes_sed_water_wt<-NULL
comps$s2_Notes_sed_water_wt<-NULL
comps$s1_Notes_carbon_nitrogen<-NULL
comps$s2_Notes_carbon_nitrogen<-NULL
comps$s1_Notes._lake_sed_pH<-NULL
comps$s2_Notes._lake_sed_pH<-NULL

# add in centimeters into the distance
comps$abs_cm<-rep(NA, nrow(comps))
comps$s1_depth<-as.numeric(comps$s1_depth)
comps$s2_depth<-as.numeric(comps$s2_depth)
i=1
for(i in 1:nrow(comps)){
  ifelse(comps$s1_lake_drive[i]==comps$s2_lake_drive[i],comps$abs_cm[i]<-abs(comps$s2_depth[i]-comps$s1_depth[i]),NA)
}

#this one takes forever and adds 1 meter to cores within the same lake
comps$dist_1m_forsamelake<-rep(NA,nrow(comps))
i=1
for(i in 1:nrow(comps)){
 if(comps$s1_lake_name[i]==comps$s2_lake_name[i] & comps$s1_lake_drive[i] != comps$s2_lake_drive[i]){comps$dist_1m_forsamelake[i]<-comps$geo_dist[i]+1}else{comps$dist_1m_forsamelake[i]<-comps$geo_dist[i]}
}


#one sample with 2 replicates, so put in 0.001 for the dist_cm_core column
comps<-comps[order(comps$abs_cm,decreasing=F),]
which(names(comps)=="abs_cm")
comps[1,which(names(comps)=="abs_cm")]<-0.001
#check
comps[1,]

rm(comm.dist.ls)
rm(coord.dist.ls)
rm(metadata_s1)
rm(metadata_s2)
rm(OTU)
rm(xy)
rm(comm.dist)
rm(dist_m_output)
rm(i)
rm(tps_norm)
#write.csv(comps, "pairwise_comparisons_13Sept2022.csv")
```

### i. add env dist

```{r}
require(cluster)
#load data and comparison file
comps<-read.csv("pairwise_comparisons_13Sept2022.csv")

#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

daisy.mat <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))
env_distance <- simba::liste(daisy.mat, entry = "env_dist")
names(env_distance)<-c("s1_samp_names","s2_samp_names","env_dist")

comps$s1_samp_names<-as.character(comps$s1_samp_names)
comps$s2_samp_names<-as.character(comps$s2_samp_names)

env_distance$s1_samp_names<-as.character(env_distance$s1_samp_names)
env_distance$s2_samp_names<-as.character(env_distance$s2_samp_names)

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
env_distance<-env_distance[order(env_distance$s1_samp_names,env_distance$s2_samp_names),]

table(comps$s1_samp_names==env_distance$s1_samp_names)
table(comps$s2_samp_names==env_distance$s2_samp_names)

comps$env_dist<-env_distance$env_dist
rm(env_distance)

#overwrite file
#write.csv(comps, "pairwise_comparisons_13Sept2022.csv")
```

### iii. Fig. 5 env and geodist decay

Absolute sediment distance, only looking at \<26 samples

```{r}
require(mgcv)
comps<-read.csv("pairwise_comparisons_13Sept2022.csv",row.names=1, header=T)
comps$geo_dist_km<-comps$geo_dist/1000 # add a column for geographic distance in km and re-write csv
#write.csv(comps, "pairwise_comparisons_13Sept2022.csv")
```

Start here

```{r}
require(mgcv)
require(patchwork)
require(ggplot2)
comps<-read.csv("pairwise_comparisons_14Sept2022.csv",row.names=1, header=T)
```

summary figure (horizons not broken out)

```{r}
model_result<-list()
theme_comm<-theme_classic()+
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
              axis.text=element_text(size=13), #change font size of axis text
              axis.title=element_text(size=15), #change font size of axis titles  
              axis.title.x = element_text(vjust=-0.3),
              axis.title.y = element_text(margin = margin(r = 10)))
ggplt<-list()
#sediment distance from an individual core
sub<-comps[is.na(comps$abs_cm)==FALSE,]
sub<-sub[sub$s1_zone%in%c("A","B","C") & sub$s2_zone%in%c("A","B","C"),]
table(sub$s1_lake_drive==sub$s2_lake_drive)
#2081 comparisons (16 Feb 2023)


Data<-data.frame(x=sub$abs_cm,y=sub$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
model_result[[1]]<-summary(dat_gam)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

## plot
ggplt[[1]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                           col="black", alpha = 0.7) +
        geom_line(linewidth=1.5) + labs(x = "Sediment distance (cm)", y = "Community similarity (1-Bray)")+
        scale_x_continuous(breaks=seq(0,26,4))+
        ylim(0,0.8)+
        annotate("text", x=(26*0.95), y=0.8, label= "A", size=7)+
        annotate("text", x=(26*0.84), y=0.7,  label="paste(italic(R)^2,\"= 0.32\")", parse=T, size=5) +
        theme_comm


"paste(italic(R)^2,\"=0.11\")"
sub_bind<-NULL
#Geographic distance#
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        sub_bind<-rbind(sub_bind,sub)
}
        Data<-data.frame(x=sub_bind$geo_dist_km,y=sub_bind$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        model_result[[2]]<-summary(dat_gam)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        

ggplt[[2]]<-ggplot(data =Data , aes(x = x, y = fit)) +
        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                    col="black", alpha = 0.7) +
        geom_line(linewidth=1.5) + labs(x = "Distance (km)", y = "")+
        scale_x_continuous(breaks=seq(0,500,100))+
        annotate("text", x=(500*0.95), y=0.8, label= "B", size=7)+
        annotate("text", x=(500*0.82), y=0.7, label="paste(italic(R)^2,\"= 0.087\")", parse=T,  size=5) +
        ylim(0,0.8)+
        theme_comm  


sub_bind<-NULL
#Geographic distance#
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        sub_bind<-rbind(sub_bind,sub)
}
Data<-data.frame(x=sub_bind$env_dist,y=sub_bind$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
model_result[[3]]<-summary(dat_gam)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

   ggplt[[3]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       col="black", alpha = 0.7) +
   geom_line(linewidth=1.5) + labs(x = "Environmental dissimilarity", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           annotate("text", x=(0.73*0.95), y=0.8, label= "C", size=7)+
        annotate("text", x=(0.73*0.82), y=0.7, label="paste(italic(R)^2,\"= 0.175\")", parse=T, size=5) +

           ylim(0,0.8)+
           theme_comm


pdf("Figures/Fig5_simplifed_seddist_envdist_geodist_20Feb2023.pdf", height = 3.5, width= 11)
ggplt[[1]]+ ggplt[[2]] + ggplt[[3]] + plot_layout(ncol = 3)
dev.off()

```

```{r}
print("sediment distance, pval, rsq")
model_result[[1]]$s.pv
model_result[[1]]$r.sq

print("geographic distance, pval, rsq")
model_result[[2]]$s.pv
model_result[[2]]$r.sq

print("environmental distance, pval, rsq")
model_result[[3]]$s.pv
model_result[[3]]$r.sq



```

supplementary figure - GAMS with different horizons

```{r}
theme_comm<-theme_classic()+
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
              axis.text=element_text(size=13), #change font size of axis text
              axis.title=element_text(size=15), #change font size of axis titles  
              axis.title.x = element_text(vjust=-0.3),
              axis.title.y = element_text(margin = margin(r = 10)))
ggplt<-list()
#sediment distance from an individual core
sub<-comps[is.na(comps$abs_cm)==FALSE,]
sub<-sub[sub$s1_zone%in%c("A","B","C") & sub$s2_zone%in%c("A","B","C"),]
table(sub$s1_lake_drive==sub$s2_lake_drive)
#2081 comparisons (16 Feb 2023)


Data<-data.frame(x=sub$abs_cm,y=sub$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

## plot
ggplt[[1]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, alpha=0.3) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                            alpha = 0.3) +
        geom_line(linewidth=1.5) + labs(x = "Sediment distance (cm)", y = "Community similarity (1-Bray)")+
        scale_x_continuous(breaks=seq(0,26,4))+
        ylim(0,0.8)+
        theme_comm





#geographic distance
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        Data<-data.frame(x=sub$geo_dist_km,y=sub$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        
        if(i==1){     
                ggplt[[2]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "", y = "")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm
        }
        if(i==2){
                ggplt[[3]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "", y = "Community similarity (1-Bray)")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm
        }
        if(i==3){
                ggplt[[4]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "Distance (km)", y = "")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm  
        }}

#environmental dissimilarity
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        Data<-data.frame(x=sub$env_dist,y=sub$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        
if(i==1){     
        ggplt[[5]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                            alpha = 0.3) +
                geom_line(linewidth=1.5) + labs(x = "", y = "")+
               scale_x_continuous(breaks=seq(0,0.73,0.1))+
                ylim(0,0.8)+
                theme_comm
}
if(i==2){
   ggplt[[6]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       alpha = 0.3) +
   geom_line(linewidth=1.5) + labs(x = "", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           ylim(0,0.8)+
           theme_comm
}
if(i==3){
   ggplt[[7]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       alpha = 0.3) +
   geom_line(linewidth=1.5) + labs(x = "Environmental dissimilarity", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           ylim(0,0.8)+
           theme_comm
}}

pdf("Figures/Fig5_CommunityAssembly_BC_16Feb2023.pdf",height=7,width=7)
ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ ggplt[[7]]+ 
        plot_layout(ncol = 2, byrow = F)
dev.off()

pdf("Figures/Fig5_CommunityAssembly_A_16Feb2023.pdf",height=2.4,width=3.3)
ggplt[[1]]
dev.off()


```

### iv. add in BNTI and RCBRAY

```{bash}
rsync jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI/WyLakeMicrobes_env_30Aug2022_finalBNTI_abundwt_10888ESVs.RData /Users/jordanscheibe/Desktop

rsync jvonegge@teton.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI/WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData /Users/jordanscheibe/Desktop
 
```

```{r}
require(simba)
load("WyLakeMicrobes_env_30Aug2022_finalBNTI_abundwt_10888ESVs.RData")
comps<-read.csv("pairwise_comparisons_13Sept2022.csv",row.names=1, header=T)

dist_bnti<-as.dist(BNTI[["comdistnt.obs.z"]])
BNTI_dist<-liste(dist_bnti, entry="BNTI")
names(BNTI_dist)<-c("s1_samp_names","s2_samp_names","BNTI")
BNTI_dist$s1_samp_names<-as.character(BNTI_dist$s1_samp_names)
BNTI_dist$s2_samp_names<-as.character(BNTI_dist$s2_samp_names)

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
BNTI_dist<-BNTI_dist[order(BNTI_dist$s1_samp_names,BNTI_dist$s2_samp_names),]

table(comps$s1_samp_names==BNTI_dist$s1_samp_names)
table(comps$s2_samp_names==BNTI_dist$s2_samp_names)

comps$BNTI<-BNTI_dist$BNTI

hist(comps$BNTI, xlab="BNTI", main="Histogram of BNTI")
write.csv(comps, "pairwise_comparisons_bnti_backup_14Sept2022.csv")
rm(physeq)
rm(tree_dist)
rm(dist_bnti)
rm(BNTI)
rm(BNTI_dist)
```

```{r}
#add in RCBRAY to comps, from above
load("WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData")

rc.nulls<-RCbray_output
physeq<-phyloseq(otu_table(physeq),tax_table(physeq),sample_data(physeq))
physeq_t <- transform_sample_counts(physeq=physeq, function(x) x / sum(x))
OTUsREL<-t(as.data.frame(physeq@otu_table))
design<-physeq_t@sam_data

obs.bray <- as.matrix(vegdist(OTUsREL, method = "bray"))
site.compares <- expand.grid(site1 = 1:nrow(obs.bray), site2 = 1:nrow(obs.bray))
site.compares <- site.compares[-which(site.compares[,1] == site.compares[,2]),]
RC.bray <- matrix(NA, nrow = nrow(obs.bray), ncol = nrow(obs.bray))
for(row.i in 1:nrow(site.compares)){
  site1 <- site.compares[row.i,1]
  site2 <- site.compares[row.i,2]
  pairwise.null <- rc.nulls[site1,site2,]
  pairwise.bray <- obs.bray[site1,site2]
  num.greater <- sum(pairwise.null > pairwise.bray)
  num.ties <- sum(pairwise.null == pairwise.bray)
  val <- -1*((((1 * num.greater) + (0.5 * num.ties))/999 - 0.5) * 2)
  RC.bray[site1, site2] <- val
}
rownames(RC.bray) <- rownames(design)
colnames(RC.bray) <- rownames(design)
RC.bray.dist <- as.dist(RC.bray)
range(RC.bray.dist)
RC.bray_dist_df <- simba::liste(RC.bray.dist, entry = "RC_bray")
names(RC.bray_dist_df)<-c("s1_samp_names","s2_samp_names","RCbray")

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
RC.bray_dist_df<-RC.bray_dist_df[order(RC.bray_dist_df$s1_samp_names,RC.bray_dist_df$s2_samp_names),]

table(comps$s1_samp_names==RC.bray_dist_df$s1_samp_names)
table(comps$s2_samp_names==RC.bray_dist_df$s2_samp_names)

comps$RCbray<-RC.bray_dist_df$RCbray

```

### v. add in selection processes

```{r}
comps$process<-rep(NA, nrow(comps))
comps[comps$BNTI< (-2),]$process<-"Homogeneous selection"
comps[comps$BNTI> (2),]$process<-"Variable selection"
comps[comps$BNTI< (2) & comps$BNTI> (-2) & comps$RCbray > (0.95),]$process<-"Dispersal limitation"
comps[comps$BNTI< (2) & comps$BNTI> (-2) & comps$RCbray < (-0.95),]$process<-"Homogenizing dispersal"
comps[comps$BNTI< (2) & comps$BNTI> (-2) & comps$RCbray > (-0.95) & comps$RCbray < 0.95,]$process<-"Drift"
#write.csv(comps, "pairwise_comparisons_14Sept2022.csv")

```

### vi. stats community assembly processes - OLD

```{r}
table(comps$process)/nrow(comps)

#deterministic
  0.6871222687 + 0.2045297054
  
  Stochastic 
  1-(  0.6871222687 + 0.2045297054)
  0.0056665175 + 0.0002543793
  
```

Dispersal limitation Drift Homogeneous selection Homogenizing dispersal 0.1024271291 0.0056665175 0.2045297054 0.0002543793 Variable selection 0.6871222687 \### Fig 6 - Continuous Assembly Processes just like the decay relationships, I'm going to only look 26 cm then I'll also plot the 26+ ones as a supplementary figure

#### 0) RUN THIS FIRST

```{r}
comps_backup<-read.csv("pairwise_comparisons_14Sept2022.csv", header=T, row.names=1) # load this as a backup file and pull from there

comps_backup[comps_backup$process=="Homogeneous selection",]$process<-"B_Homogeneous selection"
comps_backup[comps_backup$process=="Variable selection",]$process<-"A_Variable selection"
comps_backup[comps_backup$process=="Dispersal limitation",]$process<-"E_Dispersal limitation"
comps_backup[comps_backup$process=="Homogenizing dispersal",]$process<-"D_Homogenizing dispersal"
comps_backup[comps_backup$process=="Drift",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

comps<-comps_backup
comps<-comps[comps$s1_zone%in%c("A","B","C") & comps$s2_zone%in%c("A","B","C"),]
```

#### 1) sediment depth

both figures (across sed depth all processes and the GAMs)

```{r}
require(Hmisc)


comps$d_val<-abs(comps$abs_cm)
comps<-comps[is.na(comps$d_val)==F,]
comps$d_val_group_breaks<-cut2(comps$d_val, g=10)
comps$d_val_group<-as.numeric(cut2(comps$d_val, g=10))
levels(comps$d_val_group_breaks)
break_labels<- c("[0, 2.5)" ,"[2.5, 5)" ,"[5, 7)", "[7, 9)" ,"[9, 11)" ,"[11, 13)"
,"[13, 15)" ,"[15, 20)" ,"[20, 26]")

tmp<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL)
proc<-unique(comps$process)
groups<-sort(unique(comps$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:5){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i]))
  }}


ggplt <- ggplot(tally,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_line() + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
        labels=break_labels) + xlab(label = "Sediment distance (cm)")+ ylab(label = "Proportion") + geom_line(size=2) +   labs(color="Assembly process")    +theme(axis.ticks.x = element_blank()) + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) + theme(axis.text.x = element_text(angle = 45,hjust=1))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=14), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) #change font size of legend title 
pdf("Figures/Fig6A_Continuous_EAP_sediment_distance_16Sept2022.pdf", height=4, width=8)
ggplt
dev.off()

unique(tally$n)
#341 311 274 241 208 180 149 213 164
```

```{r}

require(mgcv)
#Plot the sediment depth GAM for each process
pdf("Figures/Fig7_seddepth_GAM_16Sept2022.pdf", height=3, width=10)
par(mfrow=c(1,5),mar=c(6,4,2,1))
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="black", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels)),1), las=2, labels = break_labels)
title(xlab=c("","","Sediment distance (cm)","","")[i], cex.lab=2, line=4)
title(ylab=c("Proportion","","","","")[i], cex.lab=2)

Data<-data.frame(x=sub$d_val_group, y=sub$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=new_colors[i], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(new_colors[i], alpha.f = 0.40), border = NA)
        gam_res<-summary(Gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
}
dev.off()

```

#### 1.5) SFig 7 sediment distance with deep samples only

```{r}
comps<-comps_backup
comps$d_val<-comps$abs_cm
comps<-comps[is.na(comps$d_val)==F,]
comps<-comps[comps$d_val>26,] # 119 comparisons, so no many compared to the other ones

tmp2<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL)
proc<-unique(comps_backup$process)
for(p in 1:5){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2)))
  }
tally$group<-rep("deep", nrow(tally))

EAP_plot<-ggplot(tally, aes(x = group, y = percent , fill = process)) +
    geom_bar(stat="identity") + scale_fill_manual(values=new_colors,labels= c("Variable selection", "Homogeneous selection", "Drift", "Homogenizing dispersal", "Dispersal limitation" )) 
pdf("Figures/SFig7_deep_community_assembly.pdf", height=4, width=4)
EAP_plot
dev.off()
```

#### 2) environmental distance within each zone

old breaks "[0.0162,0.117)" "[0.1167,0.163)" "[0.1632,0.208)" "[0.2080,0.240)" "[0.2399,0.276)" "[0.2759,0.323)" "[0.3234,0.384)" [8] "[0.3835,0.454)" "[0.4543,0.540)" "[0.5399,0.722]"

determine breaks here

```{r}
require(stringr)
require(Hmisc)
comps$d_val<-comps$env_dist

#make breaks for zone A
comps_tmp<-comps[comps$s1_zone=="A" & comps$s2_zone=="A",]

comps_tmp<-comps_tmp[comps_tmp$d_val!=0,]
comps_tmp$d_val_group_breaks<-cut2(comps_tmp$d_val, g=10)
levels(comps_tmp$d_val_group_breaks)
breaks<-c(as.numeric(gsub("\\]","" ,gsub("\\[","",unlist(str_split(as.character(levels(comps_tmp$d_val_group_breaks)), pattern=",")))[c(seq(1, 20, 2),20)])))

#make sure first value is below the minimum env_distance measure that isn't 0
min(comps_backup[comps_backup$env_dist!=0,]$env_dist)
#[1] 0.01621537

# make a bottom break (0.016 - just below the lowest env_dist) that is above zero but below the first break
breaks<-c(0.016,breaks[2:11]) 

full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)

zone<-c("A","B","C")
j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

#breaks_labels<- levels(comps$d_val_group_breaks)
breaks_labels<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="env"))
  }
  }

full_output<-rbind(full_output,tally)
}
#check to make sure they add to one
range(full_output$n)
#[1]  171 2020 #11 jan 2023
mean(full_output$n)
#947.6364 #11 jan 2023


#write.csv(full_output,"Community_assembly_env_dist_summary_11Jan2023.csv")
```

redox (number of comparisons) 171 0.016 655 0.1167 616 0.1632 602 0.208 646 0.2399 628 0.2759 617 0.3234 634 0.3835 621 0.4543 626 0.5399 625 0.722 transition 315 0.016 1045 0.1167 1092 0.1632 909 0.208 929 0.2399 928 0.2759 1052 0.3234 996 0.3835 1028 0.4543 1030 0.5399 972 0.722 depauperate 821 0.016 1646 0.1167 2020 0.1632 1223 0.208 1511 0.2399 1414 0.2759 1579 0.3234 1122 0.3835 1238 0.4543 1133 0.5399 828 0.722

#### 3) geographic distance

11 Jan 2023, update by separate out cores in same lake reload 0) from above

```{r}
comps[comps$s1_lake_id==comps$s2_lake_id & comps$s1_lake_drive!=comps$s2_lake_drive,]$geo_dist_km<-0.001 #add one meter for cores in the same lake
comps$d_val<-comps$geo_dist_km
breaks<-c(seq(0,15,5), seq(100, 300, 200), seq(400,500,100))  ##nothing from  43.30254  174.6661
breaks<-c(0,0.1, breaks[2:8])
full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)


zone<-c("A","B","C")
j=1
for( j in 1:3) {
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

breaks_labels<- c("[0,0]",levels(cut(comps$d_val, breaks=breaks)))

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="geo"))
  }
  }
full_output<-rbind(full_output,tally)
#count the number of comparisons in each zone
}

range(full_output$n)
#[1]  75 2604 # 11 Jan 2023
mean(full_output$n)
#1158.222
print(breaks_labels)

#write.csv(full_output,"Community_assembly_geo_dist_summary_11Jan2023.csv")
```

"[0,0]" "(0,0.1]" "(0.1,5]" "(5,10]" "(10,15]" "(15,100]" "(100,300]" "(300,400]" "(400,500]"

Redox (number of comparisons in each bin) 96 0 75 0.1 705 5 558 10 583 15 757 100 1537 300 1136 400 994 500

Transition 191 0 124 0.1 944 5 810 10 844 15 1486 100 2604 300 1869 400 1424 500 Depauperate 390 0 431 0.1 1002 5 1168 10 1448 15 2461 100 2437 300 2599 400 2599 500

combine the two dataframes

```{r}
geo_dist_tally<-read.csv("Community_assembly_geo_dist_summary_11Jan2023.csv",header=T, row.names=1) #removed from folder after combined dataframe made
env_dist_tally<-read.csv("Community_assembly_env_dist_summary_11Jan2023.csv", header=T, row.names = 1) #removed from folder after combined dataframe made
full_output<-rbind(geo_dist_tally,env_dist_tally)

#add in color
full_output$zone_col<-rep('#018571',nrow(full_output))
full_output[full_output$zone=="B",]$zone_col<-"#C4AD79"
full_output[full_output$zone=="C",]$zone_col<-'#a6611a'

#add in xlab as dist column
full_output[full_output$dist=="env",]$dist<-'Environmental dissimilarity'
full_output[full_output$dist=="geo",]$dist<-'Distance (km)'
#write.csv(full_output,"Community_assembly_env_geo_dist_summary_11Jan2023.csv")
```

####4) GAMs with 0s for each process this most recent version breaks out the intra-lake comparisons (so different cores within the same lake (just for the geographic distnace))

```{r}
require(mgcv)
full_output<-read.csv("Community_assembly_env_geo_dist_summary_11Jan2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( "[0,0]","(0,0.1]" ,  "(0.1,5]"  , "(5,10]" , "(10,15]"  ,"(15,100]" , "(100,300]", "(300,400]" ,"(400,500]")
break_labels[[2]]<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")
```

GAM figures

```{r}
model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_",unique(full_output$dist)[d],"_GAM_11Jan2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(0,(length(break_labels[[d]])-1),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=gam_res$r.sq, p=gam_res$s.pv))
        
}}
dev.off()
}

#write.csv(model_results, "CommunityAssembly_GAMS_with0_11Jan2023.csv")
```

#### 5) Redoing the GAMS without 0

```{r}
require(mgcv)
full_output<-read.csv("Community_assembly_env_geo_dist_summary_11Jan2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( "[0,0]","(0,0.1]" ,  "(0.1,5]"  , "(5,10]" , "(10,15]"  ,"(15,100]" , "(100,300]", "(300,400]" ,"(400,500]")
break_labels[[2]]<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

full_output<-full_output[full_output$d_val_group!=0,]
```

```{r}
model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_",unique(full_output$dist)[d],"_GAM_11Jan2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels[[d]])),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
        
}}
dev.off()
}
#write.csv(model_results, "CommunityAssembly_GAMS_without0_11Jan2023.csv")

```

#### 6) redoing the Fig6 with patchwork

```{r}
require(patchwork)

full_output<-read.csv("Community_assembly_env_geo_dist_summary_11Jan2023.csv", header=T, row.names = 1)

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[1]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")


ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
for(i in 1:2){
        sub2<-sub[sub$dist==unique(sub$dist)[i],]       
ifelse(j==3, 
       
       
       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_point(size=3)+ geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(size=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.9) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) +
                theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       
       , 
       
       
              ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) + geom_point(size=3)+  geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(size=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.9) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15))+ #change font size of legend title
        theme(axis.title.x=element_blank(),
        axis.text.x=element_blank()) +
        theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       )
        }}


pdf("Figures/Fig6_community_assembly_16Jan2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
  plot_layout(ncol = 2)
dev.off()


```

## 8. SFig 2 - top 3 ESVs

```{r}
require(mgcv)
#determine top 3 ESVs
means <- as.data.frame(rowMeans(ps_tr@otu_table))
names(means)<-"val"
means$otu<-rownames(means)
avgabundtax<-means[order(means$val,decreasing=T),][1:10,]
top10esvs<-merge(avgabundtax,tax_tab,by="row.names")
top10esvs<-top10esvs[order(top10esvs$val,decreasing=T),]
Eury_esvs<-top10esvs$Row.names[1:3]

# subset ps_tr by top 3 esvs
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
tax_tmp<-as.data.frame(ps_tr_26@tax_table@.Data)
row_names<-rownames(tax_tmp)

#make a string with all of the assignments up to Family
tax_tmp$centroid <-rownames(tax_tmp)
ps_tr_26@tax_table@.Data<-as.matrix(tax_tmp)

sub<-subset_taxa(ps_tr_26, centroid%in%Eury_esvs)
melt<-psmelt(sub)

pdf("Figures/SFig2_Euyarchaeota_top3_ESVs_12Sept2022.pdf", width=10, height =4)
par(mfrow=c(1,3))
titles<-c("Methanoregula","MBG-D","Methanosaeta")
i=1
gam_res<-list()
for(i in 1:3){
        tmp<-melt[melt$centroid==Eury_esvs[i],]
        Data<-data.frame(x=tmp$bin_depth,y=tmp$Abundance)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050", ylab = "Relative abundance", xlab="Sediment depth (cm)", main=c("Methanoregula","MBG-D","Methanosaeta")[i])
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res[[i]]<-summary(dat_gam)
        text(x=23, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res[[i]]$r.sq, digits=3), sep=""))
        text(x=22, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res[[i]]$s.pv, digits=3), sep=""))
         text(x=1, y=max(Data$y)*0.95, c("a", "b", "c")[i], cex=2)
        
}
dev.off()


        
```

## 9. SFig 3 - Bathy and Dehalo

both of these families are most abundant in the depauperate zone, but seem to only be at high abundances when the other is at low abundances

```{r}

fam<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/family_abundances_6sept2022.csv",header=T, row.names=1)

bath<-fam[fam$alltax_tofam=="Archaea@Miscellaneous_Crenarchaeotic_Group@Unassigned@Unassigned@Unassigned",]
names(bath)[3]<-"bathabund"
dehalo<-fam[fam$alltax_tofam=="Bacteria@Chloroflexi@Dehalococcoidia@MSBL5@Unassigned",]
names(dehalo)[3]<-"dehaloabund"

bath_dehalo<-merge(bath, dehalo, by="Sample")

pdf("Figures/SFig3_dehalo_bathy_13Sept2022.pdf", height=5, width=6)
par(mar=c(4,5,4,4))
plot(bath_dehalo$dehaloabund, bath_dehalo$bathabund, pch=16, col="#00000050", xlab="Dehalococcoidia (family MSBL5)\nrelative abundance", ylab="Bathyarchaeota (family unassigned)\nrelative abundance", main="Dehalococcoidia MSBL5 v. Bathyarchaeota")
dev.off()

```

## 10. SFig 1 - individual CONISS

```{r}
require(rioja)
require(vegan)
# use transformed data
OTU<-as.data.frame(ps_tr@otu_table)
OTU<-t(OTU)

lake_drives<-unique(metadata$lake_drive)
rn<-rownames(OTU)
meta_sed<-metadata
meta_sed<-meta_sed[order(match(rownames(meta_sed),rownames(OTU))),]
table(meta_sed$samp_names==rownames(OTU)) #all true
which(table(metadata$lake_drive)<2)

#remove lakes with one sample
lake_drives<-lake_drives[lake_drives!="BN_1"]
lake_drives<-lake_drives[lake_drives!="ML_1"]

#lakes with two samples, can't be used
lake_drives<-lake_drives[lake_drives!="LB_1"]
lake_drives<-lake_drives[lake_drives!="NB_1"]
lake_drives<-lake_drives[lake_drives!="SG_1"]


#individual lake CONISS clustering
pdf("Figures/SFig1_CONISS_individ_12Sept2022.pdf",height=12,width=17)
i=1
par(mfrow=c(5,9),mar=c(2,4,2,0))
for(i in 1:length(lake_drives)){
  subset<-meta_sed[meta_sed$lake_drive==lake_drives[i],]
  subset<-subset[order(subset$depth),]
  names<-subset$samp_names
  rows<-which(rn%in%names)
  sub_otu<-as.data.frame(OTU[rows,])
  sub_otu$rownames<-rownames(sub_otu)
  new_dataset <- sub_otu[match(names, sub_otu$rownames), ]       
  rownames(new_dataset)==new_dataset$rownames
  rownames(new_dataset)==rownames(subset)
  new_dataset$rownames<-NULL
  if(length(unique(subset$depth)) < length(subset$depth)){
      dup<-subset$depth[which(duplicated(subset$depth))]
      pos<-which(subset$depth==dup)
        subset$depth[pos[2]]<-subset$depth[pos[2]]+0.5
  }
  rownames(new_dataset)<-subset$depth
  new_dataset<-as.matrix(new_dataset)
  dist<-vegdist(new_dataset)
  clust<-chclust(dist)
  plot(clust, xvar=as.numeric(clust[["labels"]]), main=paste(subset$lake_name[1],subset$drive[1], sep=" "), ylim= c(0,5), hang=-1, horiz=TRUE, x.rev=TRUE)
}
dev.off()

```

## 11. Anero and methanos

```{r}
require(mgcv)
#check what genuses are in the Methanosaetaceae
sub<-subset_taxa(ps_tr, Family=="Methanosaetaceae")
plot_bar(sub, fill="Genus")
#methanosaeta is the only genus in the family methanosaetaceae 
sub<-tax_glom(sub,taxrank = "Family")

methanosaeta<-psmelt(sub)
names(methanosaeta)[3]<-"abund_methanosaeta"

sub<-subset_taxa(ps_tr, Family=="Anaerolineaceae")
sub<-tax_glom(sub,taxrank = "Family")

anaero<-psmelt(sub)
names(anaero)[3]<-"abund_anaero"

#lm
methano_anaero<-merge(methanosaeta,anaero, by="Sample")
plot(methano_anaero$abund_methanosaeta, methano_anaero$abund_anaero)
summary(lm(methano_anaero$abund_methanosaeta~methano_anaero$abund_anaero))


#gam
Data<-data.frame(x=methano_anaero$abund_methanosaeta,y=methano_anaero$abund_anaero)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050",ylab="Anaerolineaceae abundance", xlab="Methanosaeta abundance", main="Anaerolineaceae v. Methanosaeta")
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res<-summary(dat_gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
         
gam_res
        
```

## 12. Methanoreg and Syntro

```{r}
#look to see what genuses are in Methanoregulaceae
sub<-subset_taxa(ps_tr, Family=="Methanoregulaceae")
sub<-tax_glom(sub,taxrank = "Genus")
plot_bar(sub, x="bin_depth", fill="Genus") #mainly methanoregula

#subset and glom by the family
sub<-subset_taxa(ps_tr, Family=="Methanoregulaceae")
sub<-tax_glom(sub,taxrank = "Family")
meth<-psmelt(sub)
names(meth)[3]<-"abund_meth"

sub<-subset_taxa(ps_tr, Family=="Syntrophaceae")
sub<-tax_glom(sub,taxrank = "Family")

syn<-psmelt(sub)
names(syn)[3]<-"abund_syn"

methano_syn<-merge(meth,syn, by="Sample")
plot(methano_syn$abund_meth, methano_syn$abund_syn)
#lm
summary(lm(methano_syn$abund_meth~methano_syn$abund_syn))

#gam
Data<-data.frame(x=methano_syn$abund_meth,y=methano_syn$abund_syn)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050",ylab="Syntrophaceae abundance", xlab="Methanoregulaceae abundance", main="Syntrophaceae v. Methanoregulaceae")
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res<-summary(dat_gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
         
gam_res
```

## 13. Bathy and Thaum

```{r}
sub<-subset_taxa(ps_tr, Phylum=="Thaumarchaeota")
sub<-tax_glom(sub,taxrank = "Phylum")

sub1<-psmelt(sub)
names(sub1)[3]<-"abund_sub1"

sub<-subset_taxa(ps_tr, Phylum=="Miscellaneous_Crenarchaeotic_Group")
sub<-tax_glom(sub,taxrank = "Phylum")

sub2<-psmelt(sub)
names(sub2)[3]<-"abund_sub2"

sub3<-merge(sub1,sub2, by="Sample")
plot(sub3$abund_sub1, sub3$abund_sub2)
summary(lm(sub3$abund_sub1~sub3$abund_sub2))


Data<-data.frame(x=sub3$abund_sub1,y=sub3$abund_sub2)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=5), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        plot(Data$x,Data$y, pch=16, col="#00000050",ylab="Bathyarchaeota abundance", xlab="Thaumarchaeota abundance", main="Thaumarchaeota v. Bathyarchaeota")
        lines(Data$x,pred$fit, col="#225ea8", lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor("#41b6c4", alpha.f = 0.40), border = NA)
        gam_res<-summary(dat_gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
         
gam_res
```

## 14. SFig. 4 EnvDist vs GeoDist

```{r}
comps<-read.csv("pairwise_comparisons_14Sept2022.csv",row.names=1, header=T)

sub<-comps[,names(comps)%in%c("env_dist", "geo_dist_km")]
sub<-unique(sub)
sub<-sub[order(sub$geo_dist_km,sub$env_dist ),]
sub<-sub[2:nrow(sub),]# get rid of row with only zero for both values
pdf("Figures/SFig4_env_geo_dist_14Sept2022.pdf", height=3, width =4)
par(mar=c(4,4,2,2))
plot(sub$geo_dist_km, sub$env_dist,  pch=16, col="#00000050", ylab="Environmental dissimilarity", xlab="Geographic distance (km)")
abline(lm(sub$env_dist~sub$geo_dist_km), lwd=4, col="plum3")
dev.off()
summary(lm(sub$env_dist~sub$geo_dist_km))
```

Call: lm(formula = sub$env_dist ~ sub$geo_dist_km)

Residuals: Min 1Q Median 3Q Max -0.29498 -0.11251 -0.01654 0.10730 0.39676

Coefficients: Estimate Std. Error t value Pr(\>\|t\|)\
(Intercept) 3.255e-01 9.463e-03 34.40 \<2e-16 \*\*\* sub\$geo_dist_km -5.871e-05 3.394e-05 -1.73 0.0842 .

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1522 on 662 degrees of freedom Multiple R-squared: 0.004498, Adjusted R-squared: 0.002994 F-statistic: 2.991 on 1 and 662 DF, p-value: 0.08418

## 15. mantel tests of RCBray values

These all need to be dist objects

```{r}
#this is copied from above for the mantel tests
require(cluster)
require(geosphere)
#load data

#Environmental distance
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

env_dist <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))

#RCbray distance matrix
load("WyLakeMicrobes_10888ESVs_RCBRAY_output_30Aug2022.RData")

rc.nulls<-RCbray_output
physeq<-phyloseq(otu_table(physeq),tax_table(physeq),sample_data(physeq))
physeq_t <- transform_sample_counts(physeq=physeq, function(x) x / sum(x))
OTUsREL<-t(as.data.frame(physeq@otu_table))
design<-physeq_t@sam_data

obs.bray <- as.matrix(vegdist(OTUsREL, method = "bray"))
site.compares <- expand.grid(site1 = 1:nrow(obs.bray), site2 = 1:nrow(obs.bray))
site.compares <- site.compares[-which(site.compares[,1] == site.compares[,2]),]
RC.bray <- matrix(NA, nrow = nrow(obs.bray), ncol = nrow(obs.bray))
for(row.i in 1:nrow(site.compares)){
  site1 <- site.compares[row.i,1]
  site2 <- site.compares[row.i,2]
  pairwise.null <- rc.nulls[site1,site2,]
  pairwise.bray <- obs.bray[site1,site2]
  num.greater <- sum(pairwise.null > pairwise.bray)
  num.ties <- sum(pairwise.null == pairwise.bray)
  val <- -1*((((1 * num.greater) + (0.5 * num.ties))/999 - 0.5) * 2)
  RC.bray[site1, site2] <- val
}
rownames(RC.bray) <- rownames(design)
colnames(RC.bray) <- rownames(design)
RCbray<- as.dist(RC.bray)


# geographic distance
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
geo_dist<-as.dist(dist_m_output)

#check to make sure all labels match!
table(labels(RCbray)==labels(env_dist)) #all true
table(labels(RCbray)==labels(geo_dist)) #all true

mantel(RCbray, env_dist)
mantel(RCbray, geo_dist)

#add partial mantel test, spatially structured environmental variables
mantel.partial(RCbray, env_dist, geo_dist, method = "pearson", permutations = 999)
mantel.partial(RCbray, geo_dist, env_dist, method = "pearson", permutations = 999)
```

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = env_dist)

Mantel statistic r: 0.2314 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.00874 0.01089 0.01331 0.01641 Permutation: free Number of permutations: 999

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = geo_dist)

Mantel statistic r: 0.1938 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0091 0.0119 0.0146 0.0191 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = env_dist, zdis = geo_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.2283 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0088 0.0108 0.0130 0.0161 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = geo_dist, zdis = env_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.19 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0109 0.0147 0.0176 0.0209 Permutation: free Number of permutations: 999

## 16. Do archeal ESVs increase with depth? - I need to go back through this and make sure its clean

ran this with rarefied data, not transformed

```{r}
require(iNEXT)
arch<-subset_taxa(ps, Kingdom=="Archaea")

bio1 <- as.matrix(arch@otu_table@.Data)
bio2 <- split(bio1, as.numeric(rep(1:ncol(bio1), each = nrow(bio1))))
bio3 <- iNEXT(bio2, q=0, datatype="abundance") 

# select asymptotic estimates 
bio4 <- bio3$AsyEst
bio5 <- subset(bio4, bio4$Diversity =="Species richness")  
bio6 <- subset(bio4, bio4$Diversity =="Shannon diversity")  
bio7 <- subset(bio4, bio4$Diversity =="Simpson diversity")  

#create a table with diversity estimates
bio8 <- data.frame(cbind(bio5$Observed, bio6$Observed, bio7$Observed))
rownames(bio8) <- colnames(bio1)
colnames(bio8) <- c("Species_Richness", "Shannon_Diversity","Simpson_Dominance")
head(bio8)
#write.table(bio8,file="hillnumbers_norm_endpoint_archaea_30Sept2022.txt",sep="\t",row.names=T, col.names=T)
hill_norm<-bio8

rm(bio1)
rm(bio2)
rm(bio3) 
rm(bio4)
rm(bio5)
rm(bio6)
rm(bio7)
rm(bio8)
```

```{r}
require(mgcv)
#load environment and Hill's number file. 
meta_sed_26<-metadata[metadata$depth<=26,]
hills<-read.delim("hillnumbers_norm_endpoint_archaea_30Sept2022.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")

lm<-lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth)
plot(meta_sed_26_hills$depth, meta_sed_26_hills$Species_Richness)
abline(lm)

summary(lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Shannon_Diversity~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Simpson_Dominance~meta_sed_26_hills$depth))

#tried this with a GAM too, but not much different 
Data<-data.frame(x=meta_sed_26_hills$depth,y=meta_sed_26_hills$Species_Richness)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=3), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)
```

all three LMs are not significant in archeal ASV richness with depth

looking at archeal esvs by zone

```{r}

ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
arch<-subset_taxa(ps_tr_26, Kingdom=="Archaea")
arch_esv<-as.data.frame(otu_table(arch))

#subset metadata less than 27 cm 
meta_sub<-metadata[metadata$samp_names%in%names(arch_esv),]
meta_sub$zone<-rep("A",nrow(meta_sub))
meta_sub[meta_sub$bin_depth%in%c(6,8,10,12),]$zone<-"B"
meta_sub[meta_sub$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
meta_sub$zone<-as.factor(meta_sub$zone)

#subset, check, and merge
meta_sub<-meta_sub[,names(meta_sub)%in%c("samp_names","bin_depth","zone","lake_drive")]
arch_esv<-as.data.frame(t(as.matrix(arch_esv)))
table(rownames(meta_sub)%in%rownames(arch_esv))

combo<-merge(arch_esv, meta_sub, by="row.names")


#turn to presence absence
for(i in 1:nrow(combo)){
        for(j in 2:10421){
                if(combo[i,j]>0){
                combo[i,j]<-1
                }}}

presence<-as.data.frame(rowSums(combo[,2:10421]))
names(presence)<-"num_ESVs"

presence$depth_bin<-combo$bin_depth
presence$lake_drive<-combo$lake_drive
presence$zone<-as.factor(combo$zone)
presence$horizon<-rep(NA,nrow(presence))
presence[presence$zone=="A",]$horizon<-"Redox"
presence[presence$zone=="B",]$horizon<-"Transition"
presence[presence$zone=="C",]$horizon<-"Depauperate"

library(dplyr)
group_by(presence,zone) %>%
  summarise(
    count = n(),
    mean = mean(num_ESVs, na.rm = TRUE),
    sd = sd(num_ESVs, na.rm = TRUE)
  )


library("ggpubr")
ggboxplot(presence, x ="horizon", y = "num_ESVs", color = c('#018571','#dfc27d','#a6611a'), order=c("Redox","Transition","Depauperate"), ylab = "Number of archeal ESVs", xlab = "Horizon")

summary(aov(num_ESVs~ horizon, data = presence))

model<-(aov(num_ESVs~ horizon, data = presence))
TukeyHSD(model)


#              Df  Sum Sq Mean Sq F value  Pr(>F)   
# horizon       2  117721   58861   5.988 0.00272 **
# Residuals   426 4187360    9829                   
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


# 
#   Tukey multiple comparisons of means
#     95% family-wise confidence level
# 
# Fit: aov(formula = num_ESVs ~ horizon, data = presence)
# 
# $horizon
#                             diff       lwr       upr
# Redox-Depauperate      -26.48538 -54.67975  1.708988
# Transition-Depauperate  16.40607  -9.96735 42.779484
# Transition-Redox        42.89145  13.65890 72.123991
#                            p adj
# Redox-Depauperate      0.0707331
# Transition-Depauperate 0.3098302
# Transition-Redox       0.0017758



i=1
track<-NULL
for(i in 1:length(unique(presence$depth_bin))){
        sub<-presence[presence$depth_bin==unique(presence$depth_bin)[i],]
        track<-c(track,mean(sub$num_ESVs))
}
track<-data.frame(bin_depth=unique(presence$depth_bin), meanESV=track)
track<-track[order(track$bin_depth),]
presence<-presence[order(presence$depth_bin),]

i=1
        plot(presence$depth_bin,presence$num_ESVs, type="l", col="white", xlab="Sediment depth (cm)", ylab="Number of archeal ESVs")
for(i in 1:length(unique(presence$lake_drive))){

        sub3<-presence[presence$lake_drive==unique(presence$lake_drive)[i],]
        lines(sub3$depth_bin, sub3$num_ESVs, type="l", col="gray")
}

lines(track$bin_depth,track$meanESV, type="b", pch=16, col="darkgreen")
legend("topright", legend=c("Individual core", "Average across cores"),lty=1, lwd=2,col = c("gray","darkgreen") )



```

## 17. correlation of env dist measures

```{r}

# I'm duplicating here, but just looking
plot(metadata$water_sample_ph_bot, metadata$max_lake_depth)
summary(lm(metadata$water_sample_ph_bot~metadata$max_lake_depth))
plot(metadata$water_sample_do_bot, metadata$max_lake_depth)
summary(lm(metadata$water_sample_do_bot~metadata$max_lake_depth))
plot(metadata$water_sample_t_bot, metadata$max_lake_depth)
summary(lm(metadata$water_sample_t_bot~metadata$max_lake_depth))


cor()


```

## 18. SedaDNA sites

```{r}
library(ggplot2)
library(ggmap)
library(MetBrewer)

df<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/Collaborations/sedaDNA cyberinfrastructure workshop/V1_20220718_sedimentary_aeDNA_sites.csv", header=T)

mb<-df[df$MolecularMethod=="MB",]
mb<-mb[mb$TargetGroup=="Microorganisms",]
unique(mb$TargetTaxa)
mb<-mb[mb$TargetTaxa%in%c("Archaea","Prokaryotes","Bacteria"),]
mapview::mapview(mb$Latitude..DD.,mb$Longitude..DD.)

write.csv(mb,"Num_sedaDNA_sites.csv")

df<-mb

mapWorld <- borders("world", colour="gray50", fill="gray50") 

colnames(df)[4]<-"lat"
colnames(df)[5]<-"lon"
df$lat<-as.numeric(df$lat)
df$lon<-as.numeric(df$lon)

cols<-met.brewer(name="Egypt", n=4, type="discrete")
cols<-c("#000000",cols[1],cols[4],cols[2],cols[3])
cols<-paste(cols,"95", sep="")
point_types<-c(21,22,24,23,28)

mp <- NULL
mp <- ggplot() +   mapWorld
mp <- mp + geom_point(aes(x=df$lon, y=df$lat, fill=df$TargetGroup, shape=df$SampleType_factor), size=3) +   labs(x = "Latitude", y = "Longitude", fill= "Target group", shape = "Sample type")  + scale_shape_manual(values=point_types) +scale_fill_manual(values=cols)
mp <- mp + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line=element_blank(),axis.text.x=element_blank(),panel.border=element_blank(), axis.text.y=element_blank(),axis.ticks=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank())  + theme(legend.position="none")+ annotate("text", y=95, x=-145, label= "Microbe metabarcoding", size=6) 
mp 
```

## 19. BNTI vs env_dist

```{r}
require(mgcv)
comps<-read.csv("pairwise_comparisons_14Sept2022.csv", header=T, row.names=1) # load this as a backup file and pull from there

plot(comps$env_dist ,comps$BNTI)

Data<-data.frame(x=comps$env_dist,y=comps$BNTI)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=5), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y,)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)

table(bnti)

```

## 20. phylogenetic signal

```{r}
#load data
require(ape)
require(vegan)
require(picante)
```

Subset for the most abundant ESVs (10,888 ESVs) for computational tractability

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)

my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
```

issues is that i'm doing a weighted mean for some samples where the OTU isn't present...

from Vass's email: Find optimal niche for conductivity with abundance-weighted-mean conductivity value for each OTU Make data table of conductivity in first column, and OTU relative abundance in rest of columns Read in environmental data and separate average conductivity column for each site Read in envi data

From Tripathi et al. 2018 "To calculate the abundance-weighted mean for a given OTU, we first found all samples in which that OTU was present. We then found the abundance-weighted mean pH of all those samples. To do so, in the calculation of mean pH we weighted each pH value by the abundance of the OTU in the associated sample. This procedure was repeated for each OTU, and the resulting value was used as a rough estimate of that OTU's pH optimum. Then, between-OTU differences in pH optima were calculated as Euclidean distances. Finally, we used Mantel correlograms to measure the correlation coefficients between differences in pH optima and phylogenetic distances [3, 9], and significance of these correlations was assessed using 999 permutations with Bonferroni correction."

```{r}
#using just the 10888 most abundant ESVs
#match up with metadata
OTU<-as.data.frame(otu_table(my_subset))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),] 
#change OTUs that are 0 and turn them into NAs
OTU[OTU == 0] <- NA

metadata<-metadata[order(rownames(metadata)),] 
table(rownames(OTU)==rownames(metadata)) #all true

#bottom water temp (missing one value)
#metadata[metadata$lake_id==40,]$water_sample_pH_bot<-20.1

#Select variable
env.var <- metadata["water_sample_ph_bot"]
names(env.var)<-"environmental_variable"
table(is.na(env.var)) #all false   

#Combine env.var and OTU table
env.varOTU <- merge(env.var,OTU,by="row.names")
rownames(env.varOTU)<-env.varOTU$Row.names
env.varOTU$Row.names<-NULL

#make sure the env.var is column 1 and OTU is column 2 on..
head(names(env.varOTU))

#Find optimal temperature by applying weighted mean function to each OTU column
#subset by only the samples that HAD that otu present, and calculated the abundance weighted mean env.var for that specific esv
NiOp<-data.frame(OTUID=NULL,env.var.wt.mn=NULL)
i=2
for(i in 2:ncol(env.varOTU)){
       sub<-env.varOTU[,c(1,i)] 
       sub<-sub[complete.cases(sub),]
       NiOp<-rbind(NiOp,data.frame(OTUID=names(sub)[2],env.var.wt.mn=weighted.mean(x=sub[,1],w=sub[,2])))
}
rownames(NiOp)<-NiOp$OTUID
NiOp$OTUID<-NULL

#did not do yet
#dist_NiOp<-dist(abundwtmn)

########################################################################
#Make phylogenetic distance matrix
#Load phylogenetic tree
phylo <- read.tree("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/sed_norm_fasta_29Aug2022_muscled.nwk")
#subset by the abundant 10888 ESVs
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), phylo)
phylo <- physeq@phy_tree
rm(physeq)

# make sure these match
table(phylo$tip.label%in%unique_alldepths) #all TRUE
table(unique_alldepths%in%phylo$tip.label) #all TRUE

# making new matrix that will have standardized species scores
#JVE - did this for an OTU table with zeros instead of NAs since it wouldn't scale right below with NAs
OTU_0<-as.data.frame(otu_table(my_subset))
OTU_0<-t(OTU_0)
OTU_0<-OTU_0[order(rownames(OTU_0)),] 
otu.scores.std=(OTU_0)
otu.scores.std = decostand(otu.scores.std, "standardize", MARGIN = 2) #standardize by columns (OTUs) where  scale x to zero mean and unit variance

match.phylo.otu = match.phylo.comm(phylo, otu.scores.std) ; #or .data if transposed

#this merges the abundance weighted mean value (NiOp) back in with the OTU table and then scales the whole thing
dim(NiOp)
# JVE Jan 4 - I'm not exactly sure what the comm values are in this object
commm=t(match.phylo.otu$comm)
dim(commm)
table(is.na(NiOp$env.var.wt.mn)) #all false
NiOp=merge(NiOp, commm, by="row.names")
rownames(NiOp) <- NiOp[,1]
NiOp$Row.names<-NULL

#JV - this merges the abundance weighted mean value (NiOp) back in with the OTU table and then scales the whole thing and then promptly removes it... I thought it was the lines above, but below is what is the in the code

NiOp=merge(na.omit(NiOp), commm, by="row.names")
rownames(NiOp) <- NiOp[,1]
NiOp_s=NiOp[,-1]
NiOp_s=NiOp_s[,1, drop=FALSE]
NiOp=NiOp_s


NiOp.scal<- scale(NiOp)

# check that we get mean of 0 and sd of 1
apply(NiOp.scal, 2, mean)  # faster version of apply(scaled.dat, 2, mean)
apply(NiOp.scal, 2, sd)

#OTU niche distance matrix
spp.dist = as.matrix(dist(NiOp.scal))


# making sure the names on the phylogeny are ordered the same as the names in otu table
# the match phylo does subset the tree to the 10888 esvs, but this could be more streamlined and just subset the tree first
match.phylo.otu[[1]]; # tree
dim(match.phylo.otu[[2]]); match.phylo.otu[[2]][1:3, 1:6] ;

phylo.dist = cophenetic(match.phylo.otu[[1]]);   #Computes the cophenetic distances for a hierarchical clustering (intergroup dissimilarity)
phylo.dist = phylo.dist/max(phylo.dist) ; #Makes intergroup dissimilarities relative to the maximum? Gives a distance matrix for between-OTU phylogenetic distances

#Sort columns to match the niche distance matrix
phylo.dist<-phylo.dist[,order(colnames(phylo.dist))]
phylo.dist<-phylo.dist[order(rownames(phylo.dist)),]

#Check that niche distance matrix and phylogenetic distance matrix have OTUs in same order
table(colnames(spp.dist)==colnames(phylo.dist)) #all true
table(rownames(spp.dist)==rownames(phylo.dist)) #all true
```

Mantel test and plotting

```{r}
phylo.sig.correlog = mantel.correlog(spp.dist,
                                     phylo.dist,
                                     n.class=50,
                                     mult="bonferroni");

phylo.sig.correlog = mantel.correlog(spp.dist,
                                     phylo.dist,
                                     nperm = 1,
                                     mult="bonferroni");


plot(phylo.sig.correlog)

phylo.sig<-phylo.sig.correlog$mantel.res

env.plot=plot(phylo.sig[,c(1,3)],
                   xlab="Phylogenetic distance class", ylab="Mantel correlation") +lines(phylo.sig[,c(1,3)]) + points(phylo.sig[,c(1,3)], pch=21, bg="white", col="black", pty=4)+
        points(phylo.sig[,c(1,3)][phylo.sig[,5] < 0.05,], pch=21, bg="black", col="black", pty=4) + abline(h=0, lty=2, col="red") +
        title("Bottom water pH")

```

### a. Pearman code

```{r}
require(phyloseq)
require(ape)
require(dplyr)
library(caret)
library(picante)

```

Subset for the most abundant ESVs (10,888 ESVs) for computational tractability

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)

my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
```

could double check this if I wanted - but all looks fine. Maybe want to do with ESVs with 10+ reads

```{r}
#Load phylogenetic tree
phylo <- read.tree("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/sed_norm_fasta_29Aug2022_muscled.nwk")
#subset by the abundant 10888 ESVs
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), phylo)
tree <- physeq@phy_tree
# make sure only the 10888 ESVs are in the tree
table(tree$tip.label%in%unique_alldepths) #all TRUE
table(unique_alldepths%in%tree$tip.label) #all TRUE
rm(physeq)

#using just the 10888 most abundant ESVs
#match up with metadata
OTU<-as.data.frame(otu_table(my_subset))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),] #site by species

#order metadata
metadata<-metadata[order(rownames(metadata)),] 
#subset metadata
metadata.a <- metadata %>% select(depth, elevation_meters,max_lake_depth, water_sample_ph_bot, water_sample_do_bot, water_sample_t_bot,pH,n_perc, c_perc)

#make OTU and metadata match
table(rownames(OTU)==rownames(metadata)) #all true


#Predict the missing metadata (JVE: not very familiar with this part of John Pearman's code to predict missing metadata..)
set.seed(496)

metadata.1 <-
  preProcess(metadata.a,
             method = c("bagImpute"))

metadata.2  <- 
  predict(metadata.1 , metadata.a)


OTU_log <- log(OTU + 1) #JVE: unsure why log + 1

OTU_log.p <- match.phylo.comm(tree, OTU_log)$comm
tree_log.p <- match.phylo.comm(tree, OTU_log)$phy

phydist <- cophenetic(tree_log.p)

#phylogenetic distance
phydist_hel <- decostand(phydist, method="hellinger")

#from site x species to species x site
table <- t(OTU_log)

env.var.names<-colnames(metadata.2)

#loop through each environmental variable and create a list of each "final table"
all_env.vars_tab<-list()
q=1
for(q in 1:length(env.var.names)){

table_env<-rbind(metadata.2[,q],table)
rownames(table_env)[1]<-"env.var"

table_w <- t(table_env) #first column environmental variable 

headTable <- colnames(table_w) 
x <- headTable[1] # The first column is a constant env.var and is being compared to OTU


# Create the final_table empty

m <- matrix(0, nrow = 0, ncol = 3) 
final_table <- data.frame(m)
colnames(final_table)  <- c('ymax','d50', 'xmax') 

# the for loop, to get all otus 1 by 1 compared to env.var

for(i in seq(2,length(headTable),1)) #start at 2 because the 1 is the Eh_mv
{
  y <- headTable[i] # get otu name
  tablei <-  table_w[,c(x,y)] # extract 2 columns to create a new table
  
  head  <- colnames(tablei)
  
  ymax = which.max(tablei[,2]) # get the max relative abundance of the OTU
  
  MaxRow <- tablei[ymax,] # create a table with the line of the max OTU value
 
  tableiInf <- subset(tablei,tablei[,1]<= MaxRow[1]) #create two tables greater and lower than the x coord
  tableiSup <- subset(tablei,tablei[,1]>= MaxRow[1]) #of the max
  
  tableiInfno0 <- subset(tableiInf,tableiInf[,2]>(MaxRow[2]/20)) #remove the observations where the relative OTU abundance is 0
  tableiSupno0 <- subset(tableiSup,tableiSup[,2]>(MaxRow[2]/20)) #remove the observations where the relative OTU abundance is 0
  
  
  #Determine the lowest and highest Eh value where you find the OTU
  
  xInf <- min(tableiInfno0[,1])
  xSup <- max(tableiSupno0[,1])
  
  # calculate the d50
  
  d50  <- 0.68*abs(xSup-xInf)/2
  #JVE - why is this 0.68?
  
  # create the final table for the otu in the loop
  
  tablei_fin  <- matrix(c(MaxRow[2],d50,MaxRow[1]),ncol=3)
  rownames(tablei_fin)  <- c(head[2])
  colnames(tablei_fin)  <- c('ymax','d50', 'xmax')
  
  # add to the final_table the values for each otu 
  final_table <- rbind(final_table,tablei_fin)
  
  
}
#write the table and put into a list
write.table(final_table, paste("niche_", env.var.names[q], "_9Jan2023.txt", sep=""), sep="\t")
all_env.vars_tab[[q]]<-final_table
}

#remove all but the two objects needed for the mantel correlogs
rm(list=ls()[! ls() %in% c("all_env.vars_tab","phydist_hel")])
save.image("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")

```

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal 

```

#### i) run on Beartooth

phylogenetic_signal_1.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[1]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_1<-mantel.correlog(env_dist,phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_1.RData")

```

phylogenetic_signal_2.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[2]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_2<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_2.RData")

```

phylogenetic_signal_3.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[3]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_3<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_3.RData")

```

phylogenetic_signal_4.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[4]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_4<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_4.RData")

```

phylogenetic_signal_5.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[5]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_5<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_5.RData")

```

phylogenetic_signal_6.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[6]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_6<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_6.RData")

```

phylogenetic_signal_7.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[7]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_7<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_7.RData")

```

phylogenetic_signal_8.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[8]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_8<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_8.RData")

```

phylogenetic_signal_9.R

```{r}
require(vegan)
load("WyLakeMicrobes_env_9Jan2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[9]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_9<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_9.RData")

```

run_phylogenetic_signal.sh

```{bash}
#!/bin/bash
#SBATCH --job-name phylo_signal
#SBATCH --mem=20GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=phylo_signal_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal
date

srun Rscript phylogenetic_signal_1.R
echo "srun Rscript phylogenetic_signal_1.R"

srun Rscript phylogenetic_signal_2.R
echo "srun Rscript phylogenetic_signal_2.R"

srun Rscript phylogenetic_signal_3.R
echo "srun Rscript phylogenetic_signal_3.R"

srun Rscript phylogenetic_signal_4.R
echo "srun Rscript phylogenetic_signal_4.R"

srun Rscript phylogenetic_signal_5.R
echo "srun Rscript phylogenetic_signal_5.R"

srun Rscript phylogenetic_signal_6.R
echo "srun Rscript phylogenetic_signal_6.R"

srun Rscript phylogenetic_signal_7.R
echo "srun Rscript phylogenetic_signal_7.R"

srun Rscript phylogenetic_signal_8.R
echo "srun Rscript phylogenetic_signal_8.R"

srun Rscript phylogenetic_signal_9.R
echo "srun Rscript phylogenetic_signal_9.R"


echo "finished correlogs"
date

```

copy files back to computer

```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_1.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_2.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_3.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_4.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_5.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_6.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_7.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_8.RData /Users/jordanscheibe/Desktop

rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal/WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_9.RData /Users/jordanscheibe/Desktop


```

#### ii) Plot mantel correlograms

```{r}
env.var.names<-c("depth",               "elevation_meters"   , "max_lake_depth" ,    
"water_sample_ph_bot", "water_sample_do_bot", "water_sample_t_bot" ,
"pH"                ,  "n_perc"            ,  "c_perc"    )  
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_1.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_2.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_3.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_4.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_5.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_6.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_7.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_8.RData")
load("WyLakeMicrobes_env_29Jan2023_phylogeneticsignal_mantelcorrresult_9.RData")

figs<-list()
figs[[1]]<-mc_result_1
figs[[2]]<-mc_result_2
figs[[3]]<-mc_result_3
figs[[4]]<-mc_result_4
figs[[5]]<-mc_result_5
figs[[6]]<-mc_result_6
figs[[7]]<-mc_result_7
figs[[8]]<-mc_result_8
figs[[9]]<-mc_result_9

i=1
pdf("Figures/Phylogenetic_signal_mantel_corr_24Jan2023.pdf", height = 8, width=10)
par(mfrow=c(3,3), mar=c(4,4,2,2))
for(i in 1:9){
plot(figs[[i]])
text(0.0025,max(figs[[i]]$mantel.res[,3], na.rm=T)*0.9, env.var.names[i])
}
dev.off()




```

## 21. iCAMP - Rerun Comm. Assembly

### a) ESVs with 10+ reads

#### i) subset ESVS with 10 reads

```{r}
require(phangorn)
require(msa)
require(Biostrings)
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

# 1. subset OTU table by ESVs with 10+ reads
OTU<-as.data.frame(otu_table(ps))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),]

table(colSums(OTU)<10)
#FALSE  TRUE 
#28316 63641 
mean(colSums(OTU)) #53 reads on average

lessthan10<-which(colSums(OTU)<10)
OTU_10plusreads<-OTU[,-lessthan10]
dim(OTU_10plusreads)
table(colSums(OTU_10plusreads)<10)# all FALSE
```

#### ii) subset fasta file

in local R

```{r}
#2. subset fasta file by the ESVs with 10+ reads (normalized reads)
seqs<-Biostrings::readDNAStringSet("zotus_nonchimeric.fa") #downloaded straight from the calder file folder
seqs@ranges@NAMES<-stringr::str_split(seqs@ranges@NAMES,pattern = ";", 3, simplify = T)[,1]

seqs_sub<-seqs[colnames(OTU_10plusreads)]
table(seqs_sub@ranges@NAMES %in% colnames(OTU_10plusreads))
table(colnames(OTU_10plusreads)%in%seqs_sub@ranges@NAMES)
rm(list=ls()[! ls() %in% c("OTU_10plusreads", "seqs_sub" )])
#save.image("WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData")

```

MSA is taking forever so try with old way (clustalO)

```{r}
load("WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData") #this is the subset file that has only ESVs with 10+ reads
require(Biostrings)
writeXStringSet(seqs_sub,filepath = "ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta", format = "fasta")
```

```{bash}
rsync /Users/jordanscheibe/Desktop/ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo
```

#### iii) make tree clustalo and fasttree

Ran this is command line with salloc, pasted output below for records

```{bash}
module load arcc/1.0 miniconda3/4.12.0
conda activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo

clustalo -i ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta -o tempfile_16Jan2023_muscled.fa -v --threads=6

FastTree -nt tempfile_16Jan2023_muscled.fa > ESVs_with10ormorereads_output_16Jan2023_muscled.nwk

```

(shotgun_env) [[jvonegge\@blog1](mailto:jvonegge@blog1){.email} maketree_clustalo]\$ salloc --mem=120GB --nodes=1 --cpus-per-task=6 --account=microbiome --time=1:00:00 salloc: Pending job allocation 6104675 salloc: job 6104675 queued and waiting for resources salloc: job 6104675 has been allocated resources salloc: Granted job allocation 6104675 salloc: Waiting for resource configuration salloc: Nodes m221 are ready for job [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ module load arcc/1.0 miniconda3/4.12.0 [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ conda activate shotgun_env cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo (shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/tree_13Jan2023/maketree_clustalo (shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ ls ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta (shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ clustalo -i ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta -o tempfile_16Jan2023_muscled.fa -v --threads=6 Using 6 threads Read 28316 sequences (type: DNA) from ESVs_with10ormorereads_forBeartooth_16Jan2023.fasta Using 218 seeds (chosen with constant stride from length sorted seqs) for mBed (from a total of 28316 sequences) Calculating pairwise ktuple-distances... Ktuple-distance calculation progress done. CPU time: 494.62u 0.12s 00:08:14.74 Elapsed: 00:01:26 mBed created 448 cluster/s (with a minimum of 1 and a soft maximum of 100 sequences each) Distance calculation within sub-clusters done. CPU time: 94.02u 0.03s 00:01:34.05 Elapsed: 00:00:17 Guide-tree computation (mBed) done. Progressive alignment progress done. CPU time: 348.22u 1.90s 00:05:50.12 Elapsed: 00:04:25 Alignment written to tempfile_16Jan2023_muscled.fa

(shotgun_env) [[jvonegge\@m221](mailto:jvonegge@m221){.email} maketree_clustalo]\$ FastTree -nt tempfile_16Jan2023_muscled.fa \> ESVs_with10ormorereads_output_16Jan2023_muscled.nwk ML-NNI round 11: LogLk = -911140.600 NNIs 337 max delta 10.59 Time 656.25 (final) Optimize all lengths: LogLk = -911087.046 Time 669.69\
Total time: 748.59 seconds Unique: 28316/28316 Bad splits: 46/28313 Worst delta-LogLk 7.368

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/BNTI/iCAMP/qpen/10plus_ESVs


# Here I change name from what was used to make the phylogenetic tree - to make it more CLEAR, changed this also in my local computer files
mv WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_Beartooth_13Jan2023.RData WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_ESVswith10plusreads_Beartooth_13Jan2023.RData

```

#### v) qpen ESVs with 10+ reads

qpen_function.R

```{r}
require(phyloseq)
require(iCAMP)
require(ape)
load("../WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_ESVswith10plusreads_Beartooth_13Jan2023.RData")
comm<-OTU_10plusreads
tree<-read.tree("../ESVs_with10ormorereads_output_16Jan2023_muscled.nwk")
table(colnames(OTU_10plusreads)%in%tree[["tip.label"]]) #all TRUE
table(tree[["tip.label"]]%in%colnames(OTU_10plusreads)) #all TRUE


wd0="/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen"
nworker=6 # parallel computing thread number
rand.time=1000 # usually use 1000 for real data.
  

  # for a big dataset, pdist.big may be used
  save.wd="/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen/pdbig.qpen"
  print(save.wd)
  # please change to the folder you want to save the pd.big output.
  
  pd.big=pdist.big(tree = tree, wd=save.wd, nworker = nworker)
  qp2=qpen(comm=comm, pd=pd.big$pd.file, pd.big.wd=pd.big$pd.wd,
           pd.big.spname=pd.big$tip.label, tree=tree,
           rand.time=rand.time, nworker=nworker)
  setwd(wd0)
save.image("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
```

run_qpen_function.sh

```{bash}
#!/bin/bash
#SBATCH --job-name qpen_10plus
#SBATCH --mem=120GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=6
#SBATCH --account=microbiome
#SBATCH --output=qpen_10plus_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen

srun Rscript qpen_function.R
echo "srun Rscript qpen_function.R"

echo "finished qpen iCAMP - JVE"
date
```

```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/qpen/WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData /Users/jordanscheibe/Desktop

```

#### vi) caluclate dniche

```{bash}
rsync /Users/jordanscheibe/Desktop/WyLakeMicrobes_phyloseq_env_29Aug2022.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/dniche

```

doing this in the bash terminal:

```{bash}
module load gcc/12.2.0 r/4.2.2
```

dniche_function.R

```{r}
require(dplyr)
require(caret)
require(iCAMP)
require(ape)
require(phyloseq)
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("../WyLakeMicrobes_phyloseq_env_forPhylogeneticTree_ESVswith10plusreads_Beartooth_13Jan2023.RData")
tree<-read.tree("../ESVs_with10ormorereads_output_16Jan2023_muscled.nwk")
table(colnames(OTU_10plusreads)%in%tree[["tip.label"]]) #all TRUE
table(tree[["tip.label"]]%in%colnames(OTU_10plusreads)) #all TRUE


#order metadata
metadata<-metadata[order(rownames(metadata)),] 
#subset metadata
metadata.a <- metadata %>% select(depth, elevation_meters,max_lake_depth, water_sample_ph_bot, water_sample_do_bot, water_sample_t_bot,pH,n_perc, c_perc)

#make OTU and metadata match
table(rownames(metadata.a)==rownames(OTU_10plusreads)) #all true

#Predict the missing metadata (JVE: not very familiar with this part of John Pearman's code to predict missing metadata..)
set.seed(496)

metadata.1 <-
  preProcess(metadata.a,
             method = c("bagImpute"))

metadata.2  <- 
  predict(metadata.1 , metadata.a)

table(rownames(metadata.2)==rownames(OTU_10plusreads)) #all true

table(is.na(metadata.a))

# FALSE  TRUE 
#  3685   617 
table(is.na(metadata.2))
# FALSE 
#  4302 

comm<-OTU_10plusreads
env<-metadata.2


 wd0=getwd()
  save.wd=paste0(tempdir(),"/pdbig.ps.bin")
  # please change to the folder you want to save the big niche difference matrix.
  
  nworker=2 # parallel computing thread number
  pd.big=pdist.big(tree = tree, wd=save.wd, nworker = nworker)
    
  niche.dif=dniche(env = env, comm = comm,
                   method = "niche.value", nworker = nworker,
                   out.dist=FALSE,bigmemo=TRUE,nd.wd = save.wd,
                   nd.spname.file="nd.names.csv")
  
  ds = 0.2 # setting can be changed to explore the best choice
  bin.size.limit = 12 # setting can be changed to explore the best choice.

  
  phylobin=taxa.binphy.big(tree = tree, pd.desc = pd.big$pd.file,
                           pd.spname = pd.big$tip.label, pd.wd = pd.big$pd.wd,
                           ds = ds, bin.size.limit = bin.size.limit,
                           nworker = nworker)
  sp.bin=phylobin$sp.bin[,3,drop=FALSE]
  
  sp.ra=colMeans(comm/rowSums(comm))
  abcut=3
  # by abcut, you may remove some species,
  # if they are too rare to perform reliable correlation test.
  
  
  commc=comm[,colSums(comm)>=abcut,drop=FALSE]
  dim(commc)
  spname.use=colnames(commc)
  
  binps=ps.bin(sp.bin = sp.bin,sp.ra = sp.ra,spname.use = spname.use,
               pd.desc = pd.big$pd.file, pd.spname = pd.big$tip.label,
               pd.wd = pd.big$pd.wd, nd.list = niche.dif$nd,
               nd.spname = niche.dif$names, ndbig.wd = niche.dif$nd.wd,
               cor.method = "pearson",r.cut = 0.1, p.cut = 0.05, min.spn = 5)
  setwd(wd0)

 save.image("dniche_psbin_ESV10plusreads_26Jan2023.Rdata")
```

run_dniche_function.sh

```{bash}
#!/bin/bash
#SBATCH --job-name dniche_10plus
#SBATCH --mem=50GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=2
#SBATCH --account=microbiome
#SBATCH --output=dniche_10plus_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP_package/ESVswith10plusreads/dniche

srun Rscript dniche_function.R
echo "srun Rscript dniche_function.R"

echo "finished dniche iCAMP - JVE"
date
```

### b) qpen results

#### 1) qpen output - ESVs with 10plus reads

###### a) start here

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(metagMisc)

#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","process")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"process[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)%in%metadata$samp_names) #all true
table(colnames(dfr)%in%metadata$samp_names) # all true

#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

table(colnames(dfr)==metadata$samp_names) # all true
table(colnames(dfr)==rownames(metadata_sub)) # all true 

daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))
table(colnames(daisy.mat)==colnames(dfr)) # all true
table(rownames(daisy.mat)==rownames(dfr)) # all true

daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

process<-melt(as.matrix(dfr), na.rm=T)

table(process$Var1==env_dist$Var1) # all true
table(process$Var2==env_dist$Var2) # all true 
process$env_dist<-env_dist$value

pairwise<-process #overwrite original file

names(pairwise)[1]<-"s1_samp_names"
pairwise$s1_samp_names<-as.character(pairwise$s1_samp_names)
names(pairwise)[2]<-"s2_samp_names"
pairwise$s2_samp_names<-as.character(pairwise$s2_samp_names)
names(pairwise)[3]<-"process"

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

pairwise<-merge(pairwise, metadata_s1, by="s1_samp_names")
pairwise<-merge(pairwise, metadata_s2, by="s2_samp_names")

#remove metadata notes
pairwise$s1_notes<-NULL
pairwise$s2_notes<-NULL
pairwise$s1_Notes_sed_water_wt<-NULL
pairwise$s2_Notes_sed_water_wt<-NULL
pairwise$s1_Notes_carbon_nitrogen<-NULL
pairwise$s2_Notes_carbon_nitrogen<-NULL
pairwise$s1_Notes._lake_sed_pH<-NULL
pairwise$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
pairwise$abs_cm<-rep(NA, nrow(pairwise))
i=1
for(i in 1:nrow(pairwise)){
  ifelse(pairwise$s1_lake_drive[i]==pairwise$s2_lake_drive[i],pairwise$abs_cm[i]<-abs(pairwise$s2_depth[i]-pairwise$s1_depth[i]),NA)
}

# calculate geographic distance

require(geosphere)
for(i in 1:nrow(pairwise)){
        pairwise$geo_dist[i]<-as.numeric(distm(data.frame(X = pairwise$s1_longitude[i], Y = pairwise$s1_latitude[i]),data.frame(X = pairwise$s2_longitude[i], Y = pairwise$s2_latitude[i])))
}

pairwise$geo_dist_km<-pairwise$geo_dist/1000

write.csv(pairwise, "qpen_10plusESVs_allcomparisons_7Feb2023.csv")
```

###### b) environmental distance within each zone

```{r}
require(stringr)
require(Hmisc)
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)

comps<-comps_backup

comps$d_val<-comps$env_dist

#make breaks for zone A
comps_tmp<-comps[comps$s1_zone=="A" & comps$s2_zone=="A",]

comps_tmp<-comps_tmp[comps_tmp$d_val!=0,]
comps_tmp$d_val_group_breaks<-cut2(comps_tmp$d_val, g=10)
levels(comps_tmp$d_val_group_breaks)
# [1] "[0.0162,0.117)" "[0.1167,0.163)" "[0.1632,0.210)" "[0.2097,0.240)" "[0.2399,0.276)"
#  [6] "[0.2759,0.323)" "[0.3234,0.384)" "[0.3835,0.454)" "[0.4543,0.540)" "[0.5399,0.722]"
breaks<-c(as.numeric(gsub("\\]","" ,gsub("\\[","",unlist(str_split(as.character(levels(comps_tmp$d_val_group_breaks)), pattern=",")))[c(seq(1, 20, 2),20)])))

#make sure first value is below the minimum env_distance measure that isn't 0
min(comps_backup[comps_backup$env_dist!=0,]$env_dist)
#[1] 0.01621537

# make a bottom break (0.016 - just below the lowest env_dist) that is above zero but below the first break
breaks<-c(0.016,breaks[2:11]) 

full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)

zone<-c("A","B","C")
j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

#breaks_labels<- levels(comps$d_val_group_breaks)
breaks_labels<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="env"))
  }
  }

full_output<-rbind(full_output,tally)
}
#check to make sure they add to one
range(full_output$n)
#[1]  171 2020 #11 jan 2023
mean(full_output$n)
#947.6364 #11 jan 2023


write.csv(full_output, "qpen_10plusESVs_envdist_8Feb2023.csv")

```

###### c) geographic distance

```{r}
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)
comps<-comps_backup


comps[comps$s1_lake_id==comps$s2_lake_id & comps$s1_lake_drive!=comps$s2_lake_drive,]$geo_dist_km<-0.001 #add one meter for cores in the same lake
comps$d_val<-comps$geo_dist_km
breaks<-c(seq(0,15,5), seq(100, 300, 200), seq(400,500,100))  ##nothing from  43.30254  174.6661
breaks<-c(0,0.1, breaks[2:8])
full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)


zone<-c("A","B","C")
j=1
for( j in 1:3) {
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

breaks_labels<- c("[0,0]",levels(cut(comps$d_val, breaks=breaks)))

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="geo"))
  }
  }
full_output<-rbind(full_output,tally)
#count the number of comparisons in each zone
}

range(full_output$n)
#[1]  75 2604 # 11 Jan 2023
mean(full_output$n)
#1158.222
print(breaks_labels)

write.csv(full_output, "qpen_10plusESVs_geodist_8Feb2023.csv")
```

###### d) combine the two dataframes

```{r}
geo_dist_tally<-read.csv("qpen_10plusESVs_geodist_8Feb2023.csv", header=T, row.names=1)
env_dist_tally<-read.csv("qpen_10plusESVs_envdist_8Feb2023.csv", header=T, row.names=1)
full_output<-rbind(geo_dist_tally,env_dist_tally)

full_output[full_output$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
full_output[full_output$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
full_output[full_output$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
full_output[full_output$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
full_output[full_output$process=="Undominated",]$process<-"C_Drift"


#add in color
full_output$zone_col<-rep('#018571',nrow(full_output))
full_output[full_output$zone=="B",]$zone_col<-"#C4AD79"
full_output[full_output$zone=="C",]$zone_col<-'#a6611a'
        


#add in xlab as dist column
full_output[full_output$dist=="env",]$dist<-'Environmental dissimilarity'
full_output[full_output$dist=="geo",]$dist<-'Distance (km)'

write.csv(full_output,"qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv")

#if count needed
count<-unique(full_output[,c("d_val_group","zone","n","dist")])
write.csv(count, "qpen_10plusESVs_count_summary_8Feb2023.csv")
```

###### e) plot env and geo dist

option to plot all five processes, or just the stochastic processes

```{r}
require(patchwork)
require(ggplot2)

full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
full_output$proportion<-full_output$percent
full_output$percent<-full_output$percent*100

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
#new_colors<-c("#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[1]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")

#full_output<-full_output[full_output$process%in%sort(unique(full_output$process))[3:5],]

ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
for(i in 1:2){
        sub2<-sub[sub$dist==unique(sub$dist)[i],]       
ifelse(j==3, 
       
       
       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) +
                theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       
       , 
       
       
              ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15))+ #change font size of legend title
        theme(axis.title.x=element_blank(),
        axis.text.x=element_blank()) +
        theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       )
        }}


#pdf("Figures/qpen/10plusESVs/Fig6_community_assembly_NOSELECTION_14Feb2023.pdf",height=7,width=7)
pdf("Figures/qpen/10plusESVs/Fig6_community_assembly_14Feb2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
  plot_layout(ncol = 2)
dev.off()


```

###### f) plot sediment distance

```{r}
require(Hmisc)
qpn<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)

#subset by samples within the same core
downcore<-qpn[is.na(qpn$abs_cm)==F & qpn$abs_cm<27 ,]


downcore$d_val<-abs(downcore$abs_cm)
downcore$d_val_group_breaks<-cut2(downcore$d_val, g=10)
downcore$d_val_group<-as.numeric(cut2(downcore$d_val, g=10))
levels(downcore$d_val_group_breaks)
break_labels<- c("0 to <2.5" ,"2.5 to <5" ,"5 to <7", "7 to <9" ,"9 to <11" ,"11 to <15"
,"15 to <18" ,"18 to <22" ,"22 to <26")
#these are put into 9 groups here instead of the requested 10, must be to make it even.

tmp<-downcore
tally<-data.frame(process=NULL,percent=NULL, d_val_group=NULL,n=NULL)
groups<-sort(unique(downcore$d_val_group))
i=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  processes<-as.data.frame(table(tmp2$process)/nrow(tmp2))
  colnames(processes)[1]<-"process"
  colnames(processes)[2]<-"percent"
  processes$process<-as.character(processes$process)
  processes$d_val_group<-rep(groups[i],nrow(processes))
  processes$n<-rep(nrow(tmp2),nrow(processes))
  tally<-rbind(tally,processes)
}
  
#write.csv(tally,"qpen_sedimentdistance_13Feb2023.csv")

tally$proportion<-tally$percent
tally$percent<-tally$percent*100

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

        ggplt <- ggplot(tally,            
                        aes(x = d_val_group,
                            y = percent,
                            color= process)) +  geom_line(linewidth=2) +geom_point(size=3) + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
                                                                                                                             labels=break_labels) + xlab(label = "Sediment distance (cm)")+ ylab(label = "Proportion (%)") + geom_line(linewidth=2) +   labs(color="Assembly process")    + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) + theme(axis.text.x = element_text(angle = 45,hjust=1))+ guides(color = guide_legend(override.aes = list(linewidth = 6))) + theme(legend.position="none")+
                theme(axis.text=element_text(color="black"))+
                theme(axis.text.x = element_text(angle = 60,hjust=1))+
                theme(text=element_text(size=14), #change font size of all text
                      axis.text=element_text(size=14), #change font size of axis text
                      axis.title=element_text(size=15), #change font size of axis titles
                      plot.title=element_text(size=14), #change font size of plot title
                      legend.text=element_text(size=14), #change font size of legend text
                      legend.title=element_text(size=15),
                      panel.grid.minor = element_blank(), 
                      panel.grid.major.x = element_blank()) #change font size of legend title 

pdf("Figures/qpen/10plusESVs/Fig6A_Continuous_EAP_sediment_distance_8Feb2023.pdf", height=2.9, width=3.3)
ggplt
dev.off()

unique(tally$n)
#[1] 372 352 302 283 239 403 163 236 190

```

#### 2) GAMS without 0

doesn't include comparisons within the same lake 8Feb2023 - the labeling for geographic distance is off (double check if going to put in supplementary figures)

```{r}
require(mgcv)
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")

full_output<-full_output[full_output$d_val_group!=0,] #GAMs without 0 (without comparisons from the same lake)
tmp1<-full_output[full_output$dist=="Environmental dissimilarity",]
tmp2<-full_output[full_output$dist=="Distance (km)",]
tmp2<-tmp2[tmp2$d_val_group!=1,]
full_output<-rbind(tmp2,tmp1)


model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_qpen_",unique(full_output$dist)[d],"_GAM_8Feb2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels[[d]])),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
        
}}
dev.off()
}
write.csv(model_results, "qPEN_CommunityAssembly_GAMS_without0_8Feb2023.csv")

```

sediment distance (with zero)

```{r}

tally<-read.csv("qpen_sedimentdistance_13Feb2023.csv", header=T,row.names=1)

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
break_labels<- c("0 to <2.5" ,"2.5 to <5" ,"5 to <7", "7 to <9" ,"9 to <11" ,"11 to <15"
,"15 to <18" ,"18 to <22" ,"22 to <26")

model_results<-data.frame(process=NULL, rsq=NULL, p=NULL)


require(mgcv)
#Plot the sediment depth GAM for each process
pdf("Figures/qpen/10plusESVs/Fig7_seddepth_GAM_14Feb2023.pdf", height=3, width=10)
par(mfrow=c(1,5),mar=c(6,4,2,1))
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="black", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels)),1), las=2, labels = break_labels)
title(xlab=c("","","Sediment distance (cm)","","")[i], cex.lab=2, line=4)
title(ylab=c("Proportion","","","","")[i], cex.lab=2)

Data<-data.frame(x=sub$d_val_group, y=sub$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=new_colors[i], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(new_colors[i], alpha.f = 0.40), border = NA)
        gam_res<-summary(Gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("R-sq: ",round(gam_res$r.sq, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
        model_results<-rbind(model_results,data.frame(process=sort(unique(tally$process))[i], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
}
dev.off()

round(mean(model_results$rsq),digits=3)
```

#### 3) Stochastic vs. Deterministic

```{r}
require(patchwork)

full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

new_colors<-c("#3CB371","#6495ED")

break_labels<-list()
break_labels[[2]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[1]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
                     "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
                     "0.45 to <0.54" ,"0.54 to 0.72")


full_output$stoc_deter<-rep(NA,nrow(full_output))

full_output[full_output$process=="B_Homogeneous selection",]$stoc_deter<-"Determinisitic"
full_output[full_output$process=="A_Variable selection",]$stoc_deter<-"Determinisitic"
full_output[full_output$process=="E_Dispersal limitation",]$stoc_deter<-"Stochastic"
full_output[full_output$process=="D_Homogenizing dispersal",]$stoc_deter<-"Stochastic"
full_output[full_output$process=="C_Drift",]$stoc_deter<-"Stochastic"


i=1
j=1
k=1
d=1
stoch_deter<-data.frame(n=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL,stoc_deter=NULL)
for(d in 1:2){
        sub<-full_output[full_output$dist==c("Environmental dissimilarity","Distance (km)")[d],]
for(i in 1:length(unique(sub$d_val_group))){
        tmp<-sub[sub$d_val_group==sort(unique(sub$d_val_group))[i],]
        for(j in 1:3){
                tmp2<-tmp[tmp$zone==c("A","B","C")[j],]
        for(k in 1:2){
                tmp3<-tmp2[tmp2$stoc_deter==c("Determinisitic", "Stochastic")[k],]
                stoch_deter<-rbind(stoch_deter, data.frame(n=tmp3$n[1],percent=sum(tmp3$percent),d_val_group=tmp3$d_val_group[1], d_val_inclthisnumandbelow=tmp3$d_val_inclthisnumandbelow[1], zone=tmp3$zone[1],dist=tmp3$dist[1],stoc_deter=tmp3$stoc_deter[1]))
                                   
        }}}}
full_output<-stoch_deter


ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
        for(i in 1:2){
                sub2<-sub[sub$dist==unique(sub$dist)[i],]       
                ifelse(j==3, 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) +  geom_point(size=3)+ geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,0.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15)) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                       
                       , 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15))+ #change font size of legend title
                               theme(axis.title.x=element_blank(),
                                     axis.text.x=element_blank()) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                )
        }}


pdf("Figures/qpen/10plusESVs/Stochastic_deterministic_community_assembly_8Feb2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
        plot_layout(ncol = 2)
dev.off()

```

#### 4) Selection, dispersal, drift

```{r}
require(patchwork)

full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

new_colors<-c("#A9D18E","#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[2]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[1]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
                     "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
                     "0.45 to <0.54" ,"0.54 to 0.72")


full_output$stoc_deter<-rep(NA,nrow(full_output))

full_output[full_output$process=="B_Homogeneous selection",]$stoc_deter<-"A_Determinisitic"
full_output[full_output$process=="A_Variable selection",]$stoc_deter<-"A_Determinisitic"
full_output[full_output$process=="E_Dispersal limitation",]$stoc_deter<-"D_Dispersal limitation"
full_output[full_output$process=="D_Homogenizing dispersal",]$stoc_deter<-"C_Homogenizing dispersal"
full_output[full_output$process=="C_Drift",]$stoc_deter<-"B_Drift"


i=1
j=1
k=1
d=1
stoch_deter<-data.frame(n=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL,stoc_deter=NULL)
for(d in 1:2){
        sub<-full_output[full_output$dist==c("Environmental dissimilarity","Distance (km)")[d],]
for(i in 1:length(unique(sub$d_val_group))){
        tmp<-sub[sub$d_val_group==sort(unique(sub$d_val_group))[i],]
        for(j in 1:3){
                tmp2<-tmp[tmp$zone==c("A","B","C")[j],]
        for(k in 1:4){
                tmp3<-tmp2[tmp2$stoc_deter==c("A_Determinisitic", "B_Drift", "C_Homogenizing dispersal", "D_Dispersal limitation")[k],]
                stoch_deter<-rbind(stoch_deter, data.frame(n=tmp3$n[1],percent=sum(tmp3$percent),d_val_group=tmp3$d_val_group[1], d_val_inclthisnumandbelow=tmp3$d_val_inclthisnumandbelow[1], zone=tmp3$zone[1],dist=tmp3$dist[1],stoc_deter=tmp3$stoc_deter[1]))
                                   
        }}}}
full_output<-stoch_deter


ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
        for(i in 1:2){
                sub2<-sub[sub$dist==unique(sub$dist)[i],]       
                ifelse(j==3, 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) +  geom_point(size=3)+ geom_line() + theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15)) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                       
                       , 
                       
                       
                       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
                                                          aes(x = d_val_group,
                                                              y = percent,
                                                              color= stoc_deter)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.85) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
                               theme(legend.position="none", axis.text=element_text(color="black"))+
                               theme(text=element_text(size=14), #change font size of all text
                                     axis.text=element_text(size=13), #change font size of axis text
                                     axis.title=element_text(size=15), #change font size of axis titles
                                     plot.title=element_text(size=14), #change font size of plot title
                                     legend.text=element_text(size=14), #change font size of legend text
                                     legend.title=element_text(size=15))+ #change font size of legend title
                               theme(axis.title.x=element_blank(),
                                     axis.text.x=element_blank()) +
                               theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
                )
        }}


pdf("Figures/qpen/10plusESVs/lightgreen_Deterministic_dispersal_drift_community_assembly_8Feb2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
        plot_layout(ncol = 2)
dev.off()

```

#### 5) CAP sediment characteristics

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(metagMisc)

#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","process")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"process[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
table(colnames(dfr)==metadata$samp_names) # all true


#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("pH"
, "d_13_c"  
, "cn"
, "sulfur_perc"     
, "water_perc"     
, "protein_per") 

#pull out metadata

metadata_sub<-metadata[,names(metadata)%in%variables]

table(colnames(dfr)==rownames(metadata_sub)) # all true 


daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))
table(colnames(daisy.mat)==colnames(dfr)) # all true
table(rownames(daisy.mat)==rownames(dfr)) # all true


daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

env_dist_reexpand <- reshape(env_dist, direction="wide", idvar="Var1", timevar="Var2")
rownames(env_dist_reexpand)<-env_dist_reexpand$Var1
env_dist_reexpand$Var1<-NULL
colnames(env_dist_reexpand)<-str_split_fixed(colnames(env_dist_reexpand),"value[.]",n=2)[,2]

#figure out which ones were different
setdiff(rownames(env_dist_reexpand),colnames(env_dist_reexpand))
setdiff(colnames(env_dist_reexpand),rownames(env_dist_reexpand))
env_dist_reexpand$SV0226L<-rep(NA,nrow(env_dist_reexpand))
env_dist_reexpand[nrow(env_dist_reexpand)+1,] <- NA
rownames(env_dist_reexpand)[nrow(env_dist_reexpand)]<-"33_1_10_DNA"


env_dist_reexpand<-env_dist_reexpand[order(row.names(env_dist_reexpand)), ]
table(rownames(env_dist_reexpand)==colnames(env_dist_reexpand)) #all true

#how many NAs in each and overlap for metadata
#metadata_sub[,metadatacolnames(env_dist_reexpand)]
met<-metadata_sub[rownames(metadata_sub)%in% colnames(env_dist_reexpand),]
#pH, cn and percent water are present in most samples, but then half have d13c and half the sulfur and protein percent

#subset the process/dfr data frame by the comparisons in metadata
dfr<-dfr[colnames(env_dist_reexpand),colnames(env_dist_reexpand)]

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)==rownames(env_dist_reexpand)) #all true
table(colnames(dfr)==colnames(env_dist_reexpand)) #all true

#melt back to pairwise comparisons
env_dist<-melt(as.matrix(env_dist_reexpand))
process<-melt(as.matrix(dfr))


table(process$Var1==env_dist$Var1) # all true
table(process$Var2==env_dist$Var2) # all true 
process$env_dist<-env_dist$value
process<-process[complete.cases(process),]


pairwise<-process #overwrite original file

names(pairwise)[1]<-"s1_samp_names"
pairwise$s1_samp_names<-as.character(pairwise$s1_samp_names)
names(pairwise)[2]<-"s2_samp_names"
pairwise$s2_samp_names<-as.character(pairwise$s2_samp_names)
names(pairwise)[3]<-"process"

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

pairwise<-merge(pairwise, metadata_s1, by="s1_samp_names")
pairwise<-merge(pairwise, metadata_s2, by="s2_samp_names")

#remove metadata notes
pairwise$s1_notes<-NULL
pairwise$s2_notes<-NULL
pairwise$s1_Notes_sed_water_wt<-NULL
pairwise$s2_Notes_sed_water_wt<-NULL
pairwise$s1_Notes_carbon_nitrogen<-NULL
pairwise$s2_Notes_carbon_nitrogen<-NULL
pairwise$s1_Notes._lake_sed_pH<-NULL
pairwise$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
pairwise$abs_cm<-rep(NA, nrow(pairwise))
i=1
for(i in 1:nrow(pairwise)){
  ifelse(pairwise$s1_lake_drive[i]==pairwise$s2_lake_drive[i],pairwise$abs_cm[i]<-abs(pairwise$s2_depth[i]-pairwise$s1_depth[i]),NA)
}

# calculate geographic distance

require(geosphere)
for(i in 1:nrow(pairwise)){
        pairwise$geo_dist[i]<-as.numeric(distm(data.frame(X = pairwise$s1_longitude[i], Y = pairwise$s1_latitude[i]),data.frame(X = pairwise$s2_longitude[i], Y = pairwise$s2_latitude[i])))
}

pairwise$geo_dist_km<-pairwise$geo_dist/1000

write.csv(pairwise, "qpen_10plusESVs_sediment_characteristics_7Feb2023.csv")
```

plot all zones together

```{r}
require(Hmisc)
qpn<-read.csv("qpen_10plusESVs_sediment_characteristics_7Feb2023.csv", header=T, row.names=1)

#subset by samples within the same core
#downcore<-qpn[is.na(qpn$abs_cm)==F & qpn$abs_cm<27 ,]
downcore<-qpn

downcore$d_val<-downcore$env_dist
downcore$d_val_group_breaks<-cut2(downcore$d_val, g=10)
downcore$d_val_group<-as.numeric(cut2(downcore$d_val, g=10))
levels(downcore$d_val_group_breaks)
break_labels<- c("0 to <0.03" ,"0.03 to <0.05" ,"0.05 to <0.06", "0.06 to <0.08" ,"0.08 to <0.10" ,"0.10 to <0.13"
,"0.13 to <0.15" ,"0.15 to <0.20" ,"0.20 to <0.28", "0.28 to 1")
# [ includes ( up to 

tmp<-downcore
tally<-data.frame(process=NULL,percent=NULL, d_val_group=NULL,n=NULL)
groups<-sort(unique(downcore$d_val_group))
i=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  processes<-as.data.frame(table(tmp2$process)/nrow(tmp2))
  colnames(processes)[1]<-"process"
  colnames(processes)[2]<-"percent"
  processes$process<-as.character(processes$process)
  processes$d_val_group<-rep(groups[i],nrow(processes))
  processes$n<-rep(nrow(tmp2),nrow(processes))
  tally<-rbind(tally,processes)
}
  

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

        ggplt <- ggplot(tally,            
                        aes(x = d_val_group,
                            y = percent,
                            color= process)) +  geom_line(linewidth=2) +geom_point(size=3) + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
                                                                                                                             labels=break_labels) + xlab(label = "Sediment dissimilarity")+ ylab(label = "Proportion") + geom_line(linewidth=2) +   labs(color="Assembly process")    +theme(axis.ticks.x = element_blank()) + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.75) + theme(axis.text.x = element_text(angle = 45,hjust=1))+ guides(color = guide_legend(override.aes = list(linewidth = 6))) +
                theme(axis.text=element_text(color="black"))+
                theme(axis.text.x = element_text(angle = 60,hjust=1))+
                theme(text=element_text(size=14), #change font size of all text
                      axis.text=element_text(size=14), #change font size of axis text
                      axis.title=element_text(size=15), #change font size of axis titles
                      plot.title=element_text(size=14), #change font size of plot title
                      legend.text=element_text(size=14), #change font size of legend text
                      legend.title=element_text(size=15),
                      panel.grid.minor = element_blank(), 
                      panel.grid.major.x = element_blank()) #change font size of legend title 

pdf("Figures/qpen/10plusESVs/SFig3_sediment_characteristics_22Mar2023.pdf", height=3, width=6)
ggplt
dev.off()

unique(tally$n)
#[1] 372 352 302 283 239 403 163 236 190

```

#### 6) t-test for differences within and across lakes

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)

#indicate within or across lakes
full_output$inlake<-rep("no", nrow(full_output))
full_output[full_output$d_val_group%in%c(0,1) & full_output$dist=="Distance (km)",]$inlake<-"yes"
full_output[full_output$d_val_group==0 & full_output$dist=="Environmental dissimilarity",]$inlake<-"yes"



i=1
k=2
t_result<-data.frame(dist=NULL, process=NULL, pval=NULL)
for(i in 1:2){
        sub<-full_output[full_output$dist==unique(full_output$dist)[i],]
for(k in 1:5){
        sub2<-sub[sub$process==unique(sub$process)[k],]
        within_lake<-sub2[sub2$inlake=="yes",]
        across_lake<-sub2[sub2$inlake=="no",]
       result<-t.test(across_lake$percent, within_lake$percent, var.equal = F)
      t_result<-rbind(t_result,data.frame(dist=sub2$dist[1], process=sub2$process[1], pval=result$p.value))
}}

t_result$sig<-rep(NA, nrow(t_result))

t_result[t_result$pval<0.05,]$sig<-"sig"
t_result[t_result$pval>=0.05,]$sig<-"not sig"

max(t_result[t_result$sig=="sig",]$pval)
#[1] 0.04340025
mean(t_result[t_result$sig=="sig",]$pval)
#[1] 0.01167037

```

#### 7) mantel tests of RCBray values

These all need to be dist objects

```{r}
load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(vegan)


#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","RC")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"RC[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)%in%metadata$samp_names) #all true
table(colnames(dfr)%in%metadata$samp_names) # all true

RCbray<-as.dist(dfr)

#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
metadata_sub<-metadata[,names(metadata)%in%variables]


table(colnames(dfr)==metadata$samp_names) # all true
table(colnames(dfr)==rownames(metadata_sub)) # all true 

env_dist <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))
table(labels(RCbray)==labels(env_dist))


# geographic distance

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
geo_dist<-as.dist(dist_m_output)

#check to make sure all labels match!
table(labels(RCbray)==labels(env_dist)) #all true
table(labels(RCbray)==labels(geo_dist)) #all true

mantel(RCbray, env_dist)
mantel(RCbray, geo_dist)

#add partial mantel test, spatially structured environmental variables
mantel.partial(RCbray, env_dist, geo_dist, method = "pearson", permutations = 999)
mantel.partial(RCbray, geo_dist, env_dist, method = "pearson", permutations = 999)
```

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = env_dist)

Mantel statistic r: 0.246 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.00917 0.01263 0.01589 0.01806 Permutation: free Number of permutations: 999

Mantel statistic based on Pearson's product-moment correlation

Call: mantel(xdis = RCbray, ydis = geo_dist)

Mantel statistic r: 0.2047 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0124 0.0165 0.0197 0.0241 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = env_dist, zdis = geo_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.2433 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0103 0.0130 0.0147 0.0179 Permutation: free Number of permutations: 999

Partial Mantel statistic based on Pearson's product-moment correlation

Call: mantel.partial(xdis = RCbray, ydis = geo_dist, zdis = env_dist, method = "pearson", permutations = 999)

Mantel statistic r: 0.2014 Significance: 0.001

Upper quantiles of permutations (null model): 90% 95% 97.5% 99% 0.0120 0.0157 0.0193 0.0256 Permutation: free Number of permutations: 999

#### 8) stats of community assembly processes

```{r}
load("WyLakeMicrobes_env_16Jan2023_qpen_ESVswith10plusreads_results.RData")
round(qp2$ratio, digits=3)*100
#   Heterogeneous.Selection Homogeneous.Selection Dispersal.Limitation
# 1                    26.1                  39.2                 31.4
#   Homogenizing.Dispersal Undominated num.pair
# 1                    1.3         2.1 11400300

#selection
0.2610984 +0.3918757 
# 0.6529741
#round 65.3%

#dispersal
0.3137286 + 0.01266633 +  0.02063104
#0.347026
#round 34.7%
```

fig 6 A (sed distance)

```{r}
full_output<-read.csv("qpen_sedimentdistance_13Feb2023.csv", header=T,row.names=1)
i=1
for( i in 1:5){
sub<-full_output[full_output$d_val_group==1 & full_output$process==sort(unique(full_output$process))[i],]
print(sub$process[1])
print(signif(sub$percent, digits=3)*100)
}

i=1
for( i in 1:5){
sub<-full_output[full_output$d_val_group%in%c(8,9) & full_output$process==sort(unique(full_output$process))[i],]
print(sub$process[1])
print(signif(sub$percent, digits=3)*100)
}

```

Within an individual sediment core, samples \<= 2.5 cm apart were either homogenized (69.9%) or differentiated by selection (5.9%), regardless of horizon. As the distance between samples increased, homogenous selection declined and variable selection increased (Fig. 6, A). At sediment distances \>18 cm (18-22 and 22-26 cm), where all pairs of samples were compared across zones, variable selection predominated (34.4-43.2%), Homogenizing dispersal was highest between comparisons 0-7 cm apart (15.9-18.9%), and declined to 4.7% with increasing sediment distance. Dispersal limitation increased with sediment distance, reaching 26.8% at 22-26 cm. Drift played a minor role (\<10%) across all sediment distances.

figure 6 B and C

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)



#1) same lake (environment)

#Homogenous selection
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="B_Homogeneous selection", ]$percent), digits=3)*100


#variable selection
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="A_Variable selection", ]$percent), digits=3)*100


#disp lim
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="E_Dispersal limitation", ]$percent), digits=3)*100


#homog disp
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="D_Homogenizing dispersal", ]$percent), digits=3)*100


# [1] 60.3
# [1] 9.5
# [1] 8
# [1] 19.8

#or signif

# [1] 60.3
# [1] 9.51
# [1] 8.01
# [1] 19.8
```

Manuscript text:

Homogenous selection was dominant among comparisons within the same horizon in any given lake (mean across horizons, 60.3%), while variable selection accounted for 9.5%, homogenizing dispersal 19.8%, and dispersal limitation 8.0% of such intra-lake, intra-horizon comparisons (Fig. 6, B and C).

inter-lake comparisons (averaged across environmental distance and geographic distance)

```{r}

'%!in%' <- function(x,y)!('%in%'(x,y))

for( i in 1:5){

sub<-rbind(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%!in%c(0) & full_output$process==sort(unique(full_output$process))[i],]
      , 
      full_output[full_output$dist=="Distance (km)" & full_output$d_val_group%!in%c(0,1) & full_output$process==sort(unique(full_output$process))[i],])

print(sub$process[1])
print(signif(mean(sub$percent),digits=3)*100)
print(signif(range(sub$percent),digits=3)*100)
}


```

[1] "A_Variable selection" [1] 16.9 [1] 4.02 34.60

[1] "B_Homogeneous selection" [1] 47.3 [1] 23.2 71.7

[1] "C_Drift" [1] 2.85 [1] 0.0 11.1

[1] "D_Homogenizing dispersal" [1] 1.29 [1] 0.00 7.23

[1] "E_Dispersal limitation" [1] 31.6 [1] 15.8 46.3

As environmental dissimilarity and geographic distance increased with inter-lake comparisons, homogenizing dispersal declined while variable selection and dispersal limitation increased, however homogeneous selection was often the most dominant community assembly process in all horizons. Homogeneous selection ranged from 23.2-71.7% (avg. 47.3%), variable selection 4.0-34.6% (avg. 16.9%), and dispersal limitation 15.8-46.3% (avg. 31.6%). Homogenizing dispersal (mass effects) and drift acting alone was negligible (avg. 1.3 and 2.9%, respectively) across comparisons of abiotic lake environments and geographic distances.

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
'%!in%' <- function(x,y)!('%in%'(x,y))
z=1
i=1
for( i in 1:5){
sub<-rbind(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%!in%c(0) & full_output$process==sort(unique(full_output$process))[i],]
      , 
      full_output[full_output$dist=="Distance (km)" & full_output$d_val_group%!in%c(0,1) & full_output$process==sort(unique(full_output$process))[i],])

for(z in 1:3){
    sub2<-sub[sub$zone==c("A","B","C")[z],]    


print(sub2$process[1])
print(sub2$zone[1])
print(signif(mean(sub2$percent),digits=3)*100)
print(signif(range(sub2$percent),digits=3)*100)
}}
```

[1] "B_Homogeneous selection" [1] "A" [1] 37.7

[1] "B_Homogeneous selection" [1] "B" [1] 44.8

[1] "B_Homogeneous selection" [1] "C" [1] 59.4

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
'%!in%' <- function(x,y)!('%in%'(x,y))
z=1
for(z in 1:3){
    sub<-full_output[full_output$zone==c("A","B","C")[z],]  

sub2<-rbind(sub[sub$dist=="Environmental dissimilarity" & sub$d_val_group%!in%c(0) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],]
      , 
      sub[sub$dist=="Distance (km)" & sub$d_val_group%!in%c(0,1) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],])

 
print(sub2$zone[1])
print(signif(range(sub2$percent),digits=3)*100)
}
```

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_8Feb2023.csv", header=T, row.names = 1)
'%!in%' <- function(x,y)!('%in%'(x,y))
i=1
    sub<-full_output[full_output$zone=="C",]
sub2<-rbind(sub[sub$dist=="Environmental dissimilarity" & sub$d_val_group%!in%c(0) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],]
      , 
      sub[sub$dist=="Distance (km)" & sub$d_val_group%!in%c(0,1) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],])

hs_de<-NULL
hs_vs<-NULL
for (i in 1:length(unique(sub2$d_val_group))){
 sub3<-sub2[sub2$d_val_group==sort(unique(sub2$d_val_group))[i],]

hs_de<-c(hs_de,sub3[sub3$process=="B_Homogeneous selection",]$percent/ sub3[sub3$process=="E_Dispersal limitation",]$percent)
hs_vs<-c(hs_vs,sub3[sub3$process=="B_Homogeneous selection",]$percent/ sub3[sub3$process=="A_Variable selection",]$percent)
    
}
        print(sub3$zone[1])
          print("hs_de")
print(signif(mean(hs_de),digits=3))   
print(signif(range(hs_de),digits=3)) 
          print("hs_vs")
print(signif(mean(hs_vs),digits=3))  
print(signif(range(hs_vs),digits=3)) 

```

[1] "C" [1] "hs_de" [1] 2.19 [1] 1.26 4.53 [1] "hs_vs" [1] 6.71 [1] 3.34 14.20

#### 9) community assembly of deeper samples

```{r}
require(ggplot2)

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_7Feb2023.csv", header=T, row.names=1)
comps<-comps_backup
comps$d_val<-comps$abs_cm
comps<-comps[is.na(comps$d_val)==F,]
comps<-comps[comps$d_val>26,] # 119 comparisons, so no many compared to the other ones

tmp2<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL)
proc<-unique(comps_backup$process)
for(p in 1:5){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2)))
  }
tally$group<-rep("deep", nrow(tally))

EAP_plot<-ggplot(tally, aes(x = group, y = percent , fill = process)) +
    geom_bar(stat="identity") + scale_fill_manual(values=new_colors,labels= c("Variable selection", "Homogeneous selection", "Drift", "Homogenizing dispersal", "Dispersal limitation" )) 
pdf("Figures/qpen/10plusESVs/SFig7_deep_community_assembly.pdf", height=4, width=4)
EAP_plot
dev.off()
```

## 22. trends in trophic status of 8 lakes

```{r}
troph<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/WYLakeSedMicrobes/JordansLakesTrophicStatus.csv", header=T,row.names=1)


require(ggplot2)
my_date <- as.POSIXct("10/14/20", format="%m/%d/%y")
as.numeric(my_date)
tmp$yday

gg<-ggplot(troph, aes(x=as.numeric(as.POSIXct(date, format="%m/%d/%y")), y=pred_numeric, col=lake_name)) + geom_point() + geom_line() +   facet_wrap(vars(lake_name)) + xlab("date") + ylab("trophic status (1: olig 3: eutro)") + geom_smooth(method = "lm", se = FALSE)


gg
```

## 23. shared ESVs and shared families

will all reads

```{r}
require(Matrix)
require(metagMisc)
require(phyloseq)
require(reshape2)
require(cluster)
require(stringr)
require(geosphere)

load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

#calculate
shared_esvs <- phyloseq_num_shared_otus(ps)


#shared ESVs
shared<-as.matrix(shared_esvs[["shared"]])
shared[upper.tri(shared, diag = T)] <- NA
shared<-melt(shared,na.rm=TRUE)

colnames(shared)[3]<-"shared"

#nonshared ESVs
nonshared<-as.matrix(shared_esvs[["nonshared_total"]])
nonshared[upper.tri(nonshared, diag = T)] <- NA
nonshared<-melt(nonshared,na.rm=TRUE)
colnames(nonshared)[3]<-"nonshared"

table(nonshared$Var1==shared$Var1) #all T
table(nonshared$Var2==shared$Var2) #all T

shared$nonshared<-nonshared$nonshared
rm(nonshared)
shared$percent<-shared$shared/(shared$shared+shared$nonshared)


#now add in environmental distances
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(as.matrix(shared_esvs[["shared"]])))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))

daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

table(shared$Var1==env_dist$Var1) # all true
table(shared$Var2==env_dist$Var2) # all true 
shared$env_dist<-env_dist$value
 #overwrite original file

names(shared)[1]<-"s1_samp_names"
shared$s1_samp_names<-as.character(shared$s1_samp_names)
names(shared)[2]<-"s2_samp_names"
shared$s2_samp_names<-as.character(shared$s2_samp_names)

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

shared<-merge(shared, metadata_s1, by="s1_samp_names")
shared<-merge(shared, metadata_s2, by="s2_samp_names")

#remove metadata notes
shared$s1_notes<-NULL
shared$s2_notes<-NULL
shared$s1_Notes_sed_water_wt<-NULL
shared$s2_Notes_sed_water_wt<-NULL
shared$s1_Notes_carbon_nitrogen<-NULL
shared$s2_Notes_carbon_nitrogen<-NULL
shared$s1_Notes._lake_sed_pH<-NULL
shared$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
shared$abs_cm<-rep(NA, nrow(shared))
i=1
for(i in 1:nrow(shared)){
  ifelse(shared$s1_lake_drive[i]==shared$s2_lake_drive[i],shared$abs_cm[i]<-abs(shared$s2_depth[i]-shared$s1_depth[i]),NA)
}


# calculate geographic distance


for(i in 1:nrow(shared)){
        shared$geo_dist[i]<-as.numeric(distm(data.frame(X = shared$s1_longitude[i], Y = shared$s1_latitude[i]),data.frame(X = shared$s2_longitude[i], Y = shared$s2_latitude[i])))
}

shared$geo_dist_km<-shared$geo_dist/1000


#write.csv(shared,"sharedESVs_7Feb2023.csv")
```

```{r}
#comps_backup<-read.csv("sharedESVs_7Feb2023.csv",header=T,row.names=1)

#comps_backup$percent<-comps_backup$percent*100

zone<-c("A","B","C")
comps<-comps_backup
pdf("Figures/PercentSharedESVS_7Feb2023.pdf", width=7, height=18)
par(mfrow=c(7,3), mar=c(4,4,3,2))

j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Environmental dissimilarity", ylim=c(0,40),main=c("Redox", "Transition", "Depauperate")[j])
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$env_dist!=0,]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Environmental dissimilarity",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$geo_dist_km!=0,]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)", ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}}


j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$geo_dist_km>100,]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", xlab="Environmental dissimilarity",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}

j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j] & comps$geo_dist_km>100,]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)", ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}
}

#sediment
comps<-comps_backup[is.na(comps_backup$abs_cm)==F & comps_backup$abs_cm<27 ,]
plot(comps$abs_cm, comps$percent, ylab="Shared ESVs (%)", xlab="Sediment distance (cm)", pch=16, col="#00000050",main="All horizons, individual cores")
abline(lm(comps$percent~comps$abs_cm), col="lightgreen", lwd=2)
if(summary(lm(comps$percent~comps$abs_cm))$coefficients[2,4]<0.05){
        text(x=(max(comps$abs_cm)*.95), max(comps$percent)*0.95,"*" ,cex=3)
}

dev.off()
```

## 24. Make ESV table with taxonomy and actual sequences for Eric Capo

```{r}
tax_tab<-as.data.frame(as.matrix(tax_table(ps)))
identical(otu_tab_10knorm, as.data.frame(as.matrix(otu_table(ps)))) #true
setdiff(rownames(tax_tab),rownames(otu_tab_10knorm)) # none
setdiff(rownames(otu_tab_10knorm),rownames(tax_tab)) # none
ESV_taxa<-merge(tax_tab,otu_tab_10knorm, by="row.names")
rownames(ESV_taxa)<-ESV_taxa$Row.names
ESV_taxa$Row.names<-NULL

seqs<-Biostrings::readDNAStringSet("zotus_nonchimeric.fa") #downloaded straight from the calder file folder
seqs@ranges@NAMES<-stringr::str_split(seqs@ranges@NAMES,pattern = ";", 3, simplify = T)[,1]

seqs_sub<-seqs[rownames(otu_tab_10knorm)]
df_seqs_sub<-as.data.frame(seqs_sub)
names(df_seqs_sub)[1]<-"sequence"
table(rownames(df_seqs_sub) %in% rownames(ESV_taxa))
table(rownames(ESV_taxa)%in%rownames(df_seqs_sub))

ESV_taxa_seq<-merge(df_seqs_sub,ESV_taxa, by="row.names")
rownames(ESV_taxa_seq)<-ESV_taxa_seq$Row.names
ESV_taxa_seq$Row.names<-NULL

write.csv(ESV_taxa_seq, "20230316_Seq_Taxa_ESV.csv")

```

## 25. esvs unique to a core

```{r}


require(metagMisc)
require(Matrix)

lake<-merge_samples(ps_tr, "lake_name")
lake

shared_esvs <- phyloseq_num_shared_otus(lake)

shared<-as.matrix(shared_esvs[["shared"]])
ESV<-as.data.frame(t(as.matrix(lake@otu_table@.Data)))


lake_taxa_present<-list()
i=1
for(i in 1:36){
        sub<-ESV[,i]
sub2<-as.data.frame(cbind(rownames(ESV),sub))
names(sub2)<-c("ESV","abundance")
which<-sub2[which(sub2$abundance!=0),]
listoftaxa<-which$ESV
lake_taxa_present[[i]]<-listoftaxa
}

num_unique<-NULL
for(j in 1:36){ 
        sub<-lake_taxa_present
        sub[[j]]<-NULL
        allelse<-unique(unlist(sub))
num_unique<-c(num_unique,table(lake_taxa_present[[j]]%in% allelse)[1])
}
range(num_unique/91957)
mean(num_unique/91957)
```
