---
title: "WY Lake Sediment Prokaryotic Community Analyses"
author: "Jordan Von Eggers"
date: "20 Apr 2023"
output: html_document
editor_options: 
  chunk_output_type: console
---

#request resources in Beartooth

```{bash}
salloc --mem=60GB --nodes=1 --cpus-per-task=1 --account=microbiome --time=1:00:00
```

General notes: - Round lake didn't have a bottom water temperature recorded, but surface water was 20.1 degrees and the lake was 1.5 m so it is likely the bottom water temperature isn't too much different (around 20 deg C), so I used 20.1 as the temperature - mV for sample SR20122L: changed from 1121.6 to 121.6 since likely an error (pH matches perfectly and the surrounding measurements are between 107 and 134) -- but not a problem when looking at the ps object data, since this sample must have been removed for low reads or something - Only one truly replicated sample (LB0208L1 and LB0208L2)

# load packages
```{r}
require(stringr)
require(tidyverse)
require(vegan)
require(phyloseq)
require(iNEXT)
require(phangorn)
require(msa)
require(Biostrings)
require(colorspace)
require(cluster)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(metagMisc)
require(ggplot2)
library(MetBrewer)
require(pivottabler)
require(rioja)
require(mgcv)
require(parallelDist)
require(ape)
require(dplyr)
library(caret)
library(picante)


```


# Part I: Extract files and assign taxonomy

## 1. Grab files off Beartooth supercomputer

copy the bash file used to make the files above! (from Alex Buerkle) move to folder: /Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/original files off Teton

```{bash}
rsync jvonegge@teton.uwyo.edu:/project/microbiome/data/seq/psomagen_17sep20_novaseq2/otu/run_slurm_mkotu.pl /Users/jordanscheibe/Desktop

#from: run_slurm_mkotu.pl - this is from april 28 (I assume) so likely the final one! and all the files are here so I can conclude it is this one
foreach my $tfmdirectory (sort keys %allprojectsotu){
    # print $tfmdirectory, " -- " , $allprojectsotu{$tfmdirectory}, "\n";
    $job = '';
    unless(-e $allprojectsotu{$tfmdirectory}){ # this is the otu directory
	mkpath $allprojectsotu{$tfmdirectory};
    }

    $job .= "cd $allprojectsotu{$tfmdirectory};\n"; # this is the otu directory
    $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --derep_fulllength - --threads 32 --output uniqueSequences.fa --sizeout;\n";
    $job .= "vsearch --cluster_unoise uniqueSequences.fa --relabel 'otu' --sizein --sizeout --consout zotus.fa --minsize 8 ;\n";
    $job .= "vsearch --uchime3_denovo zotus.fa --nonchimeras zotus_nonchimeric.fa --threads 32;\n";
    $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --usearch_global - --db zotus_nonchimeric.fa --otutabout - --id 0.99 --threads 32 | sed 's/^#OTU ID/OTUID/' > 'otutable';\n";
    $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --search_exact - --db zotus_nonchimeric.fa --otutabout - --threads 32 | sed 's/^#OTU ID/OTUID/' > 'otutable.esv';\n";
   $job .= "cat $tfmdirectory/*tfmergedreads.fa $tfmdirectory/joined/*tfmergedreads.fa | vsearch --search_exact - --db zotus_nonchimeric.fa --biomout otu.esv.biom.json --threads 32\n";
    push @jobarray, $job, 
}

```

copy orginal files from the Micro Project to my folders but just grab: 1. otutable.esv 2. zotus_nonchimeric.fa

```{bash}
cd /project/microbiome/data/seq/psomagen_17sep20_novaseq2/otu/16S/Calder
cp * /project/seddna/jvonegge/WY_lake_microbes/16S/original_files
```


```{r}
rsync -r jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/original_files /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes

```

move all the other files aside from those two into an "other" folder

## 2. Assigning taxonomy
copy the SILVA training dataset over for DADA2
```{r}
rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/silva_nr99_v138.1_wSpecies_train_set.fa.gz jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/reference_database

```

Using the maintained datasets from DADA2 hosted on Zenodo

McLaren, Michael R., & Callahan, Benjamin J. (2021). Silva 138.1 prokaryotic SSU taxonomic training data formatted for DADA2 [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4587955

First, read in the fasta file with sequences, remove three duplicates (that aren't in the OTU table) and assign taxonomy using DADA2

quickly read in the OTU table - when we subset by these duplicates, we find that the duplicated sequences aren't in here, so not a problem, we will just remove them from the zotus_nonchimeric.fa file
otu_table<-read.delim("OriginalFiles/otutable.esv", header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t") dups$centroid<-str_split(rownames(dups),pattern = ";", 3, simplify = T)[,1]
dup_otu<-otu_table[which(otu_table$OTUID %in% dups$centroid),]

runR_assignTaxonomy_dada2.sh
```{bash}

#!/bin/bash
#SBATCH --job-name assign_tax_dada2
#SBATCH --mem=100GB
#SBATCH --time=3-00:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=assign_tax_dada2_%A.out
hostname; date
module load miniconda3/4.12.0
source activate dada2_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/assign_taxonomy/dada2
srun Rscript assignTaxonomy_dada2.R
echo "srun Rscript assignTaxonomy_dada2.R"
source deactivate dada2_env
echo "finished assigntax JV"
date
```



assignTaxonomy_dada2.R
```{r}
require(dada2)
require(Biostrings)
require(stringr)
fasta <- readDNAStringSet("/project/seddna/jvonegge/WY_lake_microbes/16S/original_files/zotus_nonchimeric.fa")

removefromfasta<-c("centroid=otu50237;seqs=1;size=53","centroid=otu52917;seqs=1;size=48", "centroid=otu55415;seqs=1;size=44")
fasta<-fasta[!names(fasta) %in% removefromfasta]
rm(removefromfasta)

tax_table <- assignTaxonomy(fasta, "/project/seddna/jvonegge/WY_lake_microbes/16S/reference_database/silva_nr99_v138.1_wSpecies_train_set.fa.gz")
write.csv(tax_table,"Silva138_assignedTaxonomy_toSpecies_temp1_28Apr2023.csv")
ESVseq<-data.frame(seq=as.character(fasta), names=names(fasta))
ESVseq$ESV<-str_split(ESVseq$names,pattern = ";", 3, simplify = T)[,1]
ESVseq$names<-NULL

tax_table<-merge(tax_table,df, by="seq")
tax_table$seq<-NULL
rownames(tax_table)<-tax_table$ESV
tax_table$ESV<-NULL
write.csv(tax_table,"Silva138_assignedTaxonomy_toSpecies_28Apr2023.csv")

```
This all assigned fine to the "Silva138_assignedTaxonomy_toSpecies_temp1_28Apr2023.csv" object and then I got an error afterwards in the outfile because of the merging (didn't change the tax_table column to "seq"), see below for correction


copy back to computer and add in the taxa names by merging by sequenes
```{r}
require(dada2)
require(Biostrings)
require(stringr)
fasta <- readDNAStringSet("OriginalFiles/zotus_nonchimeric.fa")
removefromfasta<-c("centroid=otu50237;seqs=1;size=53","centroid=otu52917;seqs=1;size=48", "centroid=otu55415;seqs=1;size=44")
fasta<-fasta[!names(fasta) %in% removefromfasta]
rm(removefromfasta)

tax_table<-read.csv("AssignTaxonomy/Silva138_assignedTaxonomy_toSpecies_temp1_28Apr2023.csv", header=T)
colnames(tax_table)[1]<-"seq"
ESVseq<-data.frame(seq=as.character(fasta), names=names(fasta))
ESVseq$ESV<-str_split(ESVseq$names,pattern = ";", 3, simplify = T)[,1]
ESVseq$names<-NULL

tax_table<-merge(tax_table,ESVseq, by="seq")
tax_table$seq<-NULL
rownames(tax_table)<-tax_table$ESV
tax_table$ESV<-NULL
#write.csv(tax_table,"AssignTaxonomy/FinalTaxFile/Silva138_assignedTaxonomy_toSpecies_2May2023.csv")
```
Put these into an "AssignTaxonomy" folder and then the final one in the "FinalTaxFile"

# Part 2: Data Cleaning

note: if going to rerun, make sure that there continues to have the object 'lessthan1' be 'integer (empty)' on lines 869, 1068 and 1206

## 1. remove unassigned, or incorrect OTUs

### a. read in taxa table

```{r}
tax_tab<-read.csv("AssignTaxonomy/FinalTaxFile/Silva138_assignedTaxonomy_toSpecies_2May2023.csv", header=T,row.names=1)
table(tax_tab$Kingdom) # showing NO Eukaryotes
unique(tax_tab$Kingdom) # when the cell is empty there is an NA
```

### b. remove unassigned domain, chloroplast, and mitochondria 

Initially I removed chloroplast and mitochondria, but I think I should keep them now since chloroplast is assigned to Cyanobacteria and mitochondria to Alphaproteobacteria

but then after reading chloroplasts come evolved from cyanos and mitochondria from alphaproteobacteria, but still these are found in eukaryotic cells, so need to remove

```{r}
#unassigned taxa at the kingdom/domain level (column 1)
table(is.na(tax_tab[,1]))

#  FALSE   TRUE 
# 123794    111 
# with the old Silva v123 database 2427 were unassigned at the kingdom/domain level, now only 111 (great!)

tax_tab<-tax_tab[-which(is.na(tax_tab$Kingdom)),] # remove unassigned kingdom

table(tax_tab[,]=="Chloroplast")
#  FALSE   TRUE 
# 504979   1013
#all of these are Cyanobacteria, but still from euk
123794 - 1013 
#new number of taxa should be 122781

table(tax_tab$Kingdom=="Chloroplast")
table(tax_tab$Phylum=="Chloroplast")
table(tax_tab$Class=="Chloroplast")
table(tax_tab$Order=="Chloroplast") # only here
table(tax_tab$Family=="Chloroplast")
table(tax_tab$Genus=="Chloroplast")
table(tax_tab$Species=="Chloroplast")

tax_tab<-tax_tab[-which(tax_tab$Order=="Chloroplast"),] # remove chloroplasts

table(tax_tab[,]=="Mitochondria")
#  FALSE   TRUE 
# 504925   1067
122781 - 1067
#new number of taxa should be 121714

table(tax_tab$Kingdom=="Mitochondria")
table(tax_tab$Phylum=="Mitochondria")
table(tax_tab$Class=="Mitochondria")
table(tax_tab$Order=="Mitochondria") 
table(tax_tab$Family=="Mitochondria") #only here
table(tax_tab$Genus=="Mitochondria")
table(tax_tab$Species=="Mitochondria")

#Mitochondria is only found for 
# Bacteria
# Proteobacteria
# Alphaproteobacteria
# Rickettsiales
# Mitochondria
# but maybe they evolved together? not sure if I should throw out: https://doi.org/10.1016/S0014-5793(01)02618-7
# still eukaryotes

tax_tab<-tax_tab[-which(tax_tab$Family=="Mitochondria"),]
```

### @count_track

```{r}
nrow(tax_tab)
# [1] 121714 - 11May2023
```


## 2. read in OTU table

from folder on Teton, now Beartooth /project/microbiome/data/seq/psomagen_17sep20_novaseq2/otu/16S/Calder

```{r}
otu_table<-read.delim("OriginalFiles/otutable.esv", header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
#change the OTU name to be a row name
rownames(otu_table)<-otu_table$OTUID
otu_table$OTUID<-NULL

#a double check that there are no NAs in the table - this takes awhile so checked once.
#table(is.na(otu_table[,]))
#     FALSE 
# 161770302 11May2023
```

### a. sed only and remove samples

1.  remove all water samples H24blank H3BL00_3 H59BL01_1 H6BL01_3 H64BL00_1 H66BL00_2 H67BL07_2 H7BL01_3 H70BL10_2 H72BL10_1 H73BL07_1 H74BL01_2 H9BL10_3 H29CL00_1 H31CL01_2 H34CL01_1 H35CL07_3 H36CL07_2 H43CL00_3 H46CL16_1 H48CL00_2 H51CL16_3 H55CL16_2 H61CL01_3 H65CL07_1 H11EG03_3 H12EG01_2 H13EG00_1 H14EG06_2 H15EG00_3 H16EG01_3 H17EG01_1 H18EG03_2 H19EG06_3 H20EG00_2 H21EG03_1 H22EG06_1 H5HL00_1 H10HW00_1 H4LW00_1 H68LL00_2 H38LO08_3 H39LO08_1 H40LO01_3 H44LO08_2 H45LO01_3 H50LO05_1 H52LO00_3 H53LO05_3 H54LO05_2 H56LO00_1 H57LO01_1 H69LO00_2 H2LS00_1 H58ML00_2 H1RL00_1 H25SV06_3 H28SV04_2 H30SV01_3 H32SV00_1 H33SV04_1 H41SV04_3 H47SV00_3 H49SV01_1 H60SV01_2 H62SV06_1 H63SV00_2 H71SV06_2 H26SR00_1 H23SG10_1 H27SG00_1 H37SG01_1 H42SG15_1
2.  remove samples that should not have been included RL0100L2 #should not have been sequenced RL0104L #should not have been sequenced SM0100L #private land, cannot include SM0104L #private land, cannot include SV0100L #should not have sequenced first core SV0104L #should not have sequenced first core TL0100L2 #biased productivity (near septic) TL0104L #biased productivity (near septic) LL0404L #out of order since core starts at 38 cm LW0100 \# duplicated sample, clearly wrong LWL0104L #sampled poorly 2017, should not have been sequenced LWL100L #sampled poorly 2017, should not have been sequenced

```{r}
OTU_names<-unique(names(otu_table))

water<-c("H24blank",
"H3BL00_3",
"H59BL01_1",
"H6BL01_3",
"H64BL00_1",
"H66BL00_2",
"H67BL07_2",
"H7BL01_3",
"H70BL10_2",
"H72BL10_1",
"H73BL07_1",
"H74BL01_2",
"H9BL10_3",
"H29CL00_1",
"H31CL01_2",
"H34CL01_1",
"H35CL07_3",
"H36CL07_2",
"H43CL00_3",
"H46CL16_1",
"H48CL00_2",
"H51CL16_3",
"H55CL16_2",
"H61CL01_3",
"H65CL07_1",
"H11EG03_3",
"H12EG01_2",
"H13EG00_1",
"H14EG06_2",
"H15EG00_3",
"H16EG01_3",
"H17EG01_1",
"H18EG03_2",
"H19EG06_3",
"H20EG00_2",
"H21EG03_1",
"H22EG06_1",
"H5HL00_1",
"H10HW00_1",
"H4LW00_1",
"H68LL00_2",
"H38LO08_3",
"H39LO08_1",
"H40LO01_3",
"H44LO08_2",
"H45LO01_3",
"H50LO05_1",
"H52LO00_3",
"H53LO05_3",
"H54LO05_2",
"H56LO00_1",
"H57LO01_1",
"H69LO00_2",
"H2LS00_1",
"H58ML00_2",
"H1RL00_1",
"H25SV06_3",
"H28SV04_2",
"H30SV01_3",
"H32SV00_1",
"H33SV04_1",
"H41SV04_3",
"H47SV00_3",
"H49SV01_1",
"H60SV01_2",
"H62SV06_1",
"H63SV00_2",
"H71SV06_2",
"H26SR00_1",
"H23SG10_1",
"H27SG00_1",
"H37SG01_1",
"H42SG15_1")

remv<-c("LWL0104L",
"LWL100L",
"LW0100",
"LL0404L",
"RL0100L2",
"RL0104L",
"SV0100L",
"SV0104L",
"SM0100L",
"SM0104L",
"TL0100L2",
"TL0104L")

remove<-c(remv,water)
rm(remv)
rm(water)
positions_remove<-NULL
for(i in 1:length(remove)){
   positions_remove<-c(positions_remove, grep(remove[i], OTU_names))    
}

#check that only the samples you want are removed, (i.e. not LW0100L on accident)
remove_df<-otu_table[,positions_remove]
samp_names_rm<-str_split(names(remove_df),pattern = "_", 4, simplify = T)[,4]
keep<-grep(setdiff(samp_names_rm,remove), OTU_names)  #only one sample (LW0100L) that is in the removed list that shouldn't be!
positions_remove_new<-positions_remove[!positions_remove%in%keep]
table(positions_remove_new%in%keep)
table(positions_remove_new%in%positions_remove)


positions_remove<-positions_remove_new
rm(remove_df)
rm(samp_names_rm)
rm(keep)
rm(positions_remove_new)


#overwrite otu_table with columns (samples) removed
otu_table_backup<-otu_table #make backup
otu_table[,positions_remove]<-list(NULL)

#check this worked
OTU_names<-unique(names(otu_table))
positions_remove<-NULL
i=1
for(i in 1:length(remove)){
   positions_remove<-c(positions_remove, grep(remove[i], OTU_names))    
}
OTU_names[positions_remove]
#only two left are the LW0100L samples which means it worked

rm(OTU_names)
rm(remove)
rm(positions_remove)
rm(i)
rm(otu_table_backup) #delete backup

```

### @count_track

starting, with chloroplast, mitochondria, and NAs included no water, or samples that shouldn't have been sequenced 11 blanks included here - not the water blank (that was removed) removed OTUS with no reads updated 29 aug 2022

```{r}
#starting number of reads in the OTU table including blanks
sum(colSums(otu_table))
#[1] 45453337 - 11May2023

#avg number of reads per sample before filtering with blanks
sum(colSums(otu_table))/length(otu_table)
#[1] 40153.12  - 11May2023

#starting number of OTUs, first check if any OTUs are 0
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

greaterthan0<-which(reads_per_otu$reads>0)
otu_table_zero_rm<-otu_table[greaterthan0,]

#check this visually (no taxa with 0)
reads_per_otu<-as.data.frame(rowSums(otu_table_zero_rm))

#overwrite otu table
otu_table<-otu_table_zero_rm

rm(otu_table_zero_rm)
rm(reads_per_otu)
rm(greaterthan0)

#number of OTUS
nrow(otu_table) 
#[1] 122444  - 11May2023
save.image("DataCleaning/TemporaryDataCleaning_Part2_2a.Rdata")
```

added this september 2nd to calculate: total number of reads (blanks removed), kept mitochondria, chloroplast and NAs also for reads per sample (555 samples without blanks)

!!!did not keep this going forward, skip this section if rerunning!!!!

11May2023 - did not rerun since this is trying to figure out starting OTUs, but not really the main first number.. saved the image as save.image("DataCleaning/TemporaryDataCleaning_Part2_2a.Rdata") before just in case I want to go back and do this. 

```{r}
#initiate object to store column number containing blanks
blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)

#remove blanks from otu_table  & calculate reads
otu_table_no_blanks<-otu_table[, -blanks]
num_read<-as.data.frame(colSums(otu_table_no_blanks))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


#otu table with blanks removed
#total reads
sum(colSums(otu_table_no_blanks))
#45407617 - 2 Sept 2022
#average number of reads per sample
sum(colSums(otu_table_no_blanks))/(length(otu_table_no_blanks)/2)
#[1] 81815.53- 2 Sept 2022 (double checked this is the same 10July2023 for manuscript supplementary material)


reads_per_otu<-as.data.frame(rowSums(otu_table_no_blanks))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

greaterthan0<-which(reads_per_otu$reads>0)
otu_table_zero_rm<-otu_table_no_blanks[greaterthan0,]
nrow(otu_table_zero_rm)
#[1] 122443 - 2 Sept 2022



#BLANKS BEFORE REMOVING OTUS REMOVED FROM TAXONOMY



#subset blanks from otu_table & calculate reads
blank_data<-otu_table[, blanks]
blank_reads<-as.data.frame(colSums(blank_data))
names(blank_reads)<-"reads"
blank_reads$sample<-rownames(blank_reads)
rownames(blank_reads)<-NULL
summary(blank_reads$reads) 
   #  Min.  1st Qu.   Median     Mean  3rd Qu.     Max.  - 2 Sept
   # 26.00    36.75    48.50  2078.18   106.75 30828.00 


#reads in blanks
sum(colSums(blank_data))
#[1] 45720 - 2 Sept
#number of OTUs
tmp<-as.data.frame(rowSums(blank_data))
tmp<-tmp[tmp$`rowSums(blank_data)`>0,]
length(tmp)
#[1]  8950 - 2 Sept 
#22 samples

# how much is that one sample of total reads
(13537+ 30828)/45720 #2 Sept (double checked this is the same 10July2023 for manuscript supplementary material)
rm(tmp)
rm(blanks)
rm(otu_table_no_blanks)
rm(blank_data)
rm(num_read)


```

### b. remove OTUs that were removed from the taxonomy table

otu_table and tax_tab have different numbers because we deleted OTUs with 0 reads
```{r}
keep<-rownames(tax_tab)
otus<-rownames(otu_table)
removefromotutab<-setdiff(otus,keep)
table(otus%in%keep)
table(row.names(otu_table) %in% removefromotutab)

otu_table2<-otu_table[!(row.names(otu_table) %in% removefromotutab),]
#overwrite OTU table
otu_table<-otu_table2

rm(keep)
rm(otus)
rm(removefromotutab)
rm(otu_table2)
```

### @count_track

otu table with blanks and chloroplast/mitochondria/domainNA removed as well as water and bad samples, blanks kept updated May 11, 2023

```{r}
#number of reads in the OTU table including blanks
sum(colSums(otu_table))
#[1] 43346162 - 11May2023
#number of OTUS
nrow(otu_table)
#[1] 120506 - 11May2023
#avg number of reads per sample
sum(colSums(otu_table))/length(otu_table)
#[1] 38291.66 - 11May2023

#number of samples with blanks included
ncol(otu_table)
#[1] 1132 - 11May2023

```

### c. remove blanks to assess reads

```{r}
#initiate object to store column number containing blanks
blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)

#subset blanks from otu_table & calculate reads
blank_data<-otu_table[, blanks]
blank_reads<-as.data.frame(colSums(blank_data))
names(blank_reads)<-"reads"
blank_reads$sample<-rownames(blank_reads)
rownames(blank_reads)<-NULL
summary(blank_reads$reads)
   # Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  11May2023
   #  0.0     2.0     4.0  1952.2    11.5 29682.0 

#remove blanks from otu_table  & calculate reads
otu_table_no_blanks<-otu_table[, -blanks]
num_read<-as.data.frame(colSums(otu_table_no_blanks))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


rm(blank_reads)
```

### @count_track (blanks and OTU with blanks removed before further filtering) - this is without chloroplast/NA/mitochondira

we actually remove the blanks in step 6, but this is just calculating the reads
```{r}
#otu table with blanks removed
#total reads
sum(colSums(otu_table_no_blanks))
# [1] 43303214 - 11May2023
#average number of reads per sample
sum(colSums(otu_table_no_blanks))/length(otu_table_no_blanks)
# [1] 39011.9 - 11May2023

#number of OTUs with otus with zero removed
tmp<-as.data.frame(rowSums(otu_table_no_blanks))
tmp<-tmp[tmp$`rowSums(otu_table_no_blanks)`>0,]
length(tmp)
#[1] 120506 - 11May2023
rm(tmp)
#number of (duplicated) samples
#1132-22 = 1110 - checked 11May2023


#reads in blanks
sum(colSums(blank_data))
#[1] 42948 - 11May2023
#number of OTUs
tmp<-as.data.frame(rowSums(blank_data))
tmp<-tmp[tmp$`rowSums(blank_data)`>0,]
length(tmp)
#[1]  8871 - 11May2023
#22 samples
rm(tmp)
rm(blanks)
rm(otu_table_no_blanks)
rm(blank_data)
rm(num_read)

```

## 3. remove OTUs with low reads

at this point you should just have the two data frames in your environment: 1. tax_tab 2. otu_table (taxonomy filtered, water and bad samples removed, blanks included)

### a. plot otus per read count

```{r}
#do on otu_table with blanks, with taxonomy filtering done on otu table
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

#make a table to see the counts of each otu
table_no.out_per.read<-as.data.frame(table(reads_per_otu$reads))
table_no.out_per.read$Var1<-as.numeric(as.character(table_no.out_per.read$Var1))
colnames(table_no.out_per.read)<-c("reads","no.otus")

#plot all to see that this is just happening within the first 100 reads

pdf("DataCleaning/Number_otu_per_read_notpruned_11May2023.pdf")
plot(table_no.out_per.read$reads,table_no.out_per.read$no.otus,main="number of otus per read count", xlab="reads", ylab="number of otus")
dev.off()
#plot the subset of 0-20 reads

pdf("DataCleaning/Number_otu_per_read_to100_notpruned_11May2023.pdf")
plot(table_no.out_per.read$reads[1:100],table_no.out_per.read$no.otus[1:100],main="number of otus per read count", xlab="reads", ylab="number of otus")
dev.off()


rm(table_no.out_per.read)
rm(reads_per_otu)
```

Decision: remove OTUs with less than 10 reads calculate number of OTUs and reads

### b. remove otus with low reads & @count_track

Chose 10 reads, citation: Urrutia-Cordero, P., Langenheder, S., Striebel, M., EklÃ¶v, P., Angeler, D. G., Bertilsson, S., ... Hillebrand, H. (2021). Functionally reversible impacts of disturbances on lake food webs linked to spatial and seasonal dependencies. Ecology, 102(4). <https://doi.org/10.1002/ecy.3283>

```{r}
# output values for unpruned otu table with blanks
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan10<-which(reads_per_otu$reads<10)
otu_table_lessthan10removed<-otu_table[lessthan10,]
sum(colSums(otu_table_lessthan10removed))
nrow(otu_table_lessthan10removed)
# [1] 136262 - 11May2023 #reads removed
# [1] 18378 - 11May2023 #otus removed

# @count_track
otu_table_10<-otu_table[-lessthan10,]
sum(colSums(otu_table_10))
nrow(otu_table_10)
# [1] 43209900 - 11May2023 #reads retained
# [1] 102128 - 11May2023 #otus retained

otu_table<-otu_table_10 #overwrite otu_table


rm(lessthan10)
rm(otu_table_lessthan10removed)
rm(reads_per_otu)
rm(otu_table_10)

```

### c. plot reads by OTUs for each sample

```{r}
#for filtered (reads per otu and kingdomNA/chloroplast/mito) otu table - not pruned (reads per sample) otu_table
#calculate number of reads per sample
reads<-as.data.frame(colSums(otu_table))
names(reads)<-"reads"

#calculate number of otus per sample
samples<-names(otu_table)
table(rownames(reads)==samples) # make sure this is TRUE

OTUs<-NULL
for(i in 1:ncol(otu_table)){
        tmp<-as.data.frame(otu_table[,i])
        tmp2<-tmp[tmp>0,]
        OTUs<-c(OTUs, length(tmp2))
}
rm(i)
rm(tmp)
rm(tmp2)
rm(samples)


tmp3<-as.data.frame(cbind(reads$reads,OTUs))
names(tmp3)<-c("reads3","otus3")
tmp3$sample<-rownames(reads)


pdf("DataCleaning/Reads_otus_bysamp_notpruned_10plusreadsperOTU_11May2023.pdf")
plot(OTUs,reads$reads, main="number of reads and otus for each sample", ylab="reads")
dev.off()


rm(OTUs)
rm(reads)
rm(tmp3)

```

Note - this suggest that I should normalizing (verses pruning at something like 500 or more) since the number of OTUs increases with then number of reads.

## 4. remove samples with low reads

### a. plot number of reads per sample

```{r}
### with blanks and otus with less than 10 reads removed, and taxonomy filtered####
num_read<-as.data.frame(colSums(otu_table))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


par(mar=c(2,4,2,2))
pdf("DataCleaning/Boxplot_reads_per_sample_11May2023.pdf")
boxplot(num_read$reads, main="total reads per sample before pruning")
dev.off()

summary(num_read$reads)
   # Min. 1st Qu.  Median    Mean 3rd Qu.    Max. - 11May2023
   #    0    9768   31242   38171   56606  182311 


par(mar=c(2,4,2,2))
pdf("DataCleaning/Histogram1_reads_per_sample_11May2023.pdf")
hist(num_read$reads, main="total reads per sample before pruning", xlab="number of reads", breaks=150)
dev.off()


par(mar=c(2,4,2,2))
pdf("DataCleaning/Histogram2_low_reads_per_sample_11May2023.pdf")
hist(num_read[num_read$reads<25000,]$reads, xlab="number of reads", main="total reads per sample before pruning", breaks=150)
dev.off()


par(mar=c(2,4,2,2))
pdf("DataCleaning/Histogram3_lower_reads_per_sample_11May2023.pdf")
hist(num_read[num_read$reads<10000,]$reads, xlab="number of reads", main="total reads per sample before pruning", breaks=150)
dev.off()


par(mar=c(2,4,2,2))
pdf("DataCleaning/Histogram4_lowest_reads_per_sample_11May2023.pdf")
hist(num_read[num_read$reads<5000,]$reads, xlab="number of reads", main="total reads per sample before pruning", breaks=150)
dev.off()


pdf("DataCleaning/Histogram5_samples_per_readslessthan1000_11May2023.pdf")
hist(as.data.frame((num_read[num_read$reads<1000,]$reads))[,1],breaks = 100,xlab="number of reads", ylab="freq of samples", main="number of samples by read number (<1000)")
dev.off()


pdf("DataCleaning/Histogram6_samples_per_readslessthan1000_29Aug2022.pdf")
hist(as.data.frame((num_read[num_read$reads<1000,]$reads))[,1],breaks = 50,xlab="number of reads", ylab="freq of samples", main="number of samples by read number (<1000)")
dev.off()


```

numbers of samples lost with pruning

```{r}
#this is with the blanks included, OTUs with less than 10 reads removed, and taxonomy filtering
#200 samples with less than 1000 reads... looking to see where I should cut as the "low number of reads"
#this is calculated with blanks removed already
length(num_read[num_read$reads<1000,]$reads)
length(num_read[num_read$reads<500,]$reads)
length(num_read[num_read$reads<200,]$reads)
length(num_read[num_read$reads<150,]$reads)
length(num_read[num_read$reads<100,]$reads)
#removing samples with <100 reads, we remove 97 samples

#-11May2023
# [1] 152
# [1] 135
# [1] 117
# [1] 107
# [1] 97

rm(num_read)
```

### b. prune samples @ 100

choice is to remove samples with less than 100 reads

```{r}
# Rerun the number of reads per sample using the OTU table with blanks included (otu_table)
num_read<-as.data.frame(colSums(otu_table))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL


# prune at 100 reads
prune<-which(num_read$reads<100)
# 97 duplicated samples will be removed

otu_table_pruned<-otu_table[,-prune]
ncol(otu_table_pruned)
# 1035 - 11May2023 duplicated samples remaining

#visually check those columns were removed! 
remove<-num_read[prune,] # all of these are less than 100
sum(remove$reads)
#2424 -11May2023 reads removed with pruning
num_read_pruned<-as.data.frame(colSums(otu_table_pruned)) # visually - all over 100 
sum(num_read_pruned$`colSums(otu_table_pruned)`)
#43207476 -11May2023 reads after pruning (see below for count track, these match)

#remove OTUs that are now zero 
reads_per_otu<-as.data.frame(rowSums(otu_table_pruned))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan1<-which(reads_per_otu$reads<1)
#none less than one -11May2023

#rewrite otu table
otu_table<-otu_table_pruned

rm(lessthan1)
rm(otu_table_pruned)
rm(reads_per_otu)
rm(remove)
rm(num_read_pruned)
rm(prune)
rm(num_read)
```

### @count_track

```{r}
#reads in pruned otu table
sum(colSums(otu_table))
#number of otus
nrow(otu_table)
#number of duplicated samples
ncol(otu_table)

# [1] 43207476 - 11May2023
# [1] 102128 - 11May2023
# [1] 1035 - 11May2023 
```

## 5. correlate technical replicates

### a. correlate technical replicates

```{r}
otu_tab_derep <- otu_table

#Pull out the barcode names from the current sample names containing locus ID, forward MID, reverse MID, and Library number.
#split by "_"" 4 times, save as character matrix and extract the 4th slot.
samp_names<-str_split(names(otu_tab_derep),pattern = "_", 4, simplify = T)[,4]
length(unique(samp_names))
#532 unique samples 11May2023


#I also could go look for the specific tag... could ask kevin if this is TAG jumping?? 
names(otu_tab_derep)<-samp_names

# 1. counts with zeros ####
cor_df<-list()
for(i in 1:length(samp_names)){
        for(j in 1:length(samp_names)){
                if(samp_names[i] == samp_names[j] & i!=j & i>j){
                        tmp<-cbind(otu_tab_derep[i], otu_tab_derep[j])
                        cor_stat <-data.frame(cor(tmp[1], tmp[2])) 
                        cor_df<-c(cor_df, cor_stat)
                }
        }
}

#results: 
#unlist to a dataframe containing sample ID and the correlation among technical replicates.
cor_df <- data.frame(SampleID = rep(names(cor_df), sapply(cor_df, length)), correlation = unlist(cor_df))
#Sort correlations to look at lowest ones.
cor_df %>% arrange(correlation,SampleID)

#503 unique samples (not duplicated) - 11May2023

# 70% cutoff        
low_cor_samp<-cor_df[cor_df$correlation<0.7,]
# 17 samples -11May2023

rm(tmp)
rm(cor_df)
rm(cor_stat)
#rm(low_cor_samp)
rm(i)
rm(j)
rm(otu_tab_derep)
```

### b. look at samples individually

I did this once, and the results are in an excel file, from there, I selected the samples that needed to be removed after summing technical replicates.

for reads correlated less than 70%, kept samples whose smaller reads were less than 25% of the greater reads

```{r}
tmp <- otu_table

samp_names2<-str_split(names(tmp),pattern = "_", 4, simplify = T)[,4]

reads<-as.data.frame(colSums(tmp))
names(reads)<-"reads"
reads$sample<-rownames(reads)
rownames(reads)<-NULL
reads$samp_name<-str_split(reads$sample,pattern = "_", 4, simplify = T)[,4]

xremoved<-str_remove(low_cor_samp$SampleID, "[X]")


select<-which(reads$samp_name%in%xremoved)
cor70<-reads[select,]


rm(reads)
rm(tmp)
rm(cor70)
rm(select)
rm(xremoved)
rm(samp_names2)
rm(low_cor_samp)
rm(samp_names)
```


### c. sum technical replicates

Only the ones that have good correlation, taxonomy filtered, otus with less than 10 reads removed, samples with less than 100 reads removed

```{r}

otu_tab_derep <- otu_table

samp_names<-str_split(names(otu_tab_derep),pattern = "_", 4, simplify = T)[,4]

#use the sample names to compare which ones were removed 
length(unique(samp_names))
# 532 - 11May2023

#just to check that the new sample names match up, I created a new dataframe. I check this three lines below. #old code from Gordon, not needed but good double check
otu_table2<-otu_tab_derep

#rename columns in otu_table2 with the extracted info in samp_names
names(otu_table2)<-samp_names
table(names(otu_table2)==str_split(names(otu_tab_derep),pattern = "_", 4, simplify = T)[,4])
length(unique(names(otu_table2)))
otu_table3 <- rowsum(t(otu_table2), group = colnames(otu_table2), na.rm = T)

otu3_df<-as.data.frame(t(otu_table3))
otu3_df<-otu3_df[,order(names(otu3_df))]
otu2_df<-otu_table2[,order(names(otu_table2))]

#check to make sure they are added right!
otu2_df[1:10,1:6]
otu3_df[1:10,1:5]

summed_otu_table<-otu3_df

#remove extra dataframes
rm(otu_table2)
rm(otu_table3)
rm(otu2_df)
rm(otu3_df)
rm(samp_names)

#remove uncorrelated samples - 11May2023
remove<-c("35_2_6_DNA",
"39_1_18_DNA",
"FB0104L",
"LL0104L",
"LL0112L",
"LS0120L",
"SR0102L",
"SV0408L")


where<-which(names(summed_otu_table)%in% remove) 
table(names(summed_otu_table)[where] %in% remove)
summed_otu_table_goodcorronly<-summed_otu_table[,-where]
table(names(summed_otu_table_goodcorronly)%in% remove) #all FALSE
#rewrite otu_table
otu_table<-summed_otu_table_goodcorronly

#remove OTUs with zeros now
reads_per_otu<-as.data.frame(rowSums(otu_table))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan1<-which(reads_per_otu$reads<1)
#no OTUs less than 1 (integer (empty))


rm(lessthan1)
rm(reads_per_otu)
rm(where)
rm(summed_otu_table_goodcorronly)
rm(remove)
rm(otu_tab_derep)
rm(summed_otu_table)

```

## 6. remove blanks

In this, since most of the blanks were removed, there would only be one blank used to do decontamination (if we rarefied at around 10k again)

```{r}
blanks<- c()
for (i in 1:ncol(otu_table)){
        #if string matching "Blank" or "FB" is in column name, append col number to blanks vector 
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)



#subset blanks from summed_otu_table & calculate reads
blank_data<-as.data.frame(otu_table[, blanks])
rownames(blank_data)<-rownames(otu_table)
blank_reads<-as.data.frame(colSums(blank_data))

	
colSums(blank_data)
 # JC_Blank2 JC7_Blank2 - 11May2023
 #     42396        131 



#overwrite otu table and check blanks removed
otu_table<-otu_table[,-blanks]

blanks<- c()
for (i in 1:ncol(otu_table)){
        if(length(grep("FieldBlank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }else if(length(grep("Blank", colnames(otu_table)[i], value = T))>0){
                blanks<- append(blanks, i)
        }}
rm(i)
#'blanks' object emtpy so no more blanks


rm(blanks)
rm(blank_data)
rm(blank_reads)

```

### @count_track

removed blank reads this is: before normalization 1. mictochondria/chloroplast/NA removed 2. OTUs with less than 10 reads removed 3. samples with less than 100 reads removed 4. technical replicates correlated and summed 5. blanks removed updated 29 aug 2022

```{r}
sum(colSums(otu_table))
nrow(otu_table)
ncol(otu_table)

# [1] 43057075 - 11May2023
# [1] 102128 - 11May2023
# [1] 522 - 11May2023

```

## 7. Write OTU and taxa CSVs before normalization

```{r}
write.csv(otu_table,file="DataCleaning/otu_table_notnorm_11May2023.csv",row.names = T)
write.csv(tax_tab,file="DataCleaning/tax_tab_notnorm_11May2023.csv",row.names = T)

save.image("DataCleaning/WyLakeMicrobes_otu_tax_notnorm_env_11May2023.RData")
```

## 8. normalize

using the rrarefy() function: Function rrarefy generates one randomly rarefied community data frame or vector of given sample size. The sample can be a vector giving the sample sizes for each row. If the sample size is equal to or smaller than the observed number of individuals, the non-rarefied community will be returned. The random rarefaction is made without replacement so that the variance of rarefied communities is rather related to rarefaction proportion than to the size of the sample. updated 29 aug 2022

```{r}
#performing this on pruned at 100, otus with at least 10 reads (from FIRST and only cut in step 3), taxonomy cleaned

otu_to_normalize<-otu_table
num_read<-as.data.frame(colSums(otu_to_normalize))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL

belowthreshold<-which(num_read$reads<10000)
length(belowthreshold)
#44 samples below 10,000 reads - 11May2023

#look at how many samples removed if the threshold is 5000
belowthreshold5000<-which(num_read$reads<5000)
length(belowthreshold5000)
# this is 40, so almost the same, so will go to 10K - 11May2023
rm(belowthreshold5000)

otu_to_normalize_10k<-otu_to_normalize[,-belowthreshold]
ncol(otu_to_normalize_10k)
# 478 - 11May2023 samples remaining

#check those columns were removed visually
remove<-num_read[belowthreshold,] # all of these are between 100 and 10,000
num_read_otu_to_normalize_10000<-as.data.frame(colSums(otu_to_normalize_10k)) # all over 10,000 reads

rm(remove)
rm(num_read_otu_to_normalize_10000)
rm(belowthreshold)
rm(num_read)

#remove OTUs that are now zero (only one)
reads_per_otu<-as.data.frame(rowSums(otu_to_normalize_10k))
names(reads_per_otu)<-"reads"
reads_per_otu$otu<-rownames(reads_per_otu)
rownames(reads_per_otu)<-NULL

lessthan1<-which(reads_per_otu$reads<1)
#no OTUs with reads less than 1 (integer)
rm(lessthan1)
rm(reads_per_otu)





#rarefy!
S <- specnumber(otu_to_normalize_10k) 
length(S)
#[1] 102128 - 11May2023 # observed number of species
(raremax <- min(rowSums(t(otu_to_normalize_10k)))) #minimum reads to normalize to (10292 - 11May2023)
set.seed(061319)
rrare<-rrarefy(t(otu_to_normalize_10k), sample = raremax)
df<-as.data.frame(t(rrare))


pdf("DataCleaning/Rarefied_data_10k_11May2023.pdf")
plot(S, specnumber(t(rrare)), xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")
abline(0, 1)
dev.off()


num_read<-as.data.frame(colSums(df))
names(num_read)<-"reads"
num_read$sample<-rownames(num_read)
rownames(num_read)<-NULL

# count the number of species after rarefaction
otu_counts<-as.data.frame(rowSums(df))
names(otu_counts)<-"reads"
otu_counts$OTU<-rownames(otu_counts)
rownames(otu_counts)<-NULL
zeroOTU<-which(otu_counts$reads<1)
length(zeroOTU)
# [1] 8755 - 11May2023 OTUS that are zero after rarefying.

otu_tab_10knorm<-df[-zeroOTU,]


#check you removed the right OTUS! 

check1<-as.data.frame(rowSums(otu_tab_10knorm))

rm(check1)
rm(df)
rm(num_read)
rm(otu_counts)
rm(otu_to_normalize)
rm(raremax)
rm(S)
rm(zeroOTU)
rm(rrare)

# keep the non-normalized OTU table that is cut off at 10k just in case
write.csv(otu_to_normalize_10k, "DataCleaning/otu_to_normalize_10k_11May2023.csv", row.names=T)
write.csv(otu_tab_10knorm, "DataCleaning/otu_tab_10knorm_11May2023.csv", row.names=T)
save.image("DataCleaning/WyLakeMicrobes_otu_tax_10knorm_env_11May2023.RData")
```

### @ count_track rarefied_final

```{r}
(sum(colSums(otu_tab_10knorm)))
nrow(otu_tab_10knorm)
ncol(otu_tab_10knorm)
# [1] 4919576 - 11May2023 reads
# [1] 93373 - 11May2023 ESVs
# [1] 478 - 11May2023 samples

#percent of samples removed
1-(478/555) #- 11May2023
#[1] 0.1387387 - 11May2023

#number of ESVs retained from before taxa filtering, removing OTUs with low reads, or samples with low reads
93373/122444 #-   11May2023 from line 433 (without water samples, with blanks, with NA/Chloro/mito)
#[1] 0.7625772 - 11May2023
```

### a. ordinate and procrustes

HAVE NOT RERUN 11May2023
if want to, load save.image("DataCleaning/WyLakeMicrobes_otu_tax_10knorm_env_11May2023.RData")

```{r}
require(vegan)
#compared samples greater than 10,000 reads (otu_normalize_10000) and the rarefied or normalized object (final_normalized)

#calculate vegdist
notnorm.dist <- vegdist(wisconsin(t(otu_to_normalize_10k)))
norm.dist <- vegdist(wisconsin(t(otu_tab_10knorm)))

mds.notnorm <- monoMDS(notnorm.dist)
mds.norm <- monoMDS(norm.dist)

procrust <- procrustes(mds.notnorm, mds.norm)
procrust


summary(procrust)

pdf("DataCleaning/Procrustes_error_11May2023.pdf",height=10, width=20)
plot(procrust)
dev.off()


plot(procrust, kind=2)
residuals(procrust)
summary(procrust)
eigenvals(procrust)
plot(procrust, kind=2)
residuals(procrust)


rm(notnorm.dist)
rm(norm.dist)
rm(mds.notnorm)
rm(mds.norm)
rm(procrust)
rm(otu_to_normalize_10k)
rm(otu_table)

```

# Part III: Create Phyloseq

## 1. Update and save the metadata file

```{r}
#update the EG sample name
metadata<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/16S/Wy Lake Microbes_old/Metadata_30July2022.csv",header=T,row.names=1)
rownames(metadata)[which(rownames(metadata)=="EG0318")]<-"EG0318L"
metadata[metadata$cleanedID=="EG0318",]$cleanedID<-"EG0318L"
metadata[metadata$samp_names=="EG0318",]$samp_names<-"EG0318L"


metadata_old<-read.csv("/Users/jordanscheibe/OneDrive - University of Wyoming/LakeSedDNA/Data/Sequence Data/Calder_vsearch/metadata/current/metadata_31Jan2022.csv",header=T, row.names = 1)
metadata_old$samp_names<-rownames(metadata_old)
metadata_old$water_sample_do_avg<-NULL
metadata_old$water_sample_ph_avg<-NULL
metadata_old$water_sample_t_avg<-NULL
metadata_old$bin_depth<-NULL
metadata$bin_depth<-NULL
table(names(metadata_old)==names(metadata))


add<-metadata_old[rownames(metadata_old)%in%c("BN0105L", "ML0104L"),]
metadata<-rbind(metadata,add)
rm(add)
rm(metadata_old)

#do some old checks from old RMD file
metadata[metadata$lake_id=="SR",]$max_lake_depth # should be 4.3
# all good, cut in half
metadata[metadata$lake_id=="BN" & metadata$sample_type=="sediment",]$depth
metadata[metadata$lake_id=="NB" & metadata$sample_type=="sediment",]$depth
metadata[metadata$lake_id=="LS" & metadata$sample_type=="sediment",]$depth

# add in depth bins
sort(unique(metadata$depth))
metadata$bin_depth<-metadata$depth
metadata[metadata$depth==1,]$bin_depth<-2
metadata[metadata$depth==2.5,]$bin_depth<-2
metadata[metadata$depth==3,]$bin_depth<-4
metadata[metadata$depth==5,]$bin_depth<-6
metadata[metadata$depth==7,]$bin_depth<-8
metadata[metadata$depth==9,]$bin_depth<-10
metadata[metadata$depth==11,]$bin_depth<-12
metadata[metadata$depth==13,]$bin_depth<-14
metadata[metadata$depth==15,]$bin_depth<-16
metadata[metadata$depth>26,]$bin_depth<-100
sort(unique(metadata$bin_depth))

#write.csv(metadata,"metadata_29Aug2022.csv")
```

## 2. Make phyloseq

```{r}
load("DataCleaning/WyLakeMicrobes_otu_tax_10knorm_env_11May2023.RData")
metadata<-read.csv("Metadata/metadata_29Aug2022.csv",header=T,row.names = 1)

#change East Glacier mislabeled sample from EG0308L2 to EG0318
names(otu_tab_10knorm)[which(names(otu_tab_10knorm)=="EG0308L2")]<-"EG0318L"
names(otu_table)[which(names(otu_table)=="EG0308L2")]<-"EG0318L"
names(otu_to_normalize_10k)[which(names(otu_to_normalize_10k)=="EG0308L2")]<-"EG0318L"

#check metadata and otu names match 
table(rownames(metadata)%in%names(otu_tab_10knorm)) # all true

#check only sediment
table(metadata$sample_type)

#make sure no empty taxonomies (these should all be NAs with the DADA2 training dataset)
table(tax_tab[,]=="")
#  FALSE 
# 496605 - 11May2023

#covert taxa table to phyloseq sub-object
tax_tab <-as.matrix(tax_tab)
taxa_tab <- tax_table(tax_tab)

#make sure removed - checked again once turned into phyloseq sub-object
table(taxa_tab@.Data=="") # all false

#delete non-normalized otu tables
rm(otu_table)
rm(otu_to_normalize_10k)

#convert metadata and otu table to phyloseq sub-objects
samp_dat <- sample_data(metadata)
otu_tab_norm <- otu_table(otu_tab_10knorm, taxa_are_rows = T)
ps <- phyloseq(otu_tab_norm, samp_dat, taxa_tab)

#remove phyloseq sub-objects
rm(samp_dat)
rm(otu_tab_norm)
rm(taxa_tab)

#transform sample data
ps_tr <- transform_sample_counts(ps, function(x) x / sum(x))
#save final phyloseq object
##save.image("WyLakeMicrobes_Phyloseq_11May2023.RData")
```

# Part III: Data Analysis

## 1. Load data

```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
```


## 2. iNEXT (alpha richness)

```{r}
bio1 <- as.matrix(ps@otu_table@.Data)
bio2 <- split(bio1, as.numeric(rep(1:ncol(bio1), each = nrow(bio1))))
bio3 <- iNEXT(bio2, q=0, datatype="abundance") 

# select asymptotic estimates 
bio4 <- bio3$AsyEst
bio5 <- subset(bio4, bio4$Diversity =="Species richness")  
bio6 <- subset(bio4, bio4$Diversity =="Shannon diversity")  
bio7 <- subset(bio4, bio4$Diversity =="Simpson diversity")  

#create a table with diversity estimates
bio8 <- data.frame(cbind(bio5$Observed, bio6$Observed, bio7$Observed))
rownames(bio8) <- colnames(bio1)
colnames(bio8) <- c("Species_Richness", "Shannon_Diversity","Simpson_Dominance")
head(bio8)
write.table(bio8,file="2023May26_HillNumbersNormalized.txt",sep="\t",row.names=T, col.names=T)
hill_norm<-bio8

rm(bio1)
rm(bio2)
rm(bio3) 
rm(bio4)
rm(bio5)
rm(bio6)
rm(bio7)
rm(bio8)
```


```{r}
#load environment and Hill's number file. 
meta_sed_26<-metadata[metadata$depth<=26,]
hills<-read.delim("2023May26_HillNumbersNormalized.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")

lm<-lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth)
plot(meta_sed_26_hills$depth, meta_sed_26_hills$Species_Richness)
abline(lm)

summary(lm(meta_sed_26_hills$Species_Richness~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Shannon_Diversity~meta_sed_26_hills$depth))
summary(lm(meta_sed_26_hills$Simpson_Dominance~meta_sed_26_hills$depth))

#tried this with a GAM too, but not much different 
Data<-data.frame(x=meta_sed_26_hills$depth,y=meta_sed_26_hills$Species_Richness)
Data<-Data[order(Data$x),]
 dat_gam=gam(y~s(x, k=3), data=Data)
 pred = predict.gam(dat_gam, newdata = Data[1])
 plot(Data$x,Data$y)
 lines(Data$x,pred, col="red", lwd=3) 
summary(dat_gam)
```




## 3. iCAMP community assembly

### a. make phylogenetic tree

#### i. subset ESVS with 10+ reads


```{r}
ESV<-as.data.frame(otu_table(ps))
ESV<-t(ESV)
ESV<-ESV[order(rownames(ESV)),]

table(colSums(ESV)<10)
# FALSE  TRUE  - 27May2023
# 28480 64893
mean(colSums(ESV)) #53 reads on average - 27May2023

lessthan10<-which(colSums(ESV)<10)
ESV_10plusreads<-ESV[,-lessthan10]
dim(ESV_10plusreads)
# 478 28480 - 27May2023
table(colSums(ESV_10plusreads)<10)# all FALSE
```

#### ii. subset fasta file with sequences

in local R, subset fasta file by the ESVs with 10+ reads (normalized reads)

```{r}
seqs<-Biostrings::readDNAStringSet("OriginalFiles/zotus_nonchimeric.fa") 
seqs@ranges@NAMES<-stringr::str_split(seqs@ranges@NAMES,pattern = ";", 3, simplify = T)[,1]

seqs_sub<-seqs[colnames(ESV_10plusreads)]
table(seqs_sub@ranges@NAMES %in% colnames(ESV_10plusreads))
table(colnames(ESV_10plusreads)%in%seqs_sub@ranges@NAMES)
rm(list=ls()[! ls() %in% c("ESV_10plusreads", "seqs_sub" )])
#save.image("WyLakeMicrobes_PhyloseqEnv_forPhylogeneticTree_ESVs10PlusReads_27May2023.RData")
writeXStringSet(seqs_sub,filepath = "ESVs_with10ormorereads_forBeartooth_27May2023.fasta", format = "fasta")
```

copy the fasta file over to the supercomputer

```{bash}
rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/ESVs_with10ormorereads_forBeartooth_27May2023.fasta jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree
```

#### iii. make phylogenetic tree

using clustalo to align and then fasttree to make the tree

align_maketree.sh
```{bash}
#!/bin/bash
#SBATCH --job-name phylotree
#SBATCH --mem=120GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=6
#SBATCH --account=microbiome
#SBATCH --output=phylotree_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load arcc/1.0 miniconda3/4.12.0
conda activate shotgun_env
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree

clustalo -i ESVs_with10ormorereads_forBeartooth_27May2023.fasta -o tempfile_27May2023_muscled.fa -v --threads=6

FastTree -nt tempfile_27May2023_muscled.fa > ESVs_with10ormorereads_output_27May2023_muscled.nwk

```


Copy environment over to supercomputer
```{bash}
rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/WyLakeMicrobes_PhyloseqEnv_forPhylogeneticTree_ESVs10PlusReads_27May2023.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP

```

### b. qpen ESVs with 10+ reads

qpen_function.R
```{r}
require(phyloseq)
require(iCAMP)
require(ape)
load("../WyLakeMicrobes_PhyloseqEnv_forPhylogeneticTree_ESVs10PlusReads_27May2023.RData")
comm<-ESV_10plusreads
tree<-read.tree("../ESVs_with10ormorereads_output_27May2023_muscled.nwk")
table(colnames(ESV_10plusreads)%in%tree[["tip.label"]]) #all TRUE
table(tree[["tip.label"]]%in%colnames(ESV_10plusreads)) #all TRUE


wd0="/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP/qpen"
nworker=6 # parallel computing thread number
rand.time=1000 # usually use 1000 for real data.
  

  # for a big dataset, pdist.big may be used
  save.wd="/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP/qpen/pdbig.qpen"
  print(save.wd)
  # please change to the folder you want to save the pd.big output.
  
  pd.big=pdist.big(tree = tree, wd=save.wd, nworker = nworker)
  qp2=qpen(comm=comm, pd=pd.big$pd.file, pd.big.wd=pd.big$pd.wd,
           pd.big.spname=pd.big$tip.label, tree=tree,
           rand.time=rand.time, nworker=nworker)
  setwd(wd0)
save.image("WyLakeMicrobes_env_27May2023_qpen_ESVswith10plusreads_results.RData")
```

run_qpen_function.sh
```{bash}
#!/bin/bash
#SBATCH --job-name qpen_10plus
#SBATCH --mem=120GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=6
#SBATCH --account=microbiome
#SBATCH --output=qpen_10plus_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP/qpen

srun Rscript qpen_function.R
echo "srun Rscript qpen_function.R"

echo "finished qpen iCAMP - JVE"
date
```

copy back to desktop
```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/iCAMP/qpen/WyLakeMicrobes_env_27May2023_qpen_ESVswith10plusreads_results.RData /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes
```


### c. qpen output - ESVs with 10plus reads

#### i) start here
I reran this quickly May 29, 2023 but it all looks the same, could look at again
```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
load("WyLakeMicrobes_env_27May2023_qpen_ESVswith10plusreads_results.RData")

#pull out results from iCAMP
qpn<-qp2$result[,c("sample1","sample2","process")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"process[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)%in%metadata$samp_names) #all true
table(colnames(dfr)%in%metadata$samp_names) # all true

#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

table(colnames(dfr)==metadata$samp_names) # all true
table(colnames(dfr)==rownames(metadata_sub)) # all true 

daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))
table(colnames(daisy.mat)==colnames(dfr)) # all true
table(rownames(daisy.mat)==rownames(dfr)) # all true

daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

process<-melt(as.matrix(dfr), na.rm=T)

table(process$Var1==env_dist$Var1) # all true
table(process$Var2==env_dist$Var2) # all true 
process$env_dist<-env_dist$value

pairwise<-process #overwrite original file

names(pairwise)[1]<-"s1_samp_names"
pairwise$s1_samp_names<-as.character(pairwise$s1_samp_names)
names(pairwise)[2]<-"s2_samp_names"
pairwise$s2_samp_names<-as.character(pairwise$s2_samp_names)
names(pairwise)[3]<-"process"

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

pairwise<-merge(pairwise, metadata_s1, by="s1_samp_names")
pairwise<-merge(pairwise, metadata_s2, by="s2_samp_names")

#remove metadata notes
pairwise$s1_notes<-NULL
pairwise$s2_notes<-NULL
pairwise$s1_Notes_sed_water_wt<-NULL
pairwise$s2_Notes_sed_water_wt<-NULL
pairwise$s1_Notes_carbon_nitrogen<-NULL
pairwise$s2_Notes_carbon_nitrogen<-NULL
pairwise$s1_Notes._lake_sed_pH<-NULL
pairwise$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
pairwise$abs_cm<-rep(NA, nrow(pairwise))
i=1
for(i in 1:nrow(pairwise)){
  ifelse(pairwise$s1_lake_drive[i]==pairwise$s2_lake_drive[i],pairwise$abs_cm[i]<-abs(pairwise$s2_depth[i]-pairwise$s1_depth[i]),NA)
}

# calculate geographic distance

require(geosphere)
for(i in 1:nrow(pairwise)){
        pairwise$geo_dist[i]<-as.numeric(distm(data.frame(X = pairwise$s1_longitude[i], Y = pairwise$s1_latitude[i]),data.frame(X = pairwise$s2_longitude[i], Y = pairwise$s2_latitude[i])))
}

pairwise$geo_dist_km<-pairwise$geo_dist/1000

write.csv(pairwise, "qpen_10plusESVs_allcomparisons_29May2023.csv")
```

#### ii) environmental distance within each zone

```{r}
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_29May2023.csv", header=T, row.names=1)

comps<-comps_backup

comps$d_val<-comps$env_dist

#make breaks for zone A
comps_tmp<-comps[comps$s1_zone=="A" & comps$s2_zone=="A",]

comps_tmp<-comps_tmp[comps_tmp$d_val!=0,]
comps_tmp$d_val_group_breaks<-cut2(comps_tmp$d_val, g=10)
levels(comps_tmp$d_val_group_breaks)
# [1] "[0.0162,0.117)" "[0.1167,0.163)" "[0.1632,0.210)" "[0.2097,0.240)" "[0.2399,0.276)"
#  [6] "[0.2759,0.323)" "[0.3234,0.384)" "[0.3835,0.454)" "[0.4543,0.540)" "[0.5399,0.722]"
breaks<-c(as.numeric(gsub("\\]","" ,gsub("\\[","",unlist(str_split(as.character(levels(comps_tmp$d_val_group_breaks)), pattern=",")))[c(seq(1, 20, 2),20)])))

#make sure first value is below the minimum env_distance measure that isn't 0
min(comps_backup[comps_backup$env_dist!=0,]$env_dist)
#[1] 0.01621537

# make a bottom break (0.016 - just below the lowest env_dist) that is above zero but below the first break
breaks<-c(0.016,breaks[2:11]) 

full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)

zone<-c("A","B","C")
j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

#breaks_labels<- levels(comps$d_val_group_breaks)
breaks_labels<-c("[0, 0]", "[0.016, 0.12)", "[0.12, 0.16)", "[0.16, 0.21)", "[0.21, 0.24)",
 "[0.24, 0.28)", "[0.28, 0.32)", "[0.32, 0.38)", "[0.38, 0.45)",
"[0.45, 0.54)" ,"[0.54, 0.72]")

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="env"))
  }
  }

full_output<-rbind(full_output,tally)
}
#check to make sure they add to one
range(full_output$n)
#[1]  171 2020 #29May2023
mean(full_output$n)
#947.6364 #29May2023


#write.csv(full_output, "qpen_10plusESVs_envdist_29May2023.csv")

```

#### iii) geographic distance

```{r}
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_29May2023.csv", header=T, row.names=1)
comps<-comps_backup


comps[comps$s1_lake_id==comps$s2_lake_id & comps$s1_lake_drive!=comps$s2_lake_drive,]$geo_dist_km<-0.001 #add one meter for cores in the same lake
comps$d_val<-comps$geo_dist_km
breaks<-c(seq(0,15,5), seq(100, 300, 200), seq(400,500,100))  ##nothing from  43.30254  174.6661
breaks<-c(0,0.1, breaks[2:8])
full_output<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)


zone<-c("A","B","C")
j=1
for( j in 1:3) {
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]

sub<-comps_sub[comps_sub$d_val==0,]
sub$d_val_group<-rep(0, nrow(sub))

comps_sub<-comps_sub[comps_sub$d_val!=0,]
comps_sub$d_val_group<-as.numeric(cut(comps_sub$d_val, breaks=breaks))
comps_sub<-rbind(sub, comps_sub)

breaks_labels<- c("[0,0]",levels(cut(comps$d_val, breaks=breaks)))

tmp<-comps_sub
tally<-data.frame(n=NULL,process=NULL,percent=NULL,d_val_group=NULL, d_val_inclthisnumandbelow=NULL, zone=NULL,dist=NULL)
proc<-unique(comps_backup$process)
groups<-sort(unique(tmp$d_val_group))
i=1
p=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  for(p in 1:length(proc)){
    tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2),d_val_group=groups[i], d_val_inclthisnumandbelow=breaks[i],zone=zone[j], dist="geo"))
  }
  }
full_output<-rbind(full_output,tally)
#count the number of comparisons in each zone
}

range(full_output$n)
#[1]  75 2604 # 29May2023
mean(full_output$n)
#1158.222 - 29May2023
print(breaks_labels)

#write.csv(full_output, "qpen_10plusESVs_geodist_29May2023.csv")
```

#### iv) combine the two dataframes

```{r}
geo_dist_tally<-read.csv("qpen_10plusESVs_geodist_29May2023.csv", header=T, row.names=1)
env_dist_tally<-read.csv("qpen_10plusESVs_envdist_29May2023.csv", header=T, row.names=1)
full_output<-rbind(geo_dist_tally,env_dist_tally)

full_output[full_output$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
full_output[full_output$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
full_output[full_output$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
full_output[full_output$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
full_output[full_output$process=="Undominated",]$process<-"C_Drift"


#add in color
full_output$zone_col<-rep('#018571',nrow(full_output))
full_output[full_output$zone=="B",]$zone_col<-"#C4AD79"
full_output[full_output$zone=="C",]$zone_col<-'#a6611a'
        


#add in xlab as dist column
full_output[full_output$dist=="env",]$dist<-'Environmental dissimilarity'
full_output[full_output$dist=="geo",]$dist<-'Distance (km)'

#write.csv(full_output,"qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May023.csv")

#if count needed
count<-unique(full_output[,c("d_val_group","zone","n","dist")])
#write.csv(count, "qpen_10plusESVs_count_summary_29May2023.csv")
```

#### v) plot env and geo dist

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)
full_output$proportion<-full_output$percent
full_output$percent<-full_output$percent*100

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
#new_colors<-c("#CCCCCC","#B0E2FF", "#36648B")

break_labels<-list()
break_labels[[1]]<-c( "0",">0 to 0.1" ,  ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0", "0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")

#full_output<-full_output[full_output$process%in%sort(unique(full_output$process))[3:5],]

ggplt<-list()
i=1
j=1
for(j in 1:3){
        sub<-full_output[full_output$zone==sort(unique(full_output$zone))[j],]
for(i in 1:2){
        sub2<-sub[sub$dist==unique(sub$dist)[i],]       
ifelse(j==3, 
       
       
       ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) +  geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")  + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15)) +
                theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       
       , 
       
       
              ggplt[[length(ggplt)+1]] <- ggplot(sub2,            
               aes(x = d_val_group,
                   y = percent,
                   color= process)) + geom_point(size=3)+ theme_bw()   + xlab(label = sub2$dist[1])+ scale_x_continuous(breaks = sort(unique(sub2$d_val_group)), labels=break_labels[[i]])+ ylab(label = "") + geom_line(linewidth=2) +   labs(color="Assembly process")   + scale_color_manual(values=new_colors)+ theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) +theme(axis.text.x = element_text(angle = 60,hjust=1))+      
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
        axis.text=element_text(size=13), #change font size of axis text
        axis.title=element_text(size=15), #change font size of axis titles
        plot.title=element_text(size=14), #change font size of plot title
        legend.text=element_text(size=14), #change font size of legend text
        legend.title=element_text(size=15))+ #change font size of legend title
        theme(axis.title.x=element_blank(),
        axis.text.x=element_blank()) +
        theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank())#change font size of legend title
       )
        }}



pdf("Figures/CommAssemProc_EnvDist_GeoDist_29May2023.pdf",height=7,width=7)
ggplt[[1]]+ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ 
  plot_layout(ncol = 2)
dev.off()


```

#### vi) plot sediment distance

```{r}

qpn<-read.csv("qpen_10plusESVs_allcomparisons_29May2023.csv", header=T, row.names=1)

#subset by samples within the same core
downcore<-qpn[is.na(qpn$abs_cm)==F & qpn$abs_cm<27 ,]


downcore$d_val<-abs(downcore$abs_cm)
downcore$d_val_group_breaks<-cut2(downcore$d_val, g=10)
downcore$d_val_group<-as.numeric(cut2(downcore$d_val, g=10))
levels(downcore$d_val_group_breaks)
break_labels<- c("0 to <2.5" ,"2.5 to <5" ,"5 to <7", "7 to <9" ,"9 to <11" ,"11 to <15"
,"15 to <18" ,"18 to <22" ,"22 to <26")
#these are put into 9 groups here instead of the requested 10, must be to make it even.

tmp<-downcore
tally<-data.frame(process=NULL,percent=NULL, d_val_group=NULL,n=NULL)
groups<-sort(unique(downcore$d_val_group))
i=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  processes<-as.data.frame(table(tmp2$process)/nrow(tmp2))
  colnames(processes)[1]<-"process"
  colnames(processes)[2]<-"percent"
  processes$process<-as.character(processes$process)
  processes$d_val_group<-rep(groups[i],nrow(processes))
  processes$n<-rep(nrow(tmp2),nrow(processes))
  tally<-rbind(tally,processes)
}
  


tally$proportion<-tally$percent
tally$percent<-tally$percent*100

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

        ggplt <- ggplot(tally,            
                        aes(x = d_val_group,
                            y = percent,
                            color= process)) +  geom_line(linewidth=2) +geom_point(size=3) + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
                                                                                                                             labels=break_labels) + xlab(label = "Vertical sediment\ndistance (cm)")+ ylab(label = "Proportion (%)") + geom_line(linewidth=2) +   labs(color="Assembly process")    + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,75) + theme(axis.text.x = element_text(angle = 45,hjust=1))+ guides(color = guide_legend(override.aes = list(linewidth = 6))) + theme(legend.position="none")+
                theme(axis.text=element_text(color="black"))+
                theme(axis.text.x = element_text(angle = 60,hjust=1))+
                theme(text=element_text(size=14), #change font size of all text
                      axis.text=element_text(size=14), #change font size of axis text
                      axis.title=element_text(size=15), #change font size of axis titles
                      plot.title=element_text(size=14), #change font size of plot title
                      legend.text=element_text(size=14), #change font size of legend text
                      legend.title=element_text(size=15),
                      panel.grid.minor = element_blank(), 
                      panel.grid.major.x = element_blank()) #change font size of legend title 

pdf("Figures/CommAssemProc_SedDist_29May2023.pdf", height=3.05, width=3.3)
ggplt
dev.off()

unique(tally$n)
#[1] 372 352 302 283 239 403 163 236 190

#write.csv(tally,"qpen_10plusESVs_seddist_29June2023.csv")

```

sediment distance GAM

sediment distance (with zero)
need to run most of section before (don't need to replot)
```{r}
model_results<-data.frame(process=NULL, devex=NULL, p=NULL)

#Plot the sediment depth GAM for each process
pdf("Figures/seddepth_GAM_29June2023.pdf", height=3, width=10)
par(mfrow=c(1,5),mar=c(6,4,2,1))
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="black", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels)),1), las=2, labels = break_labels)
title(xlab=c("","","Sediment distance (cm)","","")[i], cex.lab=2, line=4)
title(ylab=c("Proportion","","","","")[i], cex.lab=2)

Data<-data.frame(x=sub$d_val_group, y=sub$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=new_colors[i], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(new_colors[i], alpha.f = 0.40), border = NA)
        gam_res<-summary(Gam)
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.9, paste("Dev exp: ",round(gam_res$dev.expl, digits=3), sep=""))
        text(x=max(Data$x)*0.8, y=max(Data$y)*0.8, paste("P-val: ",signif(gam_res$s.pv, digits=3), sep=""))
        model_results<-rbind(model_results,data.frame(process=sort(unique(tally$process))[i], devex=round(gam_res$dev.expl,digits=3), p=gam_res$s.pv))
}
dev.off()

round(mean(model_results$devex),digits=3)
#[1] 0.914 # 29Jan2023
```


plot legend
```{r}
require(cowplot)
library(ggplot2) 
library(grid)
library(gridExtra) 
# Using the cowplot package
legend <- cowplot::get_legend(ggplt)

grid.newpage()
grid.draw(legend)
```


#### vii) GAMS

doesn't include comparisons within the same lake 8Feb2023 
the labeling for geographic distance is off (double check if going to put in supplementary figures)
29June2023 - can't remember logic for why I removed 0 from the GAMs..

```{r}
require(mgcv)
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)

break_labels<-list()
break_labels[[1]]<-c( ">0.1 to 5"  , ">5 to 10" , ">10 to 15"  ,">15 to 100" , ">100 to 300", ">300 to 400" ,">400 to 500")
break_labels[[2]]<-c("0.01 to <0.12", "0.12 to <0.16", "0.16 to <0.21", "0.21 to <0.24",
 "0.24 to <0.28", "0.28 to <0.32", "0.32 to <0.38", "0.38 to <0.45",
"0.45 to <0.54" ,"0.54 to 0.72")

full_output<-full_output[full_output$d_val_group!=0,] #GAMs without 0 (without comparisons from the same lake)
tmp1<-full_output[full_output$dist=="Environmental dissimilarity",]
tmp2<-full_output[full_output$dist=="Distance (km)",]
tmp2<-tmp2[tmp2$d_val_group!=1,]
full_output<-rbind(tmp2,tmp1)


model_results<-data.frame(dist=NULL, process=NULL, zone=NULL, rsq=NULL, p=NULL)

d=1
for (d in 1:2){
pdf(paste("Figures/Fig7_qpen_",unique(full_output$dist)[d],"_GAM_8Feb2023.pdf",sep=""), height=3.5, width=12)
par(mfrow=c(1,5),mar=c(8,4,2,1))
tally<-full_output[full_output$dist==unique(full_output$dist)[d],]
i=1
for(i in 1:5){
sub<-tally[tally$process==sort(unique(tally$process))[i],]
plot(sub$d_val_group,sub$percent, main=stringr::str_split(sort(unique(tally$process))[i],pattern="_")[[1]][2],ylab="", xlab="", pch=16, ylim=c(min(sub$percent),max(sub$percent)),col="white", xaxt="n")
axis(side=1,cex=0.8, at=seq(1,(length(break_labels[[d]])),1), las=2, labels = break_labels[[d]])
title(xlab=c("","",unique(full_output$dist)[d],"","")[i], cex.lab=2, line=6)
title(ylab=c("Proportion","","","","")[i], cex.lab=2, line =2)
x=1
for(x in 1:3){
sub2<-sub[sub$zone==unique(sub$zone)[x],]
points(sub2$d_val_group,sub2$percent, pch=16, col=sub2$zone_col[x])

#GAM
Data<-data.frame(x=sub2$d_val_group, y=sub2$percent)
Data<-Data[order(Data$x),]
 Gam=gam(y~s(x, k=3), data=Data)
        pred = predict.gam(Gam, newdata = Data[1], se.fit=T)
        lines(Data$x,pred$fit, col=sub2$zone_col[x], lwd=3) 
        polygon(x = c(Data$x, rev(Data$x)),
        y = c( pred$fit + (2 * pred$se.fit), 
              rev(pred$fit - (2 * pred$se.fit))),
        col =  adjustcolor(sub2$zone_col[x], alpha.f = 0.20), border = NA)
        gam_res<-summary(Gam)
        model_results<-rbind(model_results,data.frame(dist=unique(full_output$dist)[d], process=sort(unique(tally$process))[i], zone=unique(sub$zone)[x], rsq=round(gam_res$r.sq,digits=3), p=gam_res$s.pv))
        
}}
dev.off()
}
write.csv(model_results, "qPEN_CommunityAssembly_GAMS_without0_29June2023.csv")

```


#### viii) stats for manuscript
```{r}
qpn<-read.csv("qpen_10plusESVs_allcomparisons_29May2023.csv", header=T, row.names=1)
(table(qpn$process))/nrow(qpn)

 #   Dispersal.Limitation Heterogeneous.Selection   Homogeneous.Selection 
 #             0.34732419              0.22467830              0.39389314 
 # Homogenizing.Dispersal             Undominated 
 #             0.01335930              0.02074507 

#selection
0.22467830 + 0.39389314


#stochastic
0.01335930 + 0.34732419 + 0.02074507

```

fig 6 A (sed distance)

```{r}
full_output<-read.csv("qpen_10plusESVs_seddist_29June2023.csv", header=T,row.names=1)
i=1
for( i in 1:5){
sub<-full_output[full_output$d_val_group==1 & full_output$process==sort(unique(full_output$process))[i],]
print(sub$process[1])
print(signif(sub$proportion, digits=3)*100)
}

i=1
for( i in 1:5){
sub<-full_output[full_output$d_val_group%in%c(8,9) & full_output$process==sort(unique(full_output$process))[i],]
print(sub$process[1])
print(signif(sub$proportion, digits=3)*100)
}

```

Within an individual sediment core, samples â¤ 2.5 cm apart were either homogenized (homogenous selection, 71.5%) or differentiated by selection (variable selection, 5.1%), regardless of horizon. As the distance between samples increased, homogeneous selection declined and variable selection increased (Fig. 6, A). At sediment distances > 18 cm (18-22 and 22-26 cm), where all pairs of samples were compared across horizons, variable selection predominated (29.7-39.5%). Homogenizing dispersal (mass effects) was highest between comparisons 0-7 cm apart (15.3-21.5%), and declined to 5.3% with increasing sediment distance. Dispersal limitation increased with sediment distance, reaching 26.8% at 22-26 cm. Drift played a minor role (< 9%) across all sediment distances.

figure 6 B and C

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)

#1) same lake (environment)

#Homogenous selection
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="B_Homogeneous selection", ]$percent), digits=3)*100


#variable selection
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="A_Variable selection", ]$percent), digits=3)*100


#disp lim
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="E_Dispersal limitation", ]$percent), digits=3)*100


#homog disp
signif(mean(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%in%c(0) & full_output$process=="D_Homogenizing dispersal", ]$percent), digits=3)*100


#or signif
# 61.5
# 7.19
# 8.49
# 19.7


```

Manuscript text:

Homogeneous selection dominated comparisons within the same horizon within any given lake (mean across horizons, 61.5%), followed by homogenizing dispersal (19.7%), variable selection (7.19%), and dispersal limitation (8.49%) (Fig. 6, B and C).

inter-lake comparisons (averaged across environmental distance and geographic distance)

```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)

'%!in%' <- function(x,y)!('%in%'(x,y))

for( i in 1:5){

sub<-rbind(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%!in%c(0) & full_output$process==sort(unique(full_output$process))[i],]
      , 
      full_output[full_output$dist=="Distance (km)" & full_output$d_val_group%!in%c(0,1) & full_output$process==sort(unique(full_output$process))[i],])

print(sub$process[1])
print(signif(mean(sub$percent),digits=3)*100)
print(signif(range(sub$percent),digits=3)*100)
}


```

[1] "A_Variable selection" [1] 14.2 [1]  3.17 28.20
[1] "B_Homogeneous selection"[1] 47.7 [1] 26.8 72.0
[1] "C_Drift" [1] 2.77 [1]  0.0 10.9
[1] "D_Homogenizing dispersal" [1] 1.32 [1] 0.00 6.67
[1] "E_Dispersal limitation" [1] 34 [1] 17.5 47.2



As environmental dissimilarity and geographic distance increased with inter-lake comparisons, homogenizing dispersal declined while variable selection and dispersal limitation increased. Regardless, homogeneous selection often remained the most dominant community assembly process in all horizons. Homogeneous selection ranged from 26.8-72.0% (avg. 47.7%), variable selection 3.17-28.20% (avg. 14.2%), and dispersal limitation 17.5-47.2% (avg. 34.0%). Homogenizing dispersal and drift acting alone had negligible effects (avg. 1.32 and 2.77%, respectively) across comparisons of abiotic lake environments and geographic distances.


```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)

'%!in%' <- function(x,y)!('%in%'(x,y))
z=1
for(z in 1:3){
    sub<-full_output[full_output$zone==c("A","B","C")[z],]  

sub2<-rbind(sub[sub$dist=="Environmental dissimilarity" & sub$d_val_group%!in%c(0) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],]
      , 
      sub[sub$dist=="Distance (km)" & sub$d_val_group%!in%c(0,1) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],])

 
print(sub2$zone[1])
print(signif(range(sub2$percent),digits=3)*100)
}
```
In the redox horizon, the contributions of homogenous selection, variable selection, and dispersal limitation explained a comparable proportion of comparisons (11.8-47.0%).


```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)

'%!in%' <- function(x,y)!('%in%'(x,y))
z=1
i=1
for( i in 1:5){
sub<-rbind(full_output[full_output$dist=="Environmental dissimilarity" & full_output$d_val_group%!in%c(0) & full_output$process==sort(unique(full_output$process))[i],]
      , 
      full_output[full_output$dist=="Distance (km)" & full_output$d_val_group%!in%c(0,1) & full_output$process==sort(unique(full_output$process))[i],])

for(z in 1:3){
    sub2<-sub[sub$zone==c("A","B","C")[z],]    


print(sub2$process[1])
print(sub2$zone[1])
print(signif(mean(sub2$percent),digits=3)*100)
print(signif(range(sub2$percent),digits=3)*100)
}}
```

[1] "B_Homogeneous selection"
[1] "A"
[1] 38.5

[1] "B_Homogeneous selection"
[1] "B"
[1] 45.8

[1] "B_Homogeneous selection"
[1] "C"
[1] 58.9



As environmental dissimilarity and geographic distance increased between lakes, homogenous selection increased across horizons from, on average, 38.5% in the redox horizon to 58.9% in the depauperate horizon (Fig. 6, B, C).



```{r}
full_output<-read.csv("qpen_10plusESVs_community_assembly_env_geo_dist_summary_29May2023.csv", header=T, row.names = 1)

'%!in%' <- function(x,y)!('%in%'(x,y))
i=1
    sub<-full_output[full_output$zone=="C",]
sub2<-rbind(sub[sub$dist=="Environmental dissimilarity" & sub$d_val_group%!in%c(0) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],]
      , 
      sub[sub$dist=="Distance (km)" & sub$d_val_group%!in%c(0,1) & sub$process%in%sort(unique(sub$process))[c(1,2,5)],])

hs_de<-NULL
hs_vs<-NULL
for (i in 1:length(unique(sub2$d_val_group))){
 sub3<-sub2[sub2$d_val_group==sort(unique(sub2$d_val_group))[i],]

hs_de<-c(hs_de,sub3[sub3$process=="B_Homogeneous selection",]$percent/ sub3[sub3$process=="E_Dispersal limitation",]$percent)
hs_vs<-c(hs_vs,sub3[sub3$process=="B_Homogeneous selection",]$percent/ sub3[sub3$process=="A_Variable selection",]$percent)
    
}
        print(sub3$zone[1])
          print("hs_de")
print(signif(mean(hs_de),digits=3))   
print(signif(range(hs_de),digits=3)) 
          print("hs_vs")
print(signif(mean(hs_vs),digits=3))  
print(signif(range(hs_vs),digits=3)) 

```

[1] "C"

[1] "hs_de"
 
[1] 1.99

[1] 1.28 4.11

[1] "hs_vs"

[1] 8.17

[1]  3.97 17.80

 In the depauperate horizon, homogeneous selection explained the differences between assemblages twice as often as dispersal limitation, on average, and eight times as often as variable selection.






#### ix) mantel tests of RCBray values

These all need to be dist objects

```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
load("WyLakeMicrobes_env_27May2023_qpen_ESVswith10plusreads_results.RData")
require(cluster)
require(stringr)
require(reshape2)
require(geosphere)
require(Hmisc)
require(patchwork)
require(Matrix)
require(vegan)


#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","RC")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"RC[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)%in%metadata$samp_names) #all true
table(colnames(dfr)%in%metadata$samp_names) # all true

RCbray<-as.dist(dfr)

#now add in environmental distances (lake bottom water and lake depth)
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
metadata_sub<-metadata[,names(metadata)%in%variables]


table(colnames(dfr)==metadata$samp_names) # all true
table(colnames(dfr)==rownames(metadata_sub)) # all true 

env_dist <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))
table(labels(RCbray)==labels(env_dist))


# geographic distance

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
geo_dist<-as.dist(dist_m_output)

#check to make sure all labels match!
table(labels(RCbray)==labels(env_dist)) #all true
table(labels(RCbray)==labels(geo_dist)) #all true

mantel(RCbray, env_dist)
mantel(RCbray, geo_dist)

#add partial mantel test, spatially structured environmental variables
mantel.partial(RCbray, env_dist, geo_dist, method = "pearson", permutations = 999)
mantel.partial(RCbray, geo_dist, env_dist, method = "pearson", permutations = 999)
```

Call:
mantel(xdis = RCbray, ydis = env_dist) 

Mantel statistic r: 0.2452 
      Significance: 0.001 

Upper quantiles of permutations (null model):
    90%     95%   97.5%     99% 
0.00974 0.01260 0.01558 0.01895 
Permutation: free
Number of permutations: 999



Call:
mantel(xdis = RCbray, ydis = geo_dist) 

Mantel statistic r: 0.2039 
      Significance: 0.001 

Upper quantiles of permutations (null model):
   90%    95%  97.5%    99% 
0.0116 0.0155 0.0191 0.0237 
Permutation: free
Number of permutations: 999




Call:
mantel.partial(xdis = RCbray, ydis = env_dist, zdis = geo_dist,      method = "pearson", permutations = 999) 

Mantel statistic r: 0.2425 
      Significance: 0.001 

Upper quantiles of permutations (null model):
   90%    95%  97.5%    99% 
0.0101 0.0131 0.0150 0.0171 
Permutation: free
Number of permutations: 999




Call:
mantel.partial(xdis = RCbray, ydis = geo_dist, zdis = env_dist,      method = "pearson", permutations = 999) 

Mantel statistic r: 0.2005 
      Significance: 0.001 

Upper quantiles of permutations (null model):
   90%    95%  97.5%    99% 
0.0113 0.0148 0.0187 0.0228 
Permutation: free
Number of permutations: 999



#### x) deeper comparisons

```{r}
require(ggplot2)

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")
comps_backup<-read.csv("qpen_10plusESVs_allcomparisons_29May2023.csv", header=T, row.names=1)
comps<-comps_backup
comps$d_val<-comps$abs_cm
comps<-comps[is.na(comps$d_val)==F,]
comps<-comps[comps$d_val>26,] # 119 comparisons, so no many compared to the other ones

tmp2<-comps
tally<-data.frame(n=NULL,process=NULL,percent=NULL)
proc<-unique(comps_backup$process)
for(p in 1:5){
        tally<-rbind(tally,data.frame(n=nrow(tmp2),process=proc[p],percent=nrow(tmp2[tmp2$process==proc[p],])/nrow(tmp2)))
}
tally$group<-rep("deep", nrow(tally))

EAP_plot<-ggplot(tally, aes(x = group, y = percent , fill = process)) +
        geom_bar(stat="identity") + scale_fill_manual(values=new_colors,labels= c("Variable selection", "Homogeneous selection", "Drift", "Homogenizing dispersal", "Dispersal limitation" )) + guides(fill=guide_legend(title="Assembly process"))
pdf("Figures/SupplementaryFigures/SFig2_deep_community_assembly_6July2023.pdf", height=4, width=4)
EAP_plot
dev.off()

```



#### xi) CAP sediment characteristics

```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
load("WyLakeMicrobes_env_27May2023_qpen_ESVswith10plusreads_results.RData")


#can reaload data from here, or just use the qp2 object
qpn<-qp2$result[,c("sample1","sample2","process")]
names(qpn)[1]<-"s1_samp_names"
names(qpn)[2]<-"s2_samp_names"


dfr <- reshape(qpn, direction="wide", idvar="s1_samp_names", timevar="s2_samp_names")
rownames(dfr)<-dfr$s1_samp_names
dfr$s1_samp_names<-NULL
colnames(dfr)<-str_split_fixed(colnames(dfr),"process[.]",n=2)[,2]
dfr$SV0426L<-rep(NA,nrow(dfr))
dfr[nrow(dfr)+1,] <- NA
rownames(dfr)[478]<-"33_1_10_DNA"

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
metadata<-metadata[order(match(rownames(metadata),colnames(dfr))),]
table(colnames(dfr)==metadata$samp_names) # all true


#now select the sediment characteristics to examine
#using objects from above
variables<-c("pH"
, "d_13_c"  
, "cn"
, "sulfur_perc"     
, "water_perc"     
, "protein_per") 

#pull out metadata

metadata_sub<-metadata[,names(metadata)%in%variables]

table(colnames(dfr)==rownames(metadata_sub)) # all true 


daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))
table(colnames(daisy.mat)==colnames(dfr)) # all true
table(rownames(daisy.mat)==rownames(dfr)) # all true


daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
#there are a number of samples that don't have the data specified in the variables, therefore they are listed as NA and removed here.
env_dist<-melt(daisy.mat, na.rm=TRUE)

env_dist_reexpand <- reshape(env_dist, direction="wide", idvar="Var1", timevar="Var2")
rownames(env_dist_reexpand)<-env_dist_reexpand$Var1
env_dist_reexpand$Var1<-NULL
colnames(env_dist_reexpand)<-str_split_fixed(colnames(env_dist_reexpand),"value[.]",n=2)[,2]

#figure out which ones were different
setdiff(rownames(env_dist_reexpand),colnames(env_dist_reexpand))
setdiff(colnames(env_dist_reexpand),rownames(env_dist_reexpand))
env_dist_reexpand$SV0226L<-rep(NA,nrow(env_dist_reexpand))
env_dist_reexpand[nrow(env_dist_reexpand)+1,] <- NA
rownames(env_dist_reexpand)[nrow(env_dist_reexpand)]<-"33_1_10_DNA"

env_dist_reexpand<-env_dist_reexpand[order(row.names(env_dist_reexpand)), ]
table(rownames(env_dist_reexpand)==colnames(env_dist_reexpand)) #all true

#how many NAs in each and overlap for metadata
#metadata_sub[,metadatacolnames(env_dist_reexpand)]
met<-metadata_sub[rownames(metadata_sub)%in% colnames(env_dist_reexpand),]
#pH, cn and percent water are present in most samples, but then half have d13c and half the sulfur and protein percent

#subset the process/dfr data frame by the comparisons in metadata
dfr<-dfr[colnames(env_dist_reexpand),colnames(env_dist_reexpand)]

dfr<-dfr[order(row.names(dfr)), ]
table(rownames(dfr)==colnames(dfr)) #all true
table(rownames(dfr)==rownames(env_dist_reexpand)) #all true
table(colnames(dfr)==colnames(env_dist_reexpand)) #all true

#melt back to pairwise comparisons
env_dist<-melt(as.matrix(env_dist_reexpand))
process<-melt(as.matrix(dfr))


table(process$Var1==env_dist$Var1) # all true
table(process$Var2==env_dist$Var2) # all true 
process$env_dist<-env_dist$value
process<-process[complete.cases(process),]


pairwise<-process

names(pairwise)[1]<-"s1_samp_names"
pairwise$s1_samp_names<-as.character(pairwise$s1_samp_names)
names(pairwise)[2]<-"s2_samp_names"
pairwise$s2_samp_names<-as.character(pairwise$s2_samp_names)
names(pairwise)[3]<-"process"

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

pairwise<-merge(pairwise, metadata_s1, by="s1_samp_names")
pairwise<-merge(pairwise, metadata_s2, by="s2_samp_names")

#remove metadata notes
pairwise$s1_notes<-NULL
pairwise$s2_notes<-NULL
pairwise$s1_Notes_sed_water_wt<-NULL
pairwise$s2_Notes_sed_water_wt<-NULL
pairwise$s1_Notes_carbon_nitrogen<-NULL
pairwise$s2_Notes_carbon_nitrogen<-NULL
pairwise$s1_Notes._lake_sed_pH<-NULL
pairwise$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
pairwise$abs_cm<-rep(NA, nrow(pairwise))
i=1
for(i in 1:nrow(pairwise)){
  ifelse(pairwise$s1_lake_drive[i]==pairwise$s2_lake_drive[i],pairwise$abs_cm[i]<-abs(pairwise$s2_depth[i]-pairwise$s1_depth[i]),NA)
}

# calculate geographic distance

require(geosphere)
for(i in 1:nrow(pairwise)){
        pairwise$geo_dist[i]<-as.numeric(distm(data.frame(X = pairwise$s1_longitude[i], Y = pairwise$s1_latitude[i]),data.frame(X = pairwise$s2_longitude[i], Y = pairwise$s2_latitude[i])))
}

pairwise$geo_dist_km<-pairwise$geo_dist/1000

write.csv(pairwise, "qpen_10plusESVs_sediment_characteristics_10July2023.csv")

```

plot all zones together

```{r}

#qpn<-read.csv("qpen_10plusESVs_sediment_characteristics_10July2023.csv", header=T, row.names=1)
#subset by samples within the same core
downcore<-qpn[is.na(qpn$abs_cm)==F & qpn$abs_cm<27 ,]
# we don't want to subset only samples within the same core, but really just want samples that are comparable ANYWHERE
downcore<-qpn

downcore$d_val<-downcore$env_dist
downcore$d_val_group_breaks<-cut2(downcore$d_val, g=10)
downcore$d_val_group<-as.numeric(cut2(downcore$d_val, g=10))
levels(downcore$d_val_group_breaks)
break_labels<- c("0 to <0.03" ,"0.03 to <0.05" ,"0.05 to <0.06", "0.06 to <0.08" ,"0.08 to <0.10" ,"0.10 to <0.13"
,"0.13 to <0.15" ,"0.15 to <0.20" ,"0.20 to <0.28", "0.28 to 1")
# [ includes ( up to 

tmp<-downcore
tally<-data.frame(process=NULL,percent=NULL, d_val_group=NULL,n=NULL)
groups<-sort(unique(downcore$d_val_group))
i=1
for(i in 1:length(groups)){
  tmp2<-tmp[tmp$d_val_group==groups[i],]
  processes<-as.data.frame(table(tmp2$process)/nrow(tmp2))
  colnames(processes)[1]<-"process"
  colnames(processes)[2]<-"percent"
  processes$process<-as.character(processes$process)
  processes$d_val_group<-rep(groups[i],nrow(processes))
  processes$n<-rep(nrow(tmp2),nrow(processes))
  tally<-rbind(tally,processes)
}
  

tally[tally$process=="Homogeneous.Selection",]$process<-"B_Homogeneous selection"
tally[tally$process=="Heterogeneous.Selection",]$process<-"A_Variable selection"
tally[tally$process=="Dispersal.Limitation",]$process<-"E_Dispersal limitation"
tally[tally$process=="Homogenizing.Dispersal",]$process<-"D_Homogenizing dispersal"
tally[tally$process=="Undominated",]$process<-"C_Drift"

new_colors<-c("#556B2F", "#A2CD5A","#CCCCCC","#B0E2FF", "#36648B")

        ggplt <- ggplot(tally,            
                        aes(x = d_val_group,
                            y = percent,
                            color= process)) +  geom_line(linewidth=2) +geom_point(size=3) + theme_bw() + scale_x_continuous(breaks=seq(1,length(groups),1), 
                                                                                                                             labels=break_labels) + xlab(label = "Sediment characteristic\ndissimilarity")+ ylab(label = "Proportion") + geom_line(linewidth=2) +   labs(color="Assembly process")    +theme(axis.ticks.x = element_blank()) + scale_color_manual(values=new_colors, labels=c("Variable selection",  "Homogeneous selection",    "Drift",  "Homogenizing dispersal" , "Dispersal limitation" )) + theme(axis.title.x = element_text(vjust=-0.5)) + theme(panel.grid.minor.x = element_blank()) + ylim(0,.75) + theme(axis.text.x = element_text(angle = 45,hjust=1))+ guides(color = guide_legend(override.aes = list(linewidth = 6))) +
                theme(axis.text=element_text(color="black"))+
                theme(axis.text.x = element_text(angle = 60,hjust=1))+
                theme(text=element_text(size=14), #change font size of all text
                      axis.text=element_text(size=14), #change font size of axis text
                      axis.title=element_text(size=15), #change font size of axis titles
                      plot.title=element_text(size=14), #change font size of plot title
                      legend.text=element_text(size=14), #change font size of legend text
                      legend.title=element_text(size=15),
                      panel.grid.minor = element_blank(), 
                      panel.grid.major.x = element_blank()) #change font size of legend title 

#pdf("Figures/SupplementaryFigures/CAP_sediment_characteristics_intracore_10July2023.pdf", height=3.5, width=6)
pdf("Figures/SupplementaryFigures/CAP_sediment_characteristics_intercore_10July2023.pdf", height=3.5, width=6)
ggplt
dev.off()

unique(tally$n)
# 4277 4276


```


## 4. NMDS

```{r}
ps_table <- data.frame(otu_table(ps_tr))
ps_table <-t(ps_table)
sd_sed <- data.frame(sample_data(ps_tr))

ord_baseR<-metaMDS(ps_table, distance = "bray")
# Ran 29May2023
# Run 0 stress 0.1707505 
# Run 1 stress 0.1923034 
# Run 2 stress 0.1930776 
# Run 3 stress 0.1974319 
# Run 4 stress 0.2061988 
# Run 5 stress 0.1986631 
# Run 6 stress 0.4193641 
# Run 7 stress 0.1965793 
# Run 8 stress 0.198454 
# Run 9 stress 0.4193671 
# Run 10 stress 0.1963773 
# Run 11 stress 0.1957859 
# Run 12 stress 0.1903609 
# Run 13 stress 0.1973567 
# Run 14 stress 0.192619 
# Run 15 stress 0.1990084 
# Run 16 stress 0.1933836 
# Run 17 stress 0.2008754 
# Run 18 stress 0.2063872 
# Run 19 stress 0.1992947 
# Run 20 stress 0.1928635 
# *** Best solution was not repeated -- monoMDS stopping criteria:
#      3: no. of iterations >= maxit
#     12: stress ratio > sratmax
#      5: scale factor of the gradient < sfgrmin

fig<-ordiplot(ord_baseR, type="points")

NMDS1<-fig$sites[,1]
NMDS2<-fig$sites[,2]

fig_sites<-as.data.frame(fig[["sites"]])
table(fig_sites$NMDS2==NMDS2) #same for both NMDS1 and NMDS2


fig_sites$samp_names<-rownames(fig_sites)
fig_sites$samp_names<-gsub("X","",as.character(fig_sites$samp_names))
table(names(as.data.frame(ps_tr@otu_table))==fig_sites$samp_names) # all true 

#add in metadata to the dataframe with the NMDS
ord_df<-merge(fig_sites,sd_sed, by="samp_names")

#8c510a darker brown
#a6611a # dark brown
#dfc27d
#80cdc1
#018571 # dark teal

darkertan<-darken("#dfc27d", 0.1)
#C4AD79

ord_df$zone_col<-rep('#01857180',nrow(ord_df))
ord_df[ord_df$bin_depth%in%c(6,8,10,12),]$zone_col<-"#C4AD7980"
ord_df[ord_df$bin_depth%in%c(14,16,18,20,22,24,26),]$zone_col<-'#a6611a80'
ord_df[ord_df$depth>26,]$zone_col<-'#00000080' #black

zone_colors<- c('#018571','#dfc27d','#a6611a','#000000')

sd_sed$bottom_water_temp<-sd_sed$water_sample_t_bot
sd_sed$lake_depth<-sd_sed$max_lake_depth
sd_sed$sediment_depth<-sd_sed$depth

#if you want first axis reversed!
ord_df$NMDS1<-(ord_df$NMDS1*(-1))

ord.fit <- envfit(ord_baseR ~  sediment_depth + lake_depth + bottom_water_temp,  data = sd_sed,  perm = 1000, na.rm = TRUE)
ord.fit[["vectors"]][["arrows"]][1,1]<-(ord.fit[["vectors"]][["arrows"]][1,1]*(-1))

save.image("NMDS_29May2023.Rdata")
```


plot NMDS
with "deep depauperate"
```{r}
load("NMDS_29May2023.Rdata")

pdf("Figures/NMDS_28June2023_noLegend.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
```

with "deeper reference communities"
```{r}
ord_df$shape<-rep(16,nrow(ord_df))
ord_df[ord_df$depth>26,]$shape<-8
ord_df[ord_df$depth>26,]$zone_col<-'#00000098' #black

ord_df$size<-rep(1.5,nrow(ord_df))
ord_df[ord_df$depth>26,]$size<-1


pdf("Figures/NMDS_14Aug2023_refcommunities.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=ord_df$shape,cex=ord_df$size, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
```

plot just legend

```{r}

pdf("Figures/NMDS_14Aug2023_refcommunities_legend.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 5), xpd=T)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=ord_df$shape,cex=ord_df$size, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
legend("topright",legend=c("A","B"), pch=c(16,8), cex=1.5, title="Group")
dev.off()


```



### a. adonis testing

```{r}
load("NMDS_29May2023.Rdata")

ps_table <- data.frame(otu_table(ps_tr)) 
names(ps_table)<-colnames(ps_tr@otu_table)
ps_table<-as.data.frame(t(ps_table))

sd <- data.frame(sample_data(ps_tr)) 
#JVE - first adding surface water temperature for round lake bottom water temperature since the lake it 1.5 m deep
sd[sd$lake_id==40,]$water_sample_t_bot<-20.1


sd$zone<-rep("A",nrow(sd))
sd[sd$bin_depth%in%c(6,8,10,12),]$zone<-"B"
sd[sd$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
sd[sd$bin_depth==100,]$zone<-"D"
sd$zone<-as.factor(sd$zone)

#subset the sd to just the variables we are looking at
sd <- sd[names(sd)%in%c("water_sample_t_bot","max_lake_depth","zone")]
table(is.na(sd)) #make sure there are no NAs

# you must remove any samples with NAs prior to running this analysis 
table(rownames(ps_table)%in%rownames(sd))
adonis_table<-data.matrix(ps_table) 
adonis_dist<-parDist(adonis_table, method = "bray") 
adonis2(adonis_dist ~ zone+  water_sample_t_bot + max_lake_depth, data = sd)  

sd <- data.frame(sample_data(ps_tr)) 
#JVE - first adding surface water temperature for round lake bottom water temperature since the lake it 1.5 m deep
sd[sd$lake_id==40,]$water_sample_t_bot<-20.1

sd$zone<-rep("A",nrow(sd))
sd[sd$bin_depth%in%c(6,8,10,12),]$zone<-"B"
sd[sd$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
sd[sd$bin_depth==100,]$zone<-"D"
sd$zone<-as.factor(sd$zone)
#subset the sd to just the variables we are looking at
sd <- sd[names(sd)%in%c("water_sample_t_bot","max_lake_depth","depth")]
table(is.na(sd)) #make sure there are no NAs

# you must remove any samples with NAs prior to running this analysis 
table(rownames(ps_table)%in%rownames(sd))
adonis_table<-data.matrix(ps_table) 
adonis_dist<-parDist(adonis_table, method = "bray") 
adonis2(adonis_dist ~ depth +  water_sample_t_bot + max_lake_depth, data = sd)  
```

USING ZONE
Permutation test for adonis under reduced model
Terms added sequentially (first to last)
Permutation: free
Number of permutations: 999

adonis2(formula = adonis_dist ~ zone + water_sample_t_bot + max_lake_depth, data = sd)
                    Df SumOfSqs      R2      F Pr(>F)    
zone                 3   13.397 0.07213 13.072  0.001 ***
water_sample_t_bot   1    7.383 0.03975 21.613  0.001 ***
max_lake_depth       1    3.710 0.01998 10.861  0.001 ***
Residual           472  161.244 0.86814                  
Total              477  185.735 1.00000                  

Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1


USING BIN DEPTH
Permutation test for adonis under reduced model
Terms added sequentially (first to last)
Permutation: free
Number of permutations: 999

adonis2(formula = adonis_dist ~ bin_depth + water_sample_t_bot + max_lake_depth, data = sd)
                    Df SumOfSqs      R2      F Pr(>F)    
bin_depth            1    6.552 0.03528 18.479  0.001 ***
water_sample_t_bot   1    7.388 0.03978 20.837  0.001 ***
max_lake_depth       1    3.732 0.02009 10.526  0.001 ***
Residual           474  168.062 0.90485                  
Total              477  185.735 1.00000                  

Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1

****USING SED DEPTH - using this one

Permutation test for adonis under reduced model
Terms added sequentially (first to last)
Permutation: free
Number of permutations: 999

adonis2(formula = adonis_dist ~ depth + water_sample_t_bot + max_lake_depth, data = sd)
                    Df SumOfSqs      R2      F Pr(>F)    
depth                1    9.684 0.05214 27.783  0.001 ***
water_sample_t_bot   1    7.268 0.03913 20.853  0.001 ***
max_lake_depth       1    3.574 0.01924 10.255  0.001 ***
Residual           474  165.209 0.88949                  
Total              477  185.735 1.00000                  

Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1

### b. plot each zone separately

```{r}
#ord_df_backup<-ord_df
#temp<-ord_df #reversed the whole dataset to temp to subset from
zone<-c("replacement","surface","depauperate","deep_depauperate")
zone_colors<-unique(temp$zone_col)
i=1
for(i in 1:4){
pdf(paste("Figures/SupplementaryFigures/NMDS_",zone[i],"_10July2023.pdf", sep=""), width=5, height=4.5)
ord_df<-temp[temp$zone_col==zone_colors[i],]
    par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,pch=16,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col=ord_df$zone_col,xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
#plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()
}
```

### c. points by mountain range

```{r}
ord_df<-ord_df_backup
ord_df$mtn_pch<-rep(NA,nrow(ord_df))
ord_df[ord_df$mountain_range=="Snowy",]$mtn_pch<-21 #circle
ord_df[ord_df$mountain_range=="Beartooth",]$mtn_pch<-22 #square
ord_df[ord_df$mountain_range=="Wind River",]$mtn_pch<-23 #diamond
ord_df[ord_df$mountain_range=="Bighorn",]$mtn_pch<-24 #triangle
ord_df$mtn_pch<-as.numeric(ord_df$mtn_pch)

pdf("Figures/SupplementaryFigures/NMDS_mtnshapes_10July2023.pdf", width=5, height=4.5)
par(mar=c(4, 4, 2, 1), xpd=TRUE)
plot(ord_df$NMDS1, ord_df$NMDS2,cex=1.5, ylim=c(-1.5,2.3), yaxt="n", xlim=c(-3,2.3),col="black",bg=ord_df$zone_col, pch=ord_df$mtn_pch, xlab='', cex.axis=1.2,ylab='')
axis(2,at=c(-1,0,1,2),labels =c(-1,0,1,2), cex.axis=1.2, las=2, col = NA, col.ticks = 1)
title(xlab="NMDS1", line = 2.5, cex.lab=1.2)
title(ylab="NMDS2", line = 2, cex.lab=1.2)
#legend("top",inset=c(0.3,-0.13),xpd=TRUE,bty ="n",legend = c("                    ","                       ","                       ","                       "), col=zone_colors, pch=16, pt.cex = 3,cex=1,ncol=4)
#plot(ord.fit, labels=c("", "",""),p.max=0.05, col="black",lwd=6, cex=1.6)
dev.off()

```




## 5. Rel abund of dominant taxa

### a. Caluclate top phyla

Subset phyloseq by 26 cm and then glom by Phylum
```{r}
#subset samples less than or equal to 26
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
ps_tr_26_phy<-tax_glom(ps_tr_26, taxrank = "Phylum",NArm = FALSE)
ps_melt<-psmelt(ps_tr_26_phy)
#write.csv(ps_melt, "RelativeAbundance_AllPhyla_30May2023.csv")
```


```{r}
ps_melt<-read.csv("RelativeAbundance_AllPhyla_30May2023.csv", header=T, row.names=1)

#which phyla go over a threshold on AVERAGE for each bin depth
    pt <- PivotTable$new()
    pt$addData(ps_melt)
    pt$addRowDataGroups("bin_depth") 
    pt$addColumnDataGroups("Phylum") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    
    
    phy_avg_abund <- pt$asDataFrame()
    phy_avg_abund$bin_depth<-rownames(phy_avg_abund)
    phy_avg_abund<-phy_avg_abund[-which(phy_avg_abund$bin_depth=="Total"),]
    phy_avg_abund$Total<-NULL
    phy_avg_abund$bin_depth <- as.numeric(phy_avg_abund$bin_depth)
    phy_avg_abund <- melt(phy_avg_abund, id.vars = "bin_depth")
    
    rm(pt)
    
length(unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.04),]$variable)))
#10
#this one includes Desulfobacterota so we should use this threshold instead- now changed to top 10

subphy<-unique(as.character(phy_avg_abund[which(phy_avg_abund$value>0.04),]$variable))
subphy
# new ones - 26June2023
 # [1] "Acidobacteriota"   "Bacteroidota"      "Chloroflexi"       "Crenarchaeota"    
 # [5] "Cyanobacteria"     "Desulfobacterota"  "Halobacterota"     "Planctomycetota"  
 # [9] "Proteobacteria"    "Verrucomicrobiota"

### these are the old ones
# [1] "Bacteroidetes"                      "Chloroflexi"                       
# [3] "Cyanobacteria"                      "Euryarchaeota"                     
# [5] "Miscellaneous_Crenarchaeotic_Group" "Planctomycetes"                    
# [7] "Proteobacteria"                     "Verrucomicrobia"


# what percent are these 8 phyla in the entire dataset?

length(unique(ps_melt$samp_names))
#429 samples - 30May2023
sum(ps_melt[ps_melt$Phylum%in%subphy,]$Abundance)/length(unique(ps_melt$samp_names))
#[1] 0.706016 - 26June2023 total reads

 topphy %>% select(Kingdom, Phylum) %>% unique()
```


```{r}

topphy<-read.csv("RelativeAbundance_AllPhyla_30May2023.csv", header=T, row.names=1)
topphy<-topphy[topphy$Phylum%in%c("Acidobacteriota" ,  "Bacteroidota" ,    
"Chloroflexi"    ,   "Crenarchaeota"    ,"Halobacterota",
"Cyanobacteria"  ,   "Desulfobacterota" ,
"Planctomycetota" , "Proteobacteria"   ,
"Verrucomicrobiota"),]

ordphy<-c(  "Bacteroidota" ,    
"Cyanobacteria"  ,  "Proteobacteria"   ,
"Verrucomicrobiota",
"Acidobacteriota" ,"Chloroflexi",  "Desulfobacterota" ,"Halobacterota"
 ,"Crenarchaeota"   ,"Planctomycetota")

lake_drives<-unique(topphy$lake_drive)
zone_colors<- c(rep('#018571',4), rep('#C4AD79', 4), rep('#a6611a',3))

vals<-list()
labs<-list()

pdf("Figures/RelAbun_TopPhyla_26Jun2023.pdf", width=13, height=4)
par(mfrow=c(1,13), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)
for(i in 1:length(unique(topphy$Phylum))){
  temp<-topphy[topphy$Phylum==ordphy[i],]
  temp<-temp[order(temp$bin_depth,decreasing=T),]
  pt <- PivotTable$new()
    pt$addData(temp)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = T),]
    plot(temp$Abundance,temp$bin_depth,col="white",ylim=c(26,0),bty="n",xlim=c(0,max(meanabund$Abundance)*1.8),ylab="",xlab="",yaxt="n",xaxt="n",cex.axis=1.6)
    vals[[i]]<-seq(0,(max(meanabund$Abundance)*1.8), length.out=5)
    labs[[i]]<- c("0", as.character(round(vals[[i]][2:5], digits=2)))
    axis(side = 1, at = vals[[i]], labels =labs[[i]], cex.axis=1.6)

    for(l in 1:length(lake_drives)){
  temp2<-temp[temp$lake_drive==lake_drives[l],]  
  temp2<-temp2[order(temp2$bin_depth,decreasing = F),]
  lines(temp2$Abundance,temp2$bin_depth, col="gray")}
    lines(meanabund$Abundance,meanabund$bin_depth, col=zone_colors[i],lwd=4)
}
dev.off()


```



### b. Top families


```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
#subset samples less than or equal to 26
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
ps_tr_26_fam<-tax_glom(ps_tr_26, taxrank = "Family",NArm = FALSE)
ps_melt<-psmelt(ps_tr_26_fam)
write.csv(ps_melt, "RelativeAbundance_AllFamilies_30May2023.csv")
```

```{r}
ps_melt<-read.csv("RelativeAbundance_AllFamilies_30May2023.csv", header=T, row.names=1)
ps_melt$taxtofam<-paste(ps_melt$Phylum, ps_melt$Class, ps_melt$Order, ps_melt$Family, sep = "@")

#which phyla go over a threshold on AVERAGE for each bin depth
    pt <- PivotTable$new()
    pt$addData(ps_melt)
    pt$addRowDataGroups("bin_depth") 
    pt$addColumnDataGroups("taxtofam") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    
    
    fam_avg_abund <- pt$asDataFrame()
    fam_avg_abund$bin_depth<-rownames(fam_avg_abund)
    fam_avg_abund<-fam_avg_abund[-which(fam_avg_abund$bin_depth=="Total"),]
    fam_avg_abund$Total<-NULL
    fam_avg_abund$bin_depth <- as.numeric(fam_avg_abund$bin_depth)
    fam_avg_abund <- melt(fam_avg_abund, id.vars = "bin_depth")
    
    rm(pt)
    

#remove NA@NA
fam_avg_abund<-fam_avg_abund[fam_avg_abund$variable!="NA@NA@NA@NA",]
    

#but only include those that are actually assigned to a family, otherwise you are looking really at unassigned families in specific orders or classes

subfam<-unique(as.character(fam_avg_abund[which(fam_avg_abund$value>0.0135),]$variable))
table((str_split(subfam,pattern = "@", 4, simplify = T)[,4])!="NA")
subfam
#  [1] "Acidobacteriota@Aminicenantia@Aminicenantales@NA"                     
#  [2] "Acidobacteriota@Vicinamibacteria@Vicinamibacterales@NA"               
#  [3] "Bacteroidota@Bacteroidia@Bacteroidales@Bacteroidetes vadinHA17"       
#  [4] "Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"              
#  [5] "Chloroflexi@Anaerolineae@RBG-13-54-9@NA"                              
#  [6] "Chloroflexi@Dehalococcoidia@MSBL5@NA"                                 
#  [7] "Chloroflexi@KD4-96@NA@NA"                                             
#  [8] "Crenarchaeota@Bathyarchaeia@NA@NA"                                    
#  [9] "Cyanobacteria@Cyanobacteriia@Cyanobacteriales@Phormidiaceae"          
# [10] "Cyanobacteria@Cyanobacteriia@Synechococcales@Cyanobiaceae"            
# [11] "Halobacterota@Methanomicrobia@Methanomicrobiales@Methanoregulaceae"   
# [12] "Halobacterota@Methanosarcinia@Methanosarciniales@Methanosaetaceae"    
# [13] "Latescibacterota@NA@NA@NA"                                            
# [14] "Nanoarchaeota@Nanoarchaeia@Woesearchaeales@NA"                        
# [15] "Planctomycetota@Phycisphaerae@DG-20@NA"                               
# [16] "Planctomycetota@Phycisphaerae@MSBL9@SG8-4"                            
# [17] "Planctomycetota@Phycisphaerae@Phycisphaerales@AKAU3564 sediment group"
# [18] "Planctomycetota@Planctomycetes@Pirellulales@Pirellulaceae"            
# [19] "Proteobacteria@Gammaproteobacteria@Burkholderiales@Comamonadaceae"    
# [20] "Proteobacteria@Gammaproteobacteria@Burkholderiales@Rhodocyclaceae"    
# [21] "Proteobacteria@Gammaproteobacteria@Burkholderiales@SC-I-84"           
# [22] "Proteobacteria@Gammaproteobacteria@Methylococcales@Methylomonadaceae" 
# [23] "Sva0485@NA@NA@NA"                                                     
# [24] "Thermoplasmatota@Thermoplasmata@Marine Benthic Group D and DHVEG-1@NA"
# [25] "Verrucomicrobiota@Omnitrophia@Omnitrophales@Omnitrophaceae"           
# [26] "Verrucomicrobiota@Verrucomicrobiae@Pedosphaerales@Pedosphaeraceae" 

#without NAs
#  [3] "Bacteroidota@Bacteroidia@Bacteroidales@Bacteroidetes vadinHA17"       
#  [4] "Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"              
#  [9] "Cyanobacteria@Cyanobacteriia@Cyanobacteriales@Phormidiaceae"          
# [10] "Cyanobacteria@Cyanobacteriia@Synechococcales@Cyanobiaceae"            
# [11] "Halobacterota@Methanomicrobia@Methanomicrobiales@Methanoregulaceae"   
# [12] "Halobacterota@Methanosarcinia@Methanosarciniales@Methanosaetaceae"    
  # [16] "Planctomycetota@Phycisphaerae@MSBL9@SG8-4"                            
# [17] "Planctomycetota@Phycisphaerae@Phycisphaerales@AKAU3564 sediment group"
# [18] "Planctomycetota@Planctomycetes@Pirellulales@Pirellulaceae"            
# [19] "Proteobacteria@Gammaproteobacteria@Burkholderiales@Comamonadaceae"    
# [20] "Proteobacteria@Gammaproteobacteria@Burkholderiales@Rhodocyclaceae"    
# [21] "Proteobacteria@Gammaproteobacteria@Burkholderiales@SC-I-84"           
# [22] "Proteobacteria@Gammaproteobacteria@Methylococcales@Methylomonadaceae" 
# [25] "Verrucomicrobiota@Omnitrophia@Omnitrophales@Omnitrophaceae"           
# [26] "Verrucomicrobiota@Verrucomicrobiae@Pedosphaerales@Pedosphaeraceae" 

subfam<-c("Bacteroidota@Bacteroidia@Bacteroidales@Bacteroidetes vadinHA17"    
, "Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"              
, "Cyanobacteria@Cyanobacteriia@Cyanobacteriales@Phormidiaceae"          
, "Cyanobacteria@Cyanobacteriia@Synechococcales@Cyanobiaceae"            
, "Halobacterota@Methanomicrobia@Methanomicrobiales@Methanoregulaceae"   
, "Halobacterota@Methanosarcinia@Methanosarciniales@Methanosaetaceae"    
, "Planctomycetota@Phycisphaerae@MSBL9@SG8-4"                            
,"Planctomycetota@Phycisphaerae@Phycisphaerales@AKAU3564 sediment group"
, "Planctomycetota@Planctomycetes@Pirellulales@Pirellulaceae"            
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@Comamonadaceae"    
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@Rhodocyclaceae"    
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@SC-I-84"           
, "Proteobacteria@Gammaproteobacteria@Methylococcales@Methylomonadaceae" 
, "Verrucomicrobiota@Omnitrophia@Omnitrophales@Omnitrophaceae"           
, "Verrucomicrobiota@Verrucomicrobiae@Pedosphaerales@Pedosphaeraceae"  )

sum(ps_melt[ps_melt$taxtofam%in%subfam,]$Abundance)/length(unique(ps_melt$samp_names))
#[1] 0.2702896 - 26June2023
```

PLOT
```{r}
topfam<-read.csv("RelativeAbundance_AllFamilies_30May2023.csv", header=T, row.names=1)
topfam$taxtofam<-paste(topfam$Phylum, topfam$Class,topfam$Order, topfam$Family, sep = "@")
topfam<-topfam[topfam$taxtofam%in%c("Bacteroidota@Bacteroidia@Bacteroidales@Bacteroidetes vadinHA17"   
, "Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"              
, "Cyanobacteria@Cyanobacteriia@Cyanobacteriales@Phormidiaceae"          
, "Cyanobacteria@Cyanobacteriia@Synechococcales@Cyanobiaceae"            
, "Halobacterota@Methanomicrobia@Methanomicrobiales@Methanoregulaceae"   
, "Halobacterota@Methanosarcinia@Methanosarciniales@Methanosaetaceae"    
, "Planctomycetota@Phycisphaerae@MSBL9@SG8-4"                            
,"Planctomycetota@Phycisphaerae@Phycisphaerales@AKAU3564 sediment group"
, "Planctomycetota@Planctomycetes@Pirellulales@Pirellulaceae"            
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@Comamonadaceae"    
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@Rhodocyclaceae"    
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@SC-I-84"           
, "Proteobacteria@Gammaproteobacteria@Methylococcales@Methylomonadaceae" 
, "Verrucomicrobiota@Omnitrophia@Omnitrophales@Omnitrophaceae"           
, "Verrucomicrobiota@Verrucomicrobiae@Pedosphaerales@Pedosphaeraceae"  ),]

#Look at which ones are archaea
topfam %>% select(Kingdom, taxtofam) %>% unique()


         
# ordered<-c("Cyanobacteria@Cyanobacteriia@Cyanobacteriales@Phormidiaceae"          
# , "Cyanobacteria@Cyanobacteriia@Synechococcales@Cyanobiaceae"            
# , "Planctomycetota@Planctomycetes@Pirellulales@Pirellulaceae"            
# , "Proteobacteria@Gammaproteobacteria@Burkholderiales@Comamonadaceae"    
# , "Proteobacteria@Gammaproteobacteria@Burkholderiales@Rhodocyclaceae"    
# , "Proteobacteria@Gammaproteobacteria@Burkholderiales@SC-I-84"           
# , "Proteobacteria@Gammaproteobacteria@Methylococcales@Methylomonadaceae" 
# , "Verrucomicrobiota@Verrucomicrobiae@Pedosphaerales@Pedosphaeraceae" 
# ,"Bacteroidota@Bacteroidia@Bacteroidales@Bacteroidetes vadinHA17"   
# , "Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"    
# , "Halobacterota@Methanomicrobia@Methanomicrobiales@Methanoregulaceae"   
# , "Halobacterota@Methanosarcinia@Methanosarciniales@Methanosaetaceae" 
# , "Verrucomicrobiota@Omnitrophia@Omnitrophales@Omnitrophaceae"       
# , "Planctomycetota@Phycisphaerae@MSBL9@SG8-4"                            
# ,"Planctomycetota@Phycisphaerae@Phycisphaerales@AKAU3564 sediment group")

#alphabetically ordered by family name
ordered<-c("Proteobacteria@Gammaproteobacteria@Burkholderiales@Comamonadaceae" 
, "Cyanobacteria@Cyanobacteriia@Synechococcales@Cyanobiaceae" 
, "Proteobacteria@Gammaproteobacteria@Methylococcales@Methylomonadaceae" 
, "Verrucomicrobiota@Verrucomicrobiae@Pedosphaerales@Pedosphaeraceae" 
,"Cyanobacteria@Cyanobacteriia@Cyanobacteriales@Phormidiaceae"          
, "Planctomycetota@Planctomycetes@Pirellulales@Pirellulaceae"            
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@Rhodocyclaceae"    
, "Proteobacteria@Gammaproteobacteria@Burkholderiales@SC-I-84"      

, "Chloroflexi@Anaerolineae@Anaerolineales@Anaerolineaceae"
,"Bacteroidota@Bacteroidia@Bacteroidales@Bacteroidetes vadinHA17"   
, "Halobacterota@Methanomicrobia@Methanomicrobiales@Methanoregulaceae"   
, "Halobacterota@Methanosarcinia@Methanosarciniales@Methanosaetaceae" 
, "Verrucomicrobiota@Omnitrophia@Omnitrophales@Omnitrophaceae"      

, "Planctomycetota@Phycisphaerae@MSBL9@SG8-4"                            
,"Planctomycetota@Phycisphaerae@Phycisphaerales@AKAU3564 sediment group")

lake_drives<-unique(topfam$lake_drive)

zone_colors<- c(rep('#018571',8), rep('#C4AD79', 5), rep('#a6611a',2))

vals<-list()
labs<-list()

pdf("Figures/RelAbun_TopFam_26Jun2023_smallerwidth_alphabetical.pdf", width=14, height=4)
par(mfrow=c(1,16), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)
for(i in 1:length(unique(topfam$taxtofam))){
  temp<-topfam[topfam$taxtofam==ordered[i],]
  temp<-temp[order(temp$bin_depth,decreasing=T),]
  pt <- PivotTable$new()
    pt$addData(temp)
    pt$addRowDataGroups("bin_depth") 
    pt$defineCalculation(calculationName="Abundance", summariseExpression="mean(Abundance,na.rm=T)")
    pt$renderPivot()
    meanabund <- pt$asDataFrame()
    meanabund$bin_depth<-rownames(meanabund)
    meanabund<-meanabund[-which(meanabund$bin_depth=="Total"),]
    meanabund$bin_depth<-as.numeric(meanabund$bin_depth)
    meanabund<-meanabund[order(meanabund$bin_depth,decreasing = T),]
    plot(temp$Abundance,temp$bin_depth,col="white",ylim=c(26,0),bty="n",xlim=c(0,max(meanabund$Abundance)*1.8),ylab="",xlab="",yaxt="n",xaxt="n", cex.axis=1.6)
        vals[[i]]<-seq(0,(max(meanabund$Abundance)*1.8), length.out=5)
    labs[[i]]<- c("0", as.character(round(vals[[i]][2:5], digits=2)))
    axis(side = 1, at = vals[[i]], labels =labs[[i]], cex.axis=1.6)

    for(l in 1:length(lake_drives)){
  temp2<-temp[temp$lake_drive==lake_drives[l],]  
  temp2<-temp2[order(temp2$bin_depth,decreasing = F),]
  lines(temp2$Abundance,temp2$bin_depth, col="gray")}
    lines(meanabund$Abundance,meanabund$bin_depth, col=zone_colors[i],lwd=4)
}
dev.off()


```

### c. Hills numbers and elemental analyses
```{r}


load("WyLakeMicrobes_Phyloseq_11May2023.RData")
#sed char summary
ps_tr_26<-subset_samples(ps_tr, bin_depth<=26)
OTU<-as.data.frame(ps_tr_26@otu_table@.Data)
meta_sed_26<-metadata[metadata$samp_names%in%names(OTU),]

hills<-read.delim("2023May26_HillNumbersNormalized.txt",header=T,row.names = 1)
hills$samp_names<-rownames(hills)
meta_sed_26_hills<-merge(meta_sed_26,hills,by="samp_names")
meta_sed_26_hills$log_cn<-log(meta_sed_26_hills$cn)

depthbins<-sort(unique(meta_sed_26_hills$bin_depth),decreasing=F)

summary<-data.frame(bin_depth=NULL,
                    rich_mean=NULL,rich_sd=NULL,
                    shan_mean=NULL,shan_sd=NULL,
                    simp_mean=NULL,simp_sd=NULL,
                    c13_mean=NULL,c13_sd=NULL,
                    c_perc_mean=NULL,c_perc_sd=NULL,
                    n_mean=NULL,n_sd=NULL,
                    cn_mean=NULL,cn_sd=NULL)
i=1
for(i in 1:length(depthbins)){
  tmp<-meta_sed_26_hills[meta_sed_26_hills$bin_depth==depthbins[i],]
  tmp2<-data.frame(bin_depth=depthbins[i],
                   rich_mean=mean(tmp$Species_Richness,na.rm=T),rich_sd=sd(tmp$Species_Richness,na.rm=T),
                   shan_mean=mean(tmp$Shannon_Diversity,na.rm=T),shan_sd=sd(tmp$Shannon_Diversity,na.rm=T),
                   simp_mean=mean(tmp$Simpson_Dominance,na.rm=T),simp_sd=sd(tmp$Simpson_Dominance,na.rm=T),
                    c13_mean=mean(tmp$d_13_c,na.rm=T),c13_sd=sd(tmp$d_13_c,na.rm=T),
                    c_perc_mean=mean(tmp$c_perc,na.rm=T),c_perc_sd=sd(tmp$c_perc,na.rm=T),
                    n_mean=mean(tmp$n_perc,na.rm=T),n_sd=sd(tmp$n_perc,na.rm=T),
                   cn_mean=mean(tmp$cn,na.rm=T),cn_sd=sd(tmp$cn,na.rm=T))
  summary<-rbind(summary,tmp2)
}


# average CONISS across all samples
OTU<-as.data.frame(ps_tr@otu_table@.Data)
OTU<-t(OTU)

OTU_meta<-as.matrix(ps_tr@sam_data)
OTU_meta<-as.data.frame(OTU_meta)
OTU_meta$bin_depth<-as.numeric(OTU_meta$bin_depth)

#create a mean relative abundance of each ESV for each depth
mean_abundances<-data.frame()
i=1
centimeters<-c(0,2,4,6,8,10,12,14,16,18,20,22,24,26)
  sub<-OTU[rownames(OTU)%in%(OTU_meta[OTU_meta$bin_depth==centimeters[1] & is.na(OTU_meta$bin_depth)==FALSE,]$samp_names),]
  mean_abundances<-as.data.frame(colMeans(sub))
  colnames( mean_abundances)[1]<-centimeters[i]
for(i in 2:length(centimeters)){
  sub<-OTU[rownames(OTU)%in%(OTU_meta[OTU_meta$bin_depth==centimeters[i] & is.na(OTU_meta$bin_depth)==FALSE,]$samp_names),]
  means<-as.data.frame(colMeans(sub))
  colnames(means)[1]<-centimeters[i]
  mean_abundances<-cbind(mean_abundances,means)
}

tma<-as.matrix(t(mean_abundances))
dist<-vegdist(tma)
  clust<-chclust(dist)
```


Alpha diversity and elemental measurements
```{r}
pdf("Figures/RelAbun_Element_ADiversity_26June2023.pdf", width=13, height=4)  
seq<-seq(2,14,2)
lab<-c("Species\nrichness","Shannon\ndiversity","Simpson\ndiversity","d13C", "%C","%N","C:N")
lines<-c(rep(4.5,3), rep(3.5,4))
sed_col<-c(rep("#607B8B",3), rep("#616161",4))
i=1


par(mfrow=c(1,13), mar=c(6,6,1,0))
plot(0:26,0:26, bty="n", frame.plot = FALSE, xaxt="n", yaxt="n",ylab = "Depth (cm)", xlab="",ylim=c(26,0), las=1, cex.lab=1.6, pch=19, col="white")
axis(side=2, at= seq(0, 26, by=2), cex.axis=1.6,labels= seq(0, 26, by=2), las=1)
par(mar=c(6,0.5,1,1), xpd=TRUE)

for(i in 1:length(seq)){
  sum_temp<-summary[,c(1,seq[i],seq[i]+1)]
  sum_temp$sdp<-sum_temp[,2]+sum_temp[,3]
  sum_temp$sdm<-sum_temp[,2]-sum_temp[,3]
  plot(sum_temp[,2],sum_temp$bin_depth,ylim=c(26,0),xlim=c(min(sum_temp$sdm),max(sum_temp$sdp)),type="l",bty="n", yaxt="n", ylab="",col="white",lwd=2, xlab="", cex.axis=1.6)
    mtext(lab[i],side=1,line=lines[i], cex=1.1)
polygon(x = c(sum_temp$sdp,rev(sum_temp$sdm)),  # X-Coordinates of polygon 
        y = c(sum_temp$bin_depth, rev(sum_temp$bin_depth)),    # Y-Coordinates of polygon
        col = paste(sed_col[i],"50",sep=""), 
        border=paste(sed_col[i],"50",sep="")) 
lines(sum_temp[,2],sum_temp$bin_depth, col=sed_col[i],lwd=3)
}
 dev.off()
   
```

CONISS
```{r}

pdf("Figures/RelAbun_CONISS_26June2023.pdf", width=13, height=4)
par(mfrow=c(1,13), mar=c(5.5,6,1.5,0))

par(mar=c(5.5,2,1.5,1))
 plot(clust, xvar=as.numeric(clust[["labels"]]),ylim=c(0,3.2), xaxt="n",hang=-1, cex=1.6, cex.lab=1.6, cex.main=1.8, horiz=TRUE, x.rev=TRUE) 
  mtext("Distance",side=1,line=3, cex=1.1)
  mtext("CONISS", side=3, line=0, cex=1.1)
  axis(side=1,at=c(0,1,2,3),labels=c(0,1,2,3), cex.axis=1.6)
dev.off()

```





## 6. Island Biogeography for poster

Merge in the lake area measurements from Google Earth Pro polygons, plot whether lake depth increases with lake area. Generally true, but some exceptions of large shallow lakes. 
```{r}
lkarea<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/metadata_lakearea_27May2023.csv",header=T)

meta_merged<-merge(metadata,lkarea,by="lake_name")

#plot lake area by lake depth

uniq_lake<- meta_merged %>% select(lake_name,lakearea_squaremeters,max_lake_depth)
uniq_lake<-unique(uniq_lake)
plot(uniq_lake$lakearea_squaremeters,uniq_lake$max_lake_depth, pch=16, xlab="Lake area (sq m)",ylab="Max. lake depth")

```


```{r}
ESV<-as.data.frame(otu_table(ps))
ESV<-t(ESV)

#calculate the number of ESVs for each sediment core
LkDr_ESVs<-data.frame(lake_drive=NULL, num_samples=NULL,num_ESVs=NULL)
i=1
for(i in 1:length(unique(metadata$lake_drive))){
        #for each lake_drive, grab all the sample names in metadata
        samples<-metadata[metadata$lake_drive==unique(metadata$lake_drive)[i],]$samp_names
        #pull out number of ESVs from the table 
        sub_ESV<-as.data.frame(ESV[rownames(ESV)%in%samples,])  
        colsumsESV<-as.data.frame(colSums(sub_ESV))
       LkDr_ESVs<-rbind(LkDr_ESVs,data.frame(lake_drive=unique(metadata$lake_drive)[i], num_samples=length(samples),num_ESVs=colSums(colsumsESV != 0)))
}

#this didn't work if just one sample so updated the last two manually

LkDr_ESVs[47,3]<-2003
LkDr_ESVs[48,3]<-3570

```

Something to think about is that some lakes have MORE sediment samples, can plot this
```{r}
plot(LkDr_ESVs$num_samples, LkDr_ESVs$num_ESVs, pch=16, ylab="Total ESVs per core", xlab="Sediment samples")
#cores must have at least 5 samples

#remove lake_drives with less than 5 samples
LkDr_ESVs<-LkDr_ESVs[LkDr_ESVs$num_samples>4,]

#merge with lake metadata
uniq_lake<- meta_merged %>% select(lake_name,lakearea_squaremeters,max_lake_depth, lake_drive, mountain_range)
uniq_lake<-unique(uniq_lake)

#merge with lake metadata
isl_bio<-merge(LkDr_ESVs,uniq_lake, by="lake_drive")

isl_bio$col<-as.factor(isl_bio$mountain_range)
isl_bio$col<-as.numeric(isl_bio$col)
isl_bio$lakearea_squarekm<-isl_bio$lakearea_squaremeters/1000000
```

plot number of taxa with lake depth, lake size, and lake depth x size
```{r}
cols<-met.brewer(name="Egypt", n=4, type="discrete")
cols<-c("#000000",cols[1],cols[4],cols[2],cols[3])
full_cols<-cols
#cols<-paste(cols,"99", sep="")
point_types<-c(16,17,18,19,20)



pdf("Figures/ESVs_lakearea_29May2023.pdf", height=4,width=5)
par(mar=c(4,6,2,2))
expression<-expression(Lake ~ area ~ (km^2))
plot(isl_bio$lakearea_squarekm,isl_bio$num_ESVs, pch=16, cex=2, ylab="Exact sequence variants \n(ESVs) per core" ,col=cols[isl_bio$col], xlab=expression)
legend("topright",title="Mountain range", c("Beartooth" , "Bighorn"  ,  "Snowy" ,     "Wind River"), bty="n",pt.cex=2, col=full_cols, pch=16)
dev.off()

pdf("Figures/ESVs_lakevolume_29May2023.pdf", height=4,width=5)
par(mar=c(4,6,2,2))
expression<-expression(Lake ~ volume ~ (m^3))
plot((isl_bio$lakearea_squaremeters * isl_bio$max_lake_depth),isl_bio$num_ESVs, cex=2,pch=16, ylab="Exact sequence variants \n(ESVs) per core",col=cols[isl_bio$col], xlab=expression)
legend("topright",title="Mountain range", c("Beartooth" , "Bighorn"  ,  "Snowy" ,     "Wind River"), bty="n",pt.cex=2, col=full_cols, pch=16)
dev.off()
```




```{r}
plot(isl_bio$max_lake_depth,isl_bio$num_ESVs,  ylab="ESVs per core", xlab="Lake depth (m)", col=cols[isl_bio$col],pch=point_types[isl_bio$col])
```



## 7. Stats for manuscript
```{r}
# what is the total number of cores and lakes?
length(unique(metadata$lake_id)) # 36 lakes
length(unique(metadata$lake_drive)) # 48 cores 

#average and range of sediment depth for cores? 
lake_drives<-unique(metadata$lake_drive)
maxd<-NULL
for(i in 1:length(lake_drives)){
        tmp<-metadata[metadata$lake_drive==lake_drives[i],]
       maxd <-c(maxd,max(tmp$depth))
}
mean(maxd)
#21.83333
max(maxd)
#88
min(maxd)   
#2

min(metadata$max_lake_depth)
max(metadata$max_lake_depth)
#0.5-19m

#how many lakes greater than or equal to 26 cm deep? 
table(maxd>=26)
22/(26+22)
#0.4583333 of the lakes were at or greater than 26 cm 

# how many samples are less than or equal to 26 cm
nrow(metadata[metadata$depth<=26,])
# 429 samples

# how many samples greater than 26 cm
478-429
#49 - 28June2023

meta_sed_26<-metadata[metadata$depth<=26,]

# JVE notes - I need to say how many samples are measured in ALL samples, not just 26 cm 

# #how many samples <=26 have elemental measurements?
# sub<-meta_sed_26[is.na(meta_sed_26$d_13_c)==FALSE,]
# nrow(sub)
# #121 samples
# 
# #how many cores and samples have C:N data
# length(unique(ps@sam_data[is.na(ps@sam_data$cn)==FALSE,]$lake_id))
# # 28
# #samples less than 26 cm that have CN data
# nrow(meta_sed_26[is.na(meta_sed_26$cn)==FALSE,])
# # 262 samples have C:N, C%, N%
# 
# #how many cores have d13C data
# length(unique(ps@sam_data[is.na(ps@sam_data$d_13_c)==FALSE,]$lake_id))
# # 13
# #how many samples <=26 have elemental measurements?
# nrow(meta_sed_26[is.na(meta_sed_26$d_13_c)==FALSE,])
# #121 samples

# so INSTEAD - did this:
# all the true
table(is.na(metadata$pH)==F)
#278

table(is.na(metadata$cn)==F)
#282

table(is.na(metadata$d_13_c)==F)
#130



#where are those from?
unique(ps@sam_data[is.na(ps@sam_data$d_13_c)==FALSE,]$mountain_range)
#just the Snowies

#Values for elemental measurements for results section



print("surface")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$n_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(0,2,4),]$cn, na.rm=T),digits=1)


print("replacement")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$n_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12),]$cn, na.rm=T),digits=2)

print("depauperate")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$c_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$n_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(14,16,18,20,22,24,26),]$cn, na.rm=T),digits=1)

print("replacement and depauperate")
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$c_perc, na.rm=T),digits=1)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$n_perc, na.rm=T),digits=2)
round(mean(meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22,24,26),]$cn, na.rm=T),digits=2)

# 28June2023
# [1] "surface"
# [1] 13.09
# [1] 1.34
# [1] "replacement"
# [1] 11.46
# [1] 1.02
# [1] "depauperate"
# [1] 11.34
# [1] 0.95
# [1] "replacement and depauperate"
# [1] 11.40
# [1] 0.98


# a positive shift in carbon isotopic composition (Î´13C) of occurred from depths 0-4 cm
sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(0),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -30.0454 - 28June2023
sd(sub$d_13_c)
#[1] 2.124595 - 28June2023


sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(4),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
sd(sub$d_13_c)
#[1] 4.420052 - 28June2023
(-30.0454) - (-25.2764)
#-4.769 (round to 4.77%) - 28June2023

sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(6,8,10,12,14,16,18,20,22),]
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -24.65961 - 28June2023
sd(sub$d_13_c)
#[1] 4.301652  - 28June2023

sub<-meta_sed_26[meta_sed_26$bin_depth%in%c(24,26),]
nrow(sub)
#10 samples at depths 24 and 26 with isotopic data
sub<-sub[is.na(sub$d_13_c)==FALSE,]
mean(sub$d_13_c)
#[1] -26.081  - 28June2023
sd(sub$d_13_c)
#[1] 2.139629  - 28June2023


# range of elevations
range(metadata$elevation_meters,na.rm=T) #NAs are the blanks that don't have elevations
#2032 3350
#subset for 26 cm cutoff elevation
range(meta_sed_26$elevation_meters,na.rm=T) 
#2032 3350 (same)

#range of temperatures

#these are the same for the 26 cm and less samples and the entire dataset
range(metadata$water_sample_t_bot, na.rm=T)
#3.8 20.1
#these are the same for the 26 cm and less samples and the entire dataset
range(metadata$water_sample_t_surf, na.rm=T)
#11.3 20.7
range(metadata$water_sample_ph_bot, na.rm=T)
#5.39 9.68
range(metadata$water_sample_ph_surf, na.rm=T)
#6.83 9.63
range(metadata$water_sample_do_bot, na.rm=T)
#-0.06 12.14
range(metadata$water_sample_do_surf, na.rm=T)
#3.14 8.23

# Started with 555 samples
# ended with 478 samples

# How many lakes cored in 2018 and 2017
sub2017<-metadata[metadata$year_sample=="2017",]
length(unique(sub2017$lake_name))
sub2018<-metadata[metadata$year_sample=="2018",]
length(unique(sub2018$lake_name))

#how many cores from the Snowies
sub<-metadata[metadata$mountain_range=="Snowy",]
length(unique(sub$lake_name))

#looking for how many lakes were cored more than once (eight)
sort(unique(sub$lake_drive))

#bighorn
sub<-metadata[metadata$mountain_range=="Bighorn",]
length(unique(sub$lake_name))
sub<-metadata[metadata$mountain_range=="Beartooth",]
length(unique(sub$lake_name))
sub<-metadata[metadata$mountain_range=="Wind River",]
length(unique(sub$lake_name))


#create supplementary table 1

ST1<-metadata[,names(metadata)%in%c("lake_id"                
, "drive"                  
,"lake_name"              
,"lake_number"            
, "year_sample"            
, "mountain_range"         
, "latitude"               
, "longitude"              
, "elevation_meters"       
, "max_lake_depth"
,"water_sample_depth_surf"
, "water_sample_ph_surf"   
,"water_sample_do_surf"   
, "water_sample_t_surf"    
, "water_sample_depth_bot" 
,"water_sample_ph_bot"    
, "water_sample_do_bot"    
,"water_sample_t_bot")]

ST1<-unique(ST1)

write.csv(ST1,"SupplementaryTable1_28June2023.csv")


#what percent of reads are archaea?
arch<-subset_taxa(ps, Kingdom=="Archaea")
sum(rowSums(arch@otu_table))/sum(rowSums(ps@otu_table))
#[1] 0.177471 - 28June2023 this is the only new number after rerunning in June

# additional generalizations between shallow, warm and cool, deep lakes

#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata

require(GGally)

metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

cor(metadata_sub)

which(is.na(metadata_sub[3]==T))
metadata_sub[which(is.na(metadata_sub[3]==T)),]

ggpairs(metadata_sub)


#alphaproteobacteria (nitrogen reducers) decline with depth
sub<-subset_taxa(ps_tr, Class=="Alphaproteobacteria")
sub %>% group_by (bin_depth) %>% summarize_at(vars(Abundance), list(abundance = mean)) %>% plot()
```

## 8. Distance decay
```{r}
#load data
#used transformed data
OTU<-t(as.data.frame(ps_tr@otu_table@.Data))
#calculate Bray-Curtis similarity between all samples
comm.dist <- 1 - vegdist(OTU)

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),rownames(OTU))),]
table(rownames(OTU)==rownames(metadata)) #all true

#Lat&lon to distance in meters
xy <- data.frame(X = metadata$longitude, Y = metadata$latitude)
rownames(xy)<-metadata$samp_names
dist_m_output<-distm(xy)
rownames(dist_m_output)<-rownames(xy)
names(dist_m_output)<-rownames(xy)
dist_m_output<-as.dist(dist_m_output)


#transform all distance matrices into dataframes with pairwise comparisons
coord.dist.ls<-as.matrix(dist_m_output)
coord.dist.ls[upper.tri(coord.dist.ls, diag = T)] <- NA
coord.dist.ls<-reshape2::melt(coord.dist.ls, na.rm=T)
names(coord.dist.ls)<-c("s1_samp_names","s2_samp_names","geo_dist")


comm.dist.ls<-as.matrix(comm.dist)
comm.dist.ls[upper.tri(comm.dist.ls, diag = T)] <- NA
comm.dist.ls<-reshape2::melt(comm.dist.ls, na.rm=T)
names(comm.dist.ls)<-c("s1_samp_names","s2_samp_names","comm")


#check the names of these match
table(coord.dist.ls[,1]==comm.dist.ls[,1]) #all true
table(coord.dist.ls[,2]==comm.dist.ls[,2]) #all true

#create df with similarity of community and distance
comps<-data.frame(comm.dist.ls)
comps$geo_dist<-coord.dist.ls$geo_dist
comps$s1_samp_names<-as.character(comps$s1_samp_names)
comps$s2_samp_names<-as.character(comps$s2_samp_names)

#add zone into metadata
metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

#merge in metadata to table
metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

comps<-merge(comps, metadata_s1, by="s1_samp_names")
comps<-merge(comps, metadata_s2, by="s2_samp_names")

#remove metadata notes
comps$s1_notes<-NULL
comps$s2_notes<-NULL
comps$s1_Notes_sed_water_wt<-NULL
comps$s2_Notes_sed_water_wt<-NULL
comps$s1_Notes_carbon_nitrogen<-NULL
comps$s2_Notes_carbon_nitrogen<-NULL
comps$s1_Notes._lake_sed_pH<-NULL
comps$s2_Notes._lake_sed_pH<-NULL

# add in centimeters into the distance
comps$abs_cm<-rep(NA, nrow(comps))
comps$s1_depth<-as.numeric(comps$s1_depth)
comps$s2_depth<-as.numeric(comps$s2_depth)
i=1
for(i in 1:nrow(comps)){
  ifelse(comps$s1_lake_drive[i]==comps$s2_lake_drive[i],comps$abs_cm[i]<-abs(comps$s2_depth[i]-comps$s1_depth[i]),NA)
}

#this one takes forever and adds 1 meter to cores within the same lake
comps$dist_1m_forsamelake<-rep(NA,nrow(comps))
i=1
for(i in 1:nrow(comps)){
 if(comps$s1_lake_name[i]==comps$s2_lake_name[i] & comps$s1_lake_drive[i] != comps$s2_lake_drive[i]){comps$dist_1m_forsamelake[i]<-comps$geo_dist[i]+1}else{comps$dist_1m_forsamelake[i]<-comps$geo_dist[i]}
}


#one sample with 2 replicates, so put in 0.001 for the dist_cm_core column
comps<-comps[order(comps$abs_cm,decreasing=F),]
which(names(comps)=="abs_cm")
comps[1,which(names(comps)=="abs_cm")]<-0.001
#check
comps[1,]

rm(comm.dist.ls)
rm(coord.dist.ls)
rm(metadata_s1)
rm(metadata_s2)
rm(OTU)
rm(xy)
rm(comm.dist)
rm(dist_m_output)
rm(i)

#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(ps_tr@otu_table@.Data))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

daisy.mat <- as.dist(as.matrix(daisy(scale(metadata_sub), metric="gower")))

env_distance<-as.matrix(daisy.mat)
env_distance[upper.tri(env_distance, diag = T)] <- NA
env_distance<-reshape2::melt(env_distance, na.rm=T)
names(env_distance)<-c("s1_samp_names","s2_samp_names","env_dist")

env_distance$s1_samp_names<-as.character(env_distance$s1_samp_names)
env_distance$s2_samp_names<-as.character(env_distance$s2_samp_names)

comps<-comps[order(comps$s1_samp_names,comps$s2_samp_names),]
env_distance<-env_distance[order(env_distance$s1_samp_names,env_distance$s2_samp_names),]

table(comps$s1_samp_names==env_distance$s1_samp_names) #all TRUE
table(comps$s2_samp_names==env_distance$s2_samp_names) #all TRUE

comps$env_dist<-env_distance$env_dist
rm(env_distance)

comps$geo_dist_km<-comps$geo_dist/1000

#overwrite file
#write.csv(comps, "pairwise_comparisons_29June2029.csv")

```

### iii. community similarity decay

summary figure (horizons not broken out)
```{r}
comps<-read.csv("pairwise_comparisons_29June2029.csv", header=T, row.names=1)

model_result<-list()
theme_comm<-theme_classic()+
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
              axis.text=element_text(size=13), #change font size of axis text
              axis.title=element_text(size=15), #change font size of axis titles  
              axis.title.x = element_text(vjust=-0.3),
              axis.title.y = element_text(margin = margin(r = 10)))
ggplt<-list()
#sediment distance from an individual core
sub<-comps[is.na(comps$abs_cm)==FALSE,]
sub<-sub[sub$s1_zone%in%c("A","B","C") & sub$s2_zone%in%c("A","B","C"),]
table(sub$s1_lake_drive==sub$s2_lake_drive)
#2081 comparisons (29June2023)


Data<-data.frame(x=sub$abs_cm,y=sub$comm)
Data<-Data[order(Data$x),]
#dat_gam=gam(y~s(x, k=4), data=Data) - older code
dat_gam=gam(y~s(x), data=Data)
model_result[[1]]<-summary(dat_gam)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

gam.check(dat_gam)

## plot
ggplt[[1]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                           col="black", alpha = 0.7) +
        geom_line(linewidth=1.5) + labs(x = "Sediment distance (cm)", y = "Community similarity (1-Bray)")+
        scale_x_continuous(breaks=seq(0,26,4))+
        ylim(0,0.8)+
        annotate("text", x=(26*0.99), y=0.8, label= "A", size=7)+
        annotate("text", x=(26*0.75), y=0.7,  label="Dev exp = 32.1%", size=5) +
        theme_comm



sub_bind<-NULL
#Geographic distance#
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        sub_bind<-rbind(sub_bind,sub)
}
        Data<-data.frame(x=sub_bind$geo_dist_km,y=sub_bind$comm)
        Data<-Data[order(Data$x),]
        #dat_gam=gam(y~s(x, k=4), data=Data)
        dat_gam=gam(y~s(x), data=Data)
        model_result[[2]]<-summary(dat_gam)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
         model_result[[2]]
        

ggplt[[2]]<-ggplot(data =Data , aes(x = x, y = fit)) +
        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                    col="black", alpha = 0.7) +
        geom_line(linewidth=1.5) + labs(x = "Distance (km)", y = "")+
        scale_x_continuous(breaks=seq(0,500,100))+
        annotate("text", x=(500*0.99), y=0.8, label= "B", size=7)+
        annotate("text", x=(500*0.75), y=0.7, label="Dev exp = 8.8%",  size=5) +
        ylim(0,0.8)+
        theme_comm  


sub_bind<-NULL
#environmental distance distance#
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        sub_bind<-rbind(sub_bind,sub)
}
Data<-data.frame(x=sub_bind$env_dist,y=sub_bind$comm)
Data<-Data[order(Data$x),]
#dat_gam=gam(y~s(x, k=4), data=Data)
dat_gam=gam(y~s(x), data=Data)
model_result[[3]]<-summary(dat_gam)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit
model_result[[3]]

   ggplt[[3]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col="darkgray", alpha=0.3) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       col="black", alpha = 0.7) +
   geom_line(linewidth=1.5) + labs(x = "Environmental dissimilarity", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           annotate("text", x=(0.73*0.99), y=0.8, label= "C", size=7)+
        annotate("text", x=(0.73*0.75), y=0.7, label="Dev exp = 17.6%", size=5) +
           ylim(0,0.8)+
           theme_comm


# pdf("Figures/CommSimmDecay_29June2023.pdf", height = 3.5, width= 11)
# ggplt[[1]]+ ggplt[[2]] + ggplt[[3]] + plot_layout(ncol = 3)
# dev.off()

pdf("Figures/CommSimmDecay_14Aug2023.pdf", height = 3.5, width= 11)
ggplt[[1]]+ ggplt[[2]] + ggplt[[3]] + plot_layout(ncol = 3)
dev.off()

model_result[[1]]
model_result[[2]]
model_result[[3]]

```


supplementary figure - GAMS with different horizons
```{r}
theme_comm<-theme_classic()+
        theme(legend.position="none", axis.text=element_text(color="black"))+
        theme(text=element_text(size=14), #change font size of all text
              axis.text=element_text(size=13), #change font size of axis text
              axis.title=element_text(size=15), #change font size of axis titles  
              axis.title.x = element_text(vjust=-0.3),
              axis.title.y = element_text(margin = margin(r = 10)))
ggplt<-list()
#sediment distance from an individual core
sub<-comps[is.na(comps$abs_cm)==FALSE,]
sub<-sub[sub$s1_zone%in%c("A","B","C") & sub$s2_zone%in%c("A","B","C"),]
table(sub$s1_lake_drive==sub$s2_lake_drive)
#2081 comparisons (16 Feb 2023)


Data<-data.frame(x=sub$abs_cm,y=sub$comm)
Data<-Data[order(Data$x),]
dat_gam=gam(y~s(x, k=4), data=Data)
pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
Data$fit<-pred$fit
Data$se<-pred$se.fit

## plot
ggplt[[1]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, alpha=0.3) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                            alpha = 0.3) +
        geom_line(linewidth=1.5) + labs(x = "Sediment distance (cm)", y = "Community similarity (1-Bray)")+
        scale_x_continuous(breaks=seq(0,26,4))+
        ylim(0,0.8)+
        theme_comm





#geographic distance
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        Data<-data.frame(x=sub$geo_dist_km,y=sub$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        
        if(i==1){     
                ggplt[[2]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "", y = "")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm
        }
        if(i==2){
                ggplt[[3]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "", y = "Community similarity (1-Bray)")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm
        }
        if(i==3){
                ggplt[[4]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                        geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                        geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                                    alpha = 0.3) +
                        geom_line(linewidth=1.5) + labs(x = "Distance (km)", y = "")+
                        scale_x_continuous(breaks=seq(0,500,100))+
                        ylim(0,0.8)+
                        theme_comm  
        }}

#environmental dissimilarity
i=1
for(i in 1:3){
        sub<-comps[comps$s1_zone==c("A","B","C")[i] & comps$s2_zone==c("A","B","C")[i],]
        Data<-data.frame(x=sub$env_dist,y=sub$comm)
        Data<-Data[order(Data$x),]
        dat_gam=gam(y~s(x, k=4), data=Data)
        pred = predict.gam(dat_gam, newdata = Data[1], se.fit=T)
        Data$fit<-pred$fit
        Data$se<-pred$se.fit
        
if(i==1){     
        ggplt[[5]]<-ggplot(data =Data , aes(x = x, y = fit)) +
                geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
                geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                            alpha = 0.3) +
                geom_line(linewidth=1.5) + labs(x = "", y = "")+
               scale_x_continuous(breaks=seq(0,0.73,0.1))+
                ylim(0,0.8)+
                theme_comm
}
if(i==2){
   ggplt[[6]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       alpha = 0.3) +
   geom_line(linewidth=1.5) + labs(x = "", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           ylim(0,0.8)+
           theme_comm
}
if(i==3){
   ggplt[[7]]<-ggplot(data =Data , aes(x = x, y = fit)) +
           geom_point(data =  Data, aes(x = x, y = y), size=1.5, col=adjustcolor(c('#018571','#C4AD79','#a6611a')[i], alpha.f = 0.30)) +
           geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
                       alpha = 0.3) +
   geom_line(linewidth=1.5) + labs(x = "Environmental dissimilarity", y = "")+
   scale_x_continuous(breaks=seq(0,0.73,0.1))+
           ylim(0,0.8)+
           theme_comm
}}

pdf("Figures/Fig5_CommunityAssembly_BC_16Feb2023.pdf",height=7,width=7)
ggplt[[2]]+ ggplt[[3]]+ggplt[[4]]+ggplt[[5]]+ggplt[[6]]+ ggplt[[7]]+ 
        plot_layout(ncol = 2, byrow = F)
dev.off()

pdf("Figures/Fig5_CommunityAssembly_A_16Feb2023.pdf",height=2.4,width=3.3)
ggplt[[1]]
dev.off()


```




## 9. Test for phylogenetic signal (Pearman code)

Subset ESV table for ESVs with 10+ reads for computational tractability

```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylogenetic_tree/ESVs_with10ormorereads_output_27May2023_muscled.nwk /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes

```


I can't to this with 28,480 ESVs (ESVs with 10+ reads, so will use the most abundant ESVs instead)

```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
all_depths<-NULL
i=1
cm<-unique(ps_tr@sam_data$bin_depth)
for(i in 1:length(cm)){
  ps_tmp <- subset_samples(ps_tr, bin_depth==cm[i]) 
  ps_tmp <- filter_taxa(ps_tmp, function(x) sum(x) > 0, TRUE)
  means <- as.data.frame(rowMeans(ps_tmp@otu_table))
  names(means)<-"val"
  means$otu<-rownames(means)
  avgabundtax<-means[order(means$val,decreasing=T),]$otu[1:3500]
  all_depths<-c(all_depths,avgabundtax)
}

unique_alldepths<-unique(all_depths)
length(unique_alldepths)

my_subset <- subset(otu_table(ps), rownames(otu_table(ps)) %in% unique_alldepths)
table(sort(rownames(otu_table(my_subset)))==sort(unique_alldepths)) #make sure this is all TRUE
```

all taxa but three match, so going ahead with using this
OTU_log.p <- match.phylo.comm(tree, OTU_log)$comm
[1] "Dropping taxa from the community because they are not present in the phylogeny:"
[1] "centroid=otu32478" "centroid=otu44283"
[3] "centroid=otu47385"
tree_log.p <- match.phylo.comm(tree, OTU_log)$phy
[1] "Dropping taxa from the community because they are not present in the phylogeny:"
[1] "centroid=otu32478" "centroid=otu44283"
[3] "centroid=otu47385"

```{r}
#Load phylogenetic tree
phylo <- read.tree("ESVs_with10ormorereads_output_27May2023_muscled.nwk")
#subset by the abundant 10875 ESVs
physeq <- merge_phyloseq(my_subset, tax_table(ps), sample_data(ps), phylo)
tree <- physeq@phy_tree
# make sure only the 10875 ESVs are in the tree
table(tree$tip.label%in%unique_alldepths) #all TRUE
table(unique_alldepths%in%tree$tip.label) #all TRUE
rm(physeq)

#using just the 10875 most abundant ESVs
#match up with metadata
OTU<-as.data.frame(otu_table(my_subset))
OTU<-t(OTU)
OTU<-OTU[order(rownames(OTU)),] #site by species

#order metadata
metadata<-metadata[order(rownames(metadata)),] 
#subset metadata
metadata.a <- metadata %>% select(depth, elevation_meters,max_lake_depth, water_sample_ph_bot, water_sample_do_bot, water_sample_t_bot,pH,n_perc, c_perc)

#make OTU and metadata match
table(rownames(OTU)==rownames(metadata)) #all true


#Predict the missing metadata (JVE: not very familiar with this part of John Pearman's code to predict missing metadata..)
set.seed(496)

metadata.1 <-
  preProcess(metadata.a,
             method = c("bagImpute"))

metadata.2  <- 
  predict(metadata.1 , metadata.a)


OTU_log <- log(OTU + 1) #JVE: unsure why log + 1

OTU_log.p <- match.phylo.comm(tree, OTU_log)$comm
tree_log.p <- match.phylo.comm(tree, OTU_log)$phy

phydist <- cophenetic(tree_log.p)

#phylogenetic distance
phydist_hel <- decostand(phydist, method="hellinger")

#from site x species to species x site
table <- t(OTU_log)

env.var.names<-colnames(metadata.2)

#loop through each environmental variable and create a list of each "final table"
all_env.vars_tab<-list()
q=1
for(q in 1:length(env.var.names)){

table_env<-rbind(metadata.2[,q],table)
rownames(table_env)[1]<-"env.var"

table_w <- t(table_env) #first column environmental variable 

headTable <- colnames(table_w) 
x <- headTable[1] # The first column is a constant env.var and is being compared to OTU


# Create the final_table empty

m <- matrix(0, nrow = 0, ncol = 3) 
final_table <- data.frame(m)
colnames(final_table)  <- c('ymax','d50', 'xmax') 

# the for loop, to get all otus 1 by 1 compared to env.var

for(i in seq(2,length(headTable),1)) #start at 2 because the 1 is the Eh_mv
{
  y <- headTable[i] # get otu name
  tablei <-  table_w[,c(x,y)] # extract 2 columns to create a new table
  
  head  <- colnames(tablei)
  
  ymax = which.max(tablei[,2]) # get the max relative abundance of the OTU
  
  MaxRow <- tablei[ymax,] # create a table with the line of the max OTU value
 
  tableiInf <- subset(tablei,tablei[,1]<= MaxRow[1]) #create two tables greater and lower than the x coord
  tableiSup <- subset(tablei,tablei[,1]>= MaxRow[1]) #of the max
  
  tableiInfno0 <- subset(tableiInf,tableiInf[,2]>(MaxRow[2]/20)) #remove the observations where the relative OTU abundance is 0
  tableiSupno0 <- subset(tableiSup,tableiSup[,2]>(MaxRow[2]/20)) #remove the observations where the relative OTU abundance is 0
  
  
  #Determine the lowest and highest Eh value where you find the OTU
  
  xInf <- min(tableiInfno0[,1])
  xSup <- max(tableiSupno0[,1])
  
  # calculate the d50
  
  d50  <- 0.68*abs(xSup-xInf)/2
  #JVE - why is this 0.68?
  
  # create the final table for the otu in the loop
  
  tablei_fin  <- matrix(c(MaxRow[2],d50,MaxRow[1]),ncol=3)
  rownames(tablei_fin)  <- c(head[2])
  colnames(tablei_fin)  <- c('ymax','d50', 'xmax')
  
  # add to the final_table the values for each otu 
  final_table <- rbind(final_table,tablei_fin)
  
  
}
#write the table and put into a list
write.table(final_table, paste("niche_", env.var.names[q], "_9Jan2023.txt", sep=""), sep="\t")
all_env.vars_tab[[q]]<-final_table
}

#remove all but the two objects needed for the mantel correlogs
rm(list=ls()[! ls() %in% c("all_env.vars_tab","phydist_hel")])
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")

```


```{bash}
rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal

PrayingMantus@0613@
```

#### i) run on Beartooth

phylogenetic_signal_1.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[1]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_1<-mantel.correlog(env_dist,phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_1.RData")

```

phylogenetic_signal_2.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[2]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_2<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_2.RData")

```

phylogenetic_signal_3.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[3]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_3<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_3.RData")

```

phylogenetic_signal_4.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[4]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_4<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_4.RData")

```

phylogenetic_signal_5.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[5]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_5<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_5.RData")

```

phylogenetic_signal_6.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[6]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_6<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_6.RData")

```

phylogenetic_signal_7.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[7]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_7<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_7.RData")

```

phylogenetic_signal_8.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[8]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_8<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_8.RData")

```

phylogenetic_signal_9.R

```{r}
require(vegan)
load("WyLakeMicrobes_10July2023_phylogeneticsignal_forBeartooth.RData")
sub1<-all_env.vars_tab[[9]]
sub2<-sub1[match(rownames(phydist_hel), rownames(sub1)), ]
env_dist<-vegdist(sub2$xmax, method="euclidean")
mc_result_9<-mantel.correlog(env_dist, D.geo = phydist_hel, nperm=999)
save.image("WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_9.RData")

```

run_phylogenetic_signal.sh

```{bash}
#!/bin/bash
#SBATCH --job-name phylo_signal
#SBATCH --mem=20GB
#SBATCH --time=6-00:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=phylo_signal_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jvonegge@uwyo.edu

module load gcc/12.2.0 r/4.2.2
cd /project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal
date

srun Rscript phylogenetic_signal_1.R
echo "srun Rscript phylogenetic_signal_1.R"

srun Rscript phylogenetic_signal_2.R
echo "srun Rscript phylogenetic_signal_2.R"

srun Rscript phylogenetic_signal_3.R
echo "srun Rscript phylogenetic_signal_3.R"

srun Rscript phylogenetic_signal_4.R
echo "srun Rscript phylogenetic_signal_4.R"

srun Rscript phylogenetic_signal_5.R
echo "srun Rscript phylogenetic_signal_5.R"

srun Rscript phylogenetic_signal_6.R
echo "srun Rscript phylogenetic_signal_6.R"

srun Rscript phylogenetic_signal_7.R
echo "srun Rscript phylogenetic_signal_7.R"

srun Rscript phylogenetic_signal_8.R
echo "srun Rscript phylogenetic_signal_8.R"

srun Rscript phylogenetic_signal_9.R
echo "srun Rscript phylogenetic_signal_9.R"


echo "finished correlogs"
date

```

copy folder back to computer

```{bash}
rsync -r jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/phylo_signal /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes
```

#### ii) Plot mantel correlograms

```{r}
env.var.names<-c("Sediment depth (cm)",               "Elevation (m)"   , "Max lake depth (m)" ,    
"Bottom water pH", "Bottom water dissolved oxygen", "Bottom water temperature" ,
"Sediment pH"                ,  "Sediment percent nitrogen"            ,  "Sediment percent carbon"    )  
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_1.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_2.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_3.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_4.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_5.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_6.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_7.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_8.RData")
load("phylo_signal/WyLakeMicrobes_10July2023_phylogeneticsignal_mantelcorrresult_9.RData")

figs<-list()
figs[[1]]<-mc_result_1
figs[[2]]<-mc_result_2
figs[[3]]<-mc_result_3
figs[[4]]<-mc_result_4
figs[[5]]<-mc_result_5
figs[[6]]<-mc_result_6
figs[[7]]<-mc_result_7
figs[[8]]<-mc_result_8
figs[[9]]<-mc_result_9

i=1
pdf("Figures/SupplementaryFigures/Phylogenetic_signal_mantel_corr_July2023.pdf", height = 8, width=10)
par(mfrow=c(3,3), mar=c(4,4,4,2))
for(i in 1:9){
plot(figs[[i]])
title(main=env.var.names[i])
}
dev.off()


```







## 10. Upload sequences to NCBI

```{r}
require(stringr)
require(reshape)
require(dplyr)
files<-read.delim("list_of_files.txt",header=F)
names(files)[1]<-"filename"
files$samp<-str_split(files$filename,pattern = ".16S.", 2, simplify = T)[,1]
files$samp<-str_split(files$samp,pattern = "Calder.", 2, simplify = T)[,2]
files$number<-rep(seq(1:4),653)

files<-reshape(files, idvar = "samp", timevar = "number", direction = "wide")
files<-as.data.frame(files)
names(files)<-c("sample_name", "filename", "filename2", "filename3", "filename4")


files$samp1<-str_split(files$filename,pattern = ".16S.", 2, simplify = T)[,1]
files$samp1<-str_split(files$samp1,pattern = "Calder.", 2, simplify = T)[,2]

files$samp2<-str_split(files$filename2,pattern = ".16S.", 2, simplify = T)[,1]
files$samp2<-str_split(files$samp2,pattern = "Calder.", 2, simplify = T)[,2]

files$samp3<-str_split(files$filename3,pattern = ".16S.", 2, simplify = T)[,1]
files$samp3<-str_split(files$samp3,pattern = "Calder.", 2, simplify = T)[,2]

files$samp4<-str_split(files$filename4,pattern = ".16S.", 2, simplify = T)[,1]
files$samp4<-str_split(files$samp4,pattern = "Calder.", 2, simplify = T)[,2]

files$TF<-rep(NA,nrow(files))
for( i in 1:nrow(files)){
        files$TF[i]<-ifelse(files$sample_name[i]==files$samp1[i] & files$sample_name[i]==files$samp2[i] & files$sample_name[i]==files$samp3[i] & files$sample_name[i]==files$samp4[i], files$TF[i]<-TRUE,files$TF[i]<-FALSE )
}

table(files$TF)

order<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/additional_files/SRA_metadata_sampleorder.csv",header=F)

metadata<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/metadata_29Aug2022.csv", header=T, row.names=1)
table(order$V1==metadata$cleanedID)

#rename East Glacier mislabeled sample
files[which(files$sample_name=="EG0308L2"),]$sample_name<-"EG0318L"

subfiles<-filter(files, sample_name %in% metadata$samp_names)  

subfiles<-subfiles[order(match(subfiles$sample_name,metadata$samp_names)),]

table(subfiles$sample_name==metadata$samp_names)
subfiles<-subfiles[,1:5]
write.csv(subfiles, "/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/additional_files/Filenames_for_NCBI_SRA_upload.csv")

fastq_keep_list<-c(subfiles$filename, subfiles$filename2, subfiles$filename3, subfiles$filename4)
write.table(fastq_keep_list,"Metadata/additional_files/subset_list_of_files_forBeartooth.txt", sep=",",col.names = FALSE, row.names=FALSE, quote = FALSE)
```

```{bash}
rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/additional_files/subset_list_of_files_forBeartooth.txt jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload

#subset bash files on linux
cp /project/microbiome/data/seq/psomagen_17sep20_novaseq2/rawdata/sample_fastq/16S/Calder/* /project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/fastq_files

rsync -a /project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/fastq_files --files-from=/project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/subset_list_of_files_forBeartooth.txt /project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/subset_fastq_files



sftp subftp@ftp-private.ncbi.nlm.nih.gov
```


additional code - not sure if actually used it.
```{r}
require(stringr)
require(reshape)
require(dplyr)
files<-read.delim("list_of_files.txt",header=F)
names(files)[1]<-"filename"
files$samp<-str_split(files$filename,pattern = ".16S.", 2, simplify = T)[,1]
files$samp<-str_split(files$samp,pattern = "Calder.", 2, simplify = T)[,2]
files$number<-rep(seq(1:4),653)

files<-reshape(files, idvar = "samp", timevar = "number", direction = "wide")
files<-as.data.frame(files)
names(files)<-c("sample_name", "filename", "filename2", "filename3", "filename4")


files$samp1<-str_split(files$filename,pattern = ".16S.", 2, simplify = T)[,1]
files$samp1<-str_split(files$samp1,pattern = "Calder.", 2, simplify = T)[,2]

files$samp2<-str_split(files$filename2,pattern = ".16S.", 2, simplify = T)[,1]
files$samp2<-str_split(files$samp2,pattern = "Calder.", 2, simplify = T)[,2]

files$samp3<-str_split(files$filename3,pattern = ".16S.", 2, simplify = T)[,1]
files$samp3<-str_split(files$samp3,pattern = "Calder.", 2, simplify = T)[,2]

files$samp4<-str_split(files$filename4,pattern = ".16S.", 2, simplify = T)[,1]
files$samp4<-str_split(files$samp4,pattern = "Calder.", 2, simplify = T)[,2]

files$TF<-rep(NA,nrow(files))
for( i in 1:nrow(files)){
        files$TF[i]<-ifelse(files$sample_name[i]==files$samp1[i] & files$sample_name[i]==files$samp2[i] & files$sample_name[i]==files$samp3[i] & files$sample_name[i]==files$samp4[i], files$TF[i]<-TRUE,files$TF[i]<-FALSE )
}

table(files$TF)

order<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/additional_files/SRA_metadata_sampleorder.csv",header=F)

metadata<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/metadata_29Aug2022.csv", header=T, row.names=1)
table(order$V1==metadata$cleanedID)

#rename East Glacier mislabeled sample
files[which(files$sample_name=="EG0308L2"),]$sample_name<-"EG0318L"

subfiles<-filter(files, sample_name %in% metadata$samp_names)  

subfiles<-subfiles[order(match(subfiles$sample_name,metadata$samp_names)),]

table(subfiles$sample_name==metadata$samp_names)
subfiles<-subfiles[,1:5]
write.csv(subfiles, "/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/additional_files/Filenames_for_NCBI_SRA_upload.csv")

fastq_keep_list<-c(subfiles$filename, subfiles$filename2, subfiles$filename3, subfiles$filename4)
write.table(fastq_keep_list,"Metadata/additional_files/subset_list_of_files_forBeartooth.txt", sep=",",col.names = FALSE, row.names=FALSE, quote = FALSE)

rsync /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/WYLakeSedMicrobes/Metadata/additional_files/subset_list_of_files_forBeartooth.txt jvonegge@beartooth.arcc.uwyo.edu:/project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload

#subset bash files on linux
cp /project/microbiome/data/seq/psomagen_17sep20_novaseq2/rawdata/sample_fastq/16S/Calder/* /project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/fastq_files

rsync -a /project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/fastq_files --files-from=/project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/subset_list_of_files_forBeartooth.txt /project/seddna/jvonegge/WY_lake_microbes/16S/NCBI_upload/subset_fastq_files



```



## 11. shared ESVs and shared families

will all reads

```{r}
require(Matrix)
require(metagMisc)
require(phyloseq)
require(reshape2)
require(cluster)
require(stringr)
require(geosphere)

load("WyLakeMicrobes_phyloseq_env_29Aug2022.RData")

#calculate
shared_esvs <- phyloseq_num_shared_otus(ps)


#shared ESVs
shared<-as.matrix(shared_esvs[["shared"]])
shared[upper.tri(shared, diag = T)] <- NA
shared<-melt(shared,na.rm=TRUE)

colnames(shared)[3]<-"shared"

#nonshared ESVs
nonshared<-as.matrix(shared_esvs[["nonshared_total"]])
nonshared[upper.tri(nonshared, diag = T)] <- NA
nonshared<-melt(nonshared,na.rm=TRUE)
colnames(nonshared)[3]<-"nonshared"

table(nonshared$Var1==shared$Var1) #all T
table(nonshared$Var2==shared$Var2) #all T

shared$nonshared<-nonshared$nonshared
rm(nonshared)
shared$percent<-shared$shared/(shared$shared+shared$nonshared)


#now add in environmental distances
#using objects from above
variables<-c("max_lake_depth"         
, "water_sample_ph_bot"     
, "water_sample_do_bot"     
, "water_sample_t_bot") 

#pull out metadata
metadata<-metadata[order(match(rownames(metadata),colnames(as.matrix(shared_esvs[["shared"]])))),]
metadata_sub<-metadata[,names(metadata)%in%variables]

daisy.mat <- as.matrix(daisy(scale(metadata_sub), metric="gower"))

daisy.mat[upper.tri(daisy.mat, diag = T)] <- NA
env_dist<-melt(daisy.mat, na.rm=TRUE)

table(shared$Var1==env_dist$Var1) # all true
table(shared$Var2==env_dist$Var2) # all true 
shared$env_dist<-env_dist$value
 #overwrite original file

names(shared)[1]<-"s1_samp_names"
shared$s1_samp_names<-as.character(shared$s1_samp_names)
names(shared)[2]<-"s2_samp_names"
shared$s2_samp_names<-as.character(shared$s2_samp_names)

metadata$zone<-rep("A",nrow(metadata))
metadata[metadata$bin_depth%in%c(6,8,10,12),]$zone<-"B"
metadata[metadata$bin_depth%in%c(14,16,18,20,22,24,26),]$zone<-"C"
metadata[metadata$depth>26,]$zone<-"D"

metadata_s1<-metadata
colnames(metadata_s1)<-paste("s1",colnames(metadata_s1), sep="_")

metadata_s2<-metadata
colnames(metadata_s2)<-paste("s2",colnames(metadata_s2), sep="_")

shared<-merge(shared, metadata_s1, by="s1_samp_names")
shared<-merge(shared, metadata_s2, by="s2_samp_names")

#remove metadata notes
shared$s1_notes<-NULL
shared$s2_notes<-NULL
shared$s1_Notes_sed_water_wt<-NULL
shared$s2_Notes_sed_water_wt<-NULL
shared$s1_Notes_carbon_nitrogen<-NULL
shared$s2_Notes_carbon_nitrogen<-NULL
shared$s1_Notes._lake_sed_pH<-NULL
shared$s2_Notes._lake_sed_pH<-NULL

#absolute cm for sediment distance

# add in absolute centimeters into the distance
shared$abs_cm<-rep(NA, nrow(shared))
i=1
for(i in 1:nrow(shared)){
  ifelse(shared$s1_lake_drive[i]==shared$s2_lake_drive[i],shared$abs_cm[i]<-abs(shared$s2_depth[i]-shared$s1_depth[i]),NA)
}


# calculate geographic distance


for(i in 1:nrow(shared)){
        shared$geo_dist[i]<-as.numeric(distm(data.frame(X = shared$s1_longitude[i], Y = shared$s1_latitude[i]),data.frame(X = shared$s2_longitude[i], Y = shared$s2_latitude[i])))
}

shared$geo_dist_km<-shared$geo_dist/1000


#write.csv(shared,"sharedESVs_14Aug2023.csv")
```

```{r}
# comps_backup<-read.csv("sharedESVs_14Aug2023.csv",header=T,row.names=1)
# comps_backup$percent<-comps_backup$percent*100

zone<-c("A","B","C")
comps<-comps_backup
pdf("Figures/SupplementaryFigures/PercentSharedESVS_14Aug2023.pdf", width=7, height=7)
par(mfrow=c(3,3), mar=c(4,4,3,2))

j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]
plot(comps_sub$env_dist, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Environmental dissimilarity", ylim=c(0,40),main=c("Redox", "Transition", "Depauperate")[j])
abline(lm(comps_sub$percent~comps_sub$env_dist), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$env_dist))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$env_dist)*.95), 40*0.95,"*" ,cex=3)
}}



j=1
for(j in 1:3){
comps_sub<-comps[comps$s1_zone==zone[j] & comps$s2_zone==zone[j],]
plot(comps_sub$geo_dist_km, comps_sub$percent, ylab="Shared ESVs (%)", pch=16, col="#00000050",xlab="Distance (km)",ylim=c(0,40))
abline(lm(comps_sub$percent~comps_sub$geo_dist_km), col="lightgreen", lwd=2)
if(summary(lm(comps_sub$percent~comps_sub$geo_dist_km))$coefficients[2,4]<0.05){
        text(x=(max(comps_sub$geo_dist_km)*.95), 40*0.95,"*" ,cex=3)
}}



#sediment
comps<-comps_backup[is.na(comps_backup$abs_cm)==F & comps_backup$abs_cm<27 ,]
plot(comps$abs_cm, comps$percent, ylab="Shared ESVs (%)", xlab="Sediment distance (cm)", pch=16, col="#00000050",main="All horizons, individual cores")
abline(lm(comps$percent~comps$abs_cm), col="lightgreen", lwd=2)
if(summary(lm(comps$percent~comps$abs_cm))$coefficients[2,4]<0.05){
        text(x=(max(comps$abs_cm)*.95), max(comps$percent)*0.95,"*" ,cex=3)
}

dev.off()
```






#ISSUE - alpha diversity is really different with this new dataset
Going to use procrust to try to look at the differences between samples
```{r}
load("WyLakeMicrobes_Phyloseq_11May2023.RData")
ps_table <- data.frame(otu_table(ps_tr))
ps_table <-t(ps_table)
new.dist<-metaMDS(ps_table, distance = "bray")

load("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/Calder_vsearch/16S/WYLakeSedMicrobes_old/WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
ps_table <- data.frame(otu_table(ps_tr))
ps_table <-t(ps_table)
old.dist<-metaMDS(ps_table, distance = "bray") 

#calculate vegdist
load("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/LakeSedDNA/Data/WYLakeSedMicrobes/Calder_vsearch/16S/WYLakeSedMicrobes_old/WyLakeMicrobes_phyloseq_env_29Aug2022.RData")
old.dist <- vegdist(wisconsin(t(data.frame(otu_table(ps_tr)))))

load("WyLakeMicrobes_Phyloseq_11May2023.RData")
new.dist <- vegdist(wisconsin(t(data.frame(otu_table(ps_tr)))))

mds.old <- monoMDS(old.dist)
mds.new <- monoMDS(new.dist)

procrust <- procrustes(mds.old, mds.new)
procrust


summary(procrust)

pdf("Figures/Procrustes_diff_28June2023.pdf",height=10, width=20)
plot(procrust)
dev.off()


plot(procrust, kind=2)
residuals(procrust)
summary(procrust)
eigenvals(procrust)
plot(procrust, kind=2)
residuals(procrust)


rm(notnorm.dist)
rm(norm.dist)
rm(mds.notnorm)
rm(mds.norm)
rm(procrust)
rm(otu_to_normalize_10k)
rm(otu_table)

```




